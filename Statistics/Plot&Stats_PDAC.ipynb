{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **Plot&Stats**\n",
        "---\n",
        "\n",
        "<font size = 4>Code use to perform the statistical analysises used in the paper\n",
        "\n",
        "\n",
        "<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1. Prepare the session and load your data**\n",
        "--------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_BZuYOQGo1p"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install\n",
        "%pip -q install pandas scikit-learn\n",
        "%pip -q install plotly\n",
        "%pip -q install prettytable\n",
        "%pip -q install reportlab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAP0ahCzn1V6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to load the dependancies\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import itertools\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import requests\n",
        "from io import StringIO\n",
        "from IPython.display import display, clear_output\n",
        "import pandas as pd\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from prettytable import PrettyTable\n",
        "import os\n",
        "\n",
        "# Function to calculate Cohen's d\n",
        "def cohen_d(group1, group2):\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var()\n",
        "    var2 = group2.var()\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
        "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
        "\n",
        "    # Estimating the number of chunks based on the provided chunk size\n",
        "    num_chunks = int(len(df) / chunk_size) + 1\n",
        "\n",
        "    # Create a tqdm instance for progress tracking\n",
        "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
        "        # Open the file for writing\n",
        "        with open(path, \"w\") as f:\n",
        "            # Write the header once at the beginning\n",
        "            df.head(0).to_csv(f, index=False)\n",
        "\n",
        "            for chunk in np.array_split(df, num_chunks):\n",
        "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "def check_for_nans(df, df_name):\n",
        "    \"\"\"\n",
        "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
        "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
        "    \"\"\"\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            nan_count = df[col].isna().sum()\n",
        "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"No NaN values found in {df_name}.\")\n",
        "\n",
        "\n",
        "def save_parameters(params, file_path, param_type):\n",
        "    # Convert params dictionary to a DataFrame for human readability\n",
        "    new_params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])\n",
        "    new_params_df['Type'] = param_type\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        # Read existing file\n",
        "        existing_params_df = pd.read_csv(file_path)\n",
        "\n",
        "        # Merge the new parameters with the existing ones\n",
        "        # Update existing parameters or append new ones\n",
        "        updated_params_df = pd.merge(existing_params_df, new_params_df,\n",
        "                                     on=['Type', 'Parameter'],\n",
        "                                     how='outer',\n",
        "                                     suffixes=('', '_new'))\n",
        "\n",
        "        # If there's a new value, update it, otherwise keep the old value\n",
        "        updated_params_df['Value'] = updated_params_df['Value_new'].combine_first(updated_params_df['Value'])\n",
        "\n",
        "        # Drop the temporary new value column\n",
        "        updated_params_df.drop(columns='Value_new', inplace=True)\n",
        "    else:\n",
        "        # Use new parameters DataFrame directly if file doesn't exist\n",
        "        updated_params_df = new_params_df\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    updated_params_df.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDAwkSOo1gV"
      },
      "source": [
        "## **1.3. Load your dataset**\n",
        "---\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yIX1uUc3NpCS"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Load your dataset:\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from io import StringIO\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Initialize dataset_df as an empty DataFrame globally\n",
        "dataset_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# Create widgets\n",
        "dataset_path_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to your dataset',\n",
        "    description='Dataset Path:',\n",
        "    layout={'width': '80%'}\n",
        ")\n",
        "\n",
        "results_folder_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to your results folder',\n",
        "    description='Results Folder:',\n",
        "    layout={'width': '80%'}\n",
        ")\n",
        "\n",
        "data_textarea = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Or copy and paste your tab sperated data here (direct copy and paste from a spreedsheet)',\n",
        "    description='Or Paste Data:',\n",
        "    layout={'width': '80%', 'height': '200px'}\n",
        ")\n",
        "\n",
        "load_button = widgets.Button(\n",
        "    description='Load Data',\n",
        "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to load the data',\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "# Load data function\n",
        "def load_data(b):\n",
        "    global dataset_df\n",
        "    global Results_Folder\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "        Results_Folder = results_folder_input.value.strip()\n",
        "        if not Results_Folder:\n",
        "            Results_Folder = './Results'  # Default path if not provided\n",
        "        if not os.path.exists(Results_Folder):\n",
        "            os.makedirs(Results_Folder)  # Create the folder if it doesn't exist\n",
        "        print(f\"Results folder is located at: {Results_Folder}\")\n",
        "\n",
        "        if dataset_path_input.value.strip():\n",
        "            dataset_path = dataset_path_input.value.strip()\n",
        "            try:\n",
        "                dataset_df = pd.read_csv(dataset_path)\n",
        "                print(f\"Loaded dataset from {dataset_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load dataset from {dataset_path}: {e}\")\n",
        "        elif data_textarea.value.strip():\n",
        "            input_data = StringIO(data_textarea.value)\n",
        "            try:\n",
        "                dataset_df = pd.read_csv(input_data, sep='\\t')\n",
        "                print(\"Loaded dataset from pasted tab-separated data\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load dataset from pasted data: {e}\")\n",
        "        else:\n",
        "            print(\"No dataset path provided or data pasted. Please provide a dataset.\")\n",
        "            return\n",
        "\n",
        "        # Perform a check for NaNs or any other required processing here\n",
        "        check_for_nans(dataset_df, \"your dataset\")\n",
        "\n",
        "        display(dataset_df.head())\n",
        "\n",
        "# Set the button click event\n",
        "load_button.on_click(load_data)\n",
        "\n",
        "# Display the widgets\n",
        "display(widgets.VBox([dataset_path_input, results_folder_input, data_textarea, load_button, output]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4. Turn your dataset to tidy**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Qe07WRMya5uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Figure_panel = 'FigS7I'  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "tidy_df = pd.melt(dataset_df, var_name='Condition', value_name=Figure_panel)\n",
        "tidy_df['Repeat'] = 1\n",
        "tidy_df = tidy_df.dropna()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AxRp6iJ5BsA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11aD1AmQh7ST"
      },
      "source": [
        "-------------------------------------------\n",
        "\n",
        "# **Part 2. Plot your dataset**\n",
        "-------------------------------------------\n",
        "\n",
        "<font size = 4> In this section you can plot your data. Data and graphs are automatically saved in your result folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pjRDlYOgr3r"
      },
      "source": [
        "## **2.1. Plot your entire dataset**\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIa_MlR2Ktu-"
      },
      "source": [
        "##**Statistical analyses**\n",
        "### Cohen's d (Effect Size):\n",
        "<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n",
        "\n",
        "### Randomization Test:\n",
        "<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Plot (entire dataset)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.ticker import FixedLocator\n",
        "import itertools\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Plots\"  # Change to your actual folder path\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = tidy_df  # Change to your actual dataframe variable\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "    # Select only numerical columns\n",
        "    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def create_condition_selector(df, column_name):\n",
        "    conditions = df[column_name].unique()\n",
        "    condition_selector = SelectMultiple(\n",
        "        options=conditions,\n",
        "        description='Conditions:',\n",
        "        disabled=False,\n",
        "        layout=Layout(width='100%')  # Adjusting the layout width\n",
        "    )\n",
        "    return condition_selector\n",
        "\n",
        "def display_condition_selection(df, column_name):\n",
        "    condition_selector = create_condition_selector(df, column_name)\n",
        "\n",
        "    condition_accordion = Accordion(children=[VBox([condition_selector])])\n",
        "    condition_accordion.set_title(0, 'Select Conditions')\n",
        "    display(condition_accordion)\n",
        "    return condition_selector\n",
        "\n",
        "def format_scientific_for_ticks(x):\n",
        "    \"\"\"Format p-values for ticks: use scientific notation for values below 0.001, otherwise use standard notation.\"\"\"\n",
        "    if x < 0.001:\n",
        "        return f\"{x:.1e}\"\n",
        "    else:\n",
        "        return f\"{x:.4f}\"\n",
        "\n",
        "def format_p_value(x):\n",
        "    \"\"\"Format p-values to four significant digits.\"\"\"\n",
        "    if x < 0.001:\n",
        "        return \"< 0.001\"\n",
        "    else:\n",
        "        return f\"{x:.4g}\"  # .4g ensures four significant digits\n",
        "\n",
        "def safe_log10_p_values(matrix):\n",
        "    \"\"\"Apply a safe logarithmic transformation to p-values, handling p=1 specifically.\"\"\"\n",
        "    # Replace non-positive values with a very small number just greater than 0\n",
        "    small_value = np.nextafter(0, 1)\n",
        "    adjusted_matrix = np.where(matrix > 0, matrix, small_value)\n",
        "\n",
        "    logged_matrix = -np.log10(adjusted_matrix)\n",
        "    logged_matrix[matrix == 1] = -np.log10(0.999)\n",
        "    return logged_matrix\n",
        "\n",
        "def plot_heatmap(ax, matrix, title, cmap='viridis'):\n",
        "    \"\"\"Plot a heatmap with logarithmic scaling of p-values and real p-values as annotations.\n",
        "    Skip annotations if there are more than 7 conditions.\"\"\"\n",
        "    log_matrix = safe_log10_p_values(matrix.fillna(1))\n",
        "\n",
        "    # Define the normalization range\n",
        "    vmin = -np.log10(0.1)  # Set vmin to the log-transformed value of 0.1\n",
        "    vmax = np.max(log_matrix[np.isfinite(log_matrix)])\n",
        "\n",
        "    if vmin > vmax:\n",
        "        vmin = vmax\n",
        "\n",
        "    # Format annotations if conditions are 6 or fewer\n",
        "    num_conditions = len(matrix.columns)\n",
        "    if num_conditions <= 7:\n",
        "        formatted_annotations = matrix.applymap(lambda x: format_p_value(x) if pd.notna(x) else \"NaN\")\n",
        "    else:\n",
        "        formatted_annotations = False  # No annotations\n",
        "\n",
        "    # Plot the heatmap without the color bar\n",
        "    heatmap = sns.heatmap(log_matrix, ax=ax, cmap=cmap, annot=formatted_annotations,\n",
        "                          fmt=\"\", xticklabels=matrix.columns, yticklabels=matrix.index, cbar=False, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    # Create a color bar with conditional formatting for ticks\n",
        "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = ax.figure.colorbar(sm, ax=ax)\n",
        "\n",
        "    # Set custom ticks and labels for the color bar\n",
        "    num_ticks = 5\n",
        "    tick_locs = np.linspace(vmin, vmax, num_ticks)\n",
        "    tick_labels = [format_scientific_for_ticks(10**-tick) for tick in tick_locs]\n",
        "    cbar.set_ticks(tick_locs)\n",
        "    cbar.set_ticklabels(tick_labels)\n",
        "\n",
        "def cohen_d(group1, group2):\n",
        "    \"\"\"Calculate Cohen's d for measuring effect size between two groups.\"\"\"\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var(ddof=1)  # ddof=1 for sample variance\n",
        "    var2 = group2.var(ddof=1)\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "def perform_randomization_test(df, cond1, cond2, var, n_iterations=1000):\n",
        "    \"\"\"Perform a randomization test using Cohen's d as the effect size metric.\"\"\"\n",
        "    group1 = df[df['Condition'] == cond1][var]\n",
        "    group2 = df[df['Condition'] == cond2][var]\n",
        "    observed_effect_size = cohen_d(group1, group2)\n",
        "    combined = np.concatenate([group1, group2])\n",
        "    count_extreme = 0\n",
        "    # Perform the randomization test\n",
        "    for i in range(n_iterations):\n",
        "      if i % 100 == 0:\n",
        "        np.random.shuffle(combined)\n",
        "        new_group1 = combined[:len(group1)]\n",
        "        new_group2 = combined[len(group1):]\n",
        "        new_effect_size = cohen_d(new_group1, new_group2)\n",
        "      if abs(new_effect_size) >= abs(observed_effect_size):\n",
        "          count_extreme += 1\n",
        "\n",
        "    p_value = (count_extreme + 1) / (n_iterations + 1)\n",
        "    return p_value\n",
        "\n",
        "def perform_t_test(df, cond1, cond2, var):\n",
        "    \"\"\"\n",
        "    Perform a t-test directly between the two groups (conditions) for the given variable.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input dataframe in tidy format.\n",
        "        cond1 (str): The name of the first condition.\n",
        "        cond2 (str): The name of the second condition.\n",
        "        var (str): The variable to perform the t-test on.\n",
        "\n",
        "    Returns:\n",
        "        float: The p-value from the t-test.\n",
        "    \"\"\"\n",
        "    # Extract the data for the two conditions\n",
        "    group1 = df[df['Condition'] == cond1][var]\n",
        "    group2 = df[df['Condition'] == cond2][var]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)  # Welch's t-test for unequal variances\n",
        "\n",
        "    return p_value\n",
        "\n",
        "def plot_selected_vars(button, df, Conditions, Results_Folder, condition_selector, stat_method_selector):\n",
        "    plt.clf()  # Clear the current figure before creating a new plot\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "    # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    print(f\"Variables to plot: {variables_to_plot}\")\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "    # Get selected conditions\n",
        "    selected_conditions = condition_selector.value\n",
        "    print(f\"Selected conditions: {selected_conditions}\")\n",
        "    selected_conditions = condition_selector.value\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "    if n_selected_conditions == 0:\n",
        "        print(\"No conditions selected for plotting, therefore all available conditions are selected by default\")\n",
        "        selected_conditions = df[Conditions].unique().tolist()\n",
        "\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "\n",
        "    # Use only selected and ordered conditions\n",
        "    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n",
        "\n",
        "    unique_conditions = filtered_df[Conditions].unique().tolist()\n",
        "\n",
        "    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n",
        "    n_iterations = 10000\n",
        "    method = stat_method_selector.value\n",
        "    print(f\"Selected method: {method}\")\n",
        "\n",
        "    effect_size_matrices = {}\n",
        "    p_value_matrices = {}\n",
        "    bonferroni_matrices = {}\n",
        "\n",
        "    for var in variables_to_plot:\n",
        "        print(f\"Processing variable: {var}\")\n",
        "        effect_size_matrices[var] = pd.DataFrame(0, index=unique_conditions, columns=unique_conditions)\n",
        "        p_value_matrices[var] = pd.DataFrame(1, index=unique_conditions, columns=unique_conditions)\n",
        "\n",
        "\n",
        "        for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n",
        "            group1 = filtered_df[filtered_df[Conditions] == cond1][var]\n",
        "            group2 = filtered_df[filtered_df[Conditions] == cond2][var]\n",
        "\n",
        "            effect_size = abs(cohen_d(group1, group2))\n",
        "\n",
        "            if method == 't-test':\n",
        "                p_value = perform_t_test(filtered_df, cond1, cond2, var)\n",
        "            elif method == 'randomization test':\n",
        "                p_value = perform_randomization_test(filtered_df, cond1, cond2, var, n_iterations=n_iterations)\n",
        "\n",
        "            # Set and mirror effect sizes and p-values\n",
        "            effect_size_matrices[var].loc[cond1, cond2] = effect_size_matrices[var].loc[cond2, cond1] = effect_size\n",
        "            p_value_matrices[var].loc[cond1, cond2] = p_value_matrices[var].loc[cond2, cond1] = p_value\n",
        "\n",
        "\n",
        "        # Save to CSV\n",
        "        combined_df = pd.concat([\n",
        "            effect_size_matrices[var].rename(columns=lambda x: f\"{x} (Effect Size)\"),\n",
        "            p_value_matrices[var].rename(columns=lambda x: f\"{x} ({method} P-Value)\")\n",
        "        ], axis=1)\n",
        "\n",
        "        combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n",
        "        print(f\"Saved statistics to CSV for variable: {var}\")\n",
        "\n",
        "        # Create a new figure\n",
        "        fig = plt.figure(figsize=(16, 10))\n",
        "        gs = GridSpec(2, 2, height_ratios=[1, 1])\n",
        "        ax_box = fig.add_subplot(gs[0, :])\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n",
        "        Q1 = df[var].quantile(0.1)\n",
        "        Q3 = df[var].quantile(0.9)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define bounds for the outliers\n",
        "        multiplier = 10\n",
        "        lower_bound = Q1 - multiplier * IQR\n",
        "        upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "        # Plotting\n",
        "        sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n",
        "        ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n",
        "        ax_box.set_title(f\"{var}\")\n",
        "        ax_box.set_xlabel('Condition')\n",
        "        ax_box.set_ylabel(var)\n",
        "        tick_labels = ax_box.get_xticklabels()\n",
        "        tick_locations = ax_box.get_xticks()\n",
        "        ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n",
        "        ax_box.set_xticklabels(tick_labels, rotation=90)\n",
        "        ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "        # Statistical Analyses and Heatmaps\n",
        "\n",
        "        # Effect Size heatmap\n",
        "        ax_d = fig.add_subplot(gs[1, 0])\n",
        "        ax_d.set_xticklabels(ax_d.get_xticklabels(), rotation=90)\n",
        "        sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n",
        "        ax_d.set_title(f\"Effect Size (Cohen's d)\")\n",
        "\n",
        "        # p-value heatmap using the new function\n",
        "        ax_p = fig.add_subplot(gs[1, 1])\n",
        "        plot_heatmap(ax_p, p_value_matrices[var], f\"{method} p-value\")\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n",
        "        pdf_pages.savefig(fig)\n",
        "        pdf_pages.close()\n",
        "        print(f\"Saved PDF for variable: {var}\")\n",
        "        plt.show()\n",
        "\n",
        "# Initialize UI elements\n",
        "selectable_columns = get_selectable_columns(df_to_plot)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "condition_selector = display_condition_selection(df_to_plot, Conditions)\n",
        "stat_method_selector = widgets.Dropdown(\n",
        "    options=['randomization test', 't-test'],\n",
        "    value='randomization test',\n",
        "    description='Stat Method:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars(b, df_to_plot, Conditions, base_folder, condition_selector, stat_method_selector))\n",
        "\n",
        "display(VBox([stat_method_selector, button]))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vQcRlZmd8TOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLn_OF4689rS"
      },
      "source": [
        "## **2.2. Export data summaries**\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Export data summaries\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib import colors\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "# Assuming Results_Folder, dataset_df, and Conditions are defined\n",
        "save_path = f\"{Results_Folder}/variables_summary\"\n",
        "df_to_plot = tidy_df  # Assuming dataset_df is the DataFrame to work with\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "def save_table_as_pdf(data, column_names, filename):\n",
        "    \"\"\"\n",
        "    Saves the table data as a nicely formatted PDF using ReportLab's Table capabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - data: List of lists containing the table data (including the header).\n",
        "    - column_names: List of column names for the table.\n",
        "    - filename: Path to save the PDF file.\n",
        "    \"\"\"\n",
        "    pdf = SimpleDocTemplate(filename, pagesize=letter)\n",
        "    elements = []\n",
        "\n",
        "    # Create a table with the data\n",
        "    table_data = [column_names] + data\n",
        "\n",
        "    # Set up the table with style\n",
        "    table = Table(table_data)\n",
        "    table.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),  # Header background color\n",
        "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),  # Header text color\n",
        "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),  # Center all text\n",
        "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),  # Header font\n",
        "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),  # Padding below header\n",
        "        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),  # Background color for data\n",
        "        ('GRID', (0, 0), (-1, -1), 1, colors.black),  # Grid lines\n",
        "    ]))\n",
        "\n",
        "    # Append table to elements and build PDF\n",
        "    elements.append(table)\n",
        "    pdf.build(elements)\n",
        "\n",
        "def generate_display_and_save_statistics(df, columns, Conditions, save_path):\n",
        "    \"\"\"\n",
        "    Generates, displays using prettytable, and saves as CSV and PDF the statistical summaries\n",
        "    for selected columns of the DataFrame, grouped by the specified condition column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to analyze.\n",
        "    - columns: List of column names to generate statistics for.\n",
        "    - Conditions: Column name to group by.\n",
        "    - save_path: Directory path where CSV files will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the save directory exists\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for var in columns:\n",
        "        if var in df.columns:\n",
        "            # Compute descriptive statistics and additional metrics\n",
        "            grouped_stats = df.groupby(Conditions)[var].describe()\n",
        "\n",
        "            # Round all values to 3 decimal places\n",
        "            grouped_stats = grouped_stats.round(3)\n",
        "\n",
        "            # Save the summary to a CSV file\n",
        "            csv_filename = f\"{var}_summary.csv\"\n",
        "            grouped_stats.to_csv(os.path.join(save_path, csv_filename))\n",
        "            print(f\"Saved statistical summary for {var} to {csv_filename}\")\n",
        "\n",
        "            # Convert DataFrame to list of lists for PDF table\n",
        "            table_data = [list(row) for row in grouped_stats.itertuples()]\n",
        "            column_names = [\"Condition\"] + list(grouped_stats.columns)\n",
        "\n",
        "            # Save the table as a nicely formatted PDF\n",
        "            pdf_filename = os.path.join(save_path, f\"{var}_summary.pdf\")\n",
        "            save_table_as_pdf(table_data, column_names, pdf_filename)\n",
        "            print(f\"Saved statistical summary for {var} to {pdf_filename}\")\n",
        "\n",
        "# Generate and save statistics as CSV and PDF\n",
        "generate_display_and_save_statistics(df_to_plot, selectable_columns, Conditions, save_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jpu1Qk7VSaI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}