{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **PDAC CellTracksColab - Count arrested tracks**\n",
        "---\n",
        "\n",
        "<font size = 4>The CellTracksColab notebooks have been adapted to analyze tracking data featured in the manuscript \"Quantitative analysis of pancreatic cancer cell attachment to endothelial cells.\" This suite of notebooks is part of the CellTracksColab project, designed to facilitate comprehensive analyses of cell tracking data. The project's resources are accessible via the GitHub repository provided below.\n",
        "\n",
        "<font size = 4>The CellTracksColab project repository: [CellMigrationLab/CellTracksColab](https://github.com/CellMigrationLab/CellTracksColab).\n",
        "\n",
        "<font size = 4>This notebook is designed to process dataframes generated by the General Notebook within the CellTracksColab framework. It focuses on analyzing cell tracking data by identifying and counting the number of tracks that exhibit arrest behavior at each time point throughout the observation period. Key functionalities of this notebook include:\n",
        "\n",
        "- **Loading CellTracksColab Dataframes:** It starts by importing the comprehensive dataframes prepared by the General Notebook, ensuring a seamless transition from data compilation to detailed analysis.\n",
        "\n",
        "- **Counting Arrested Tracks:** The notebook identifies tracks where cells have ceased to move, categorizing them as 'arrested'. This allows for a quantitative analysis of cell arrest dynamics over time.\n",
        "\n",
        "- **Computing Attachment Rates:** Beyond counting arrested tracks, it calculates the rate of cell attachment over the observation period. This metric is crucial for understanding how PDAC cells interact with endothelial cells over time.\n",
        "\n",
        "- **Generating Attachment Plots:** Visual representation of attachment rates and arrested cell counts are produced, offering intuitive insights into the temporal dynamics of cell behavior.\n",
        "\n",
        "<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1: Prepare the session and load your data**\n",
        "--------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rAP0ahCzn1V6"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install\n",
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install plotly\n",
        "!pip -q install tqdm\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import itertools\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import requests\n",
        "\n",
        "\n",
        "#----------------------- Key functions -----------------------------#\n",
        "\n",
        "# Function to calculate Cohen's d\n",
        "def cohen_d(group1, group2):\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var()\n",
        "    var2 = group2.var()\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
        "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
        "\n",
        "    # Estimating the number of chunks based on the provided chunk size\n",
        "    num_chunks = int(len(df) / chunk_size) + 1\n",
        "\n",
        "    # Create a tqdm instance for progress tracking\n",
        "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
        "        # Open the file for writing\n",
        "        with open(path, \"w\") as f:\n",
        "            # Write the header once at the beginning\n",
        "            df.head(0).to_csv(f, index=False)\n",
        "\n",
        "            for chunk in np.array_split(df, num_chunks):\n",
        "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "\n",
        "def check_for_nans(df, df_name):\n",
        "    \"\"\"\n",
        "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
        "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
        "    \"\"\"\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            nan_count = df[col].isna().sum()\n",
        "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"No NaN values found in {df_name}.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDAwkSOo1gV"
      },
      "source": [
        "## **1.3. Compile your data or load existing dataframes**\n",
        "---\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQKXq3giI3nX"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Provide the path to your dataset (chunk):\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "#@markdown ###You have existing dataframes, provide the path to your:\n",
        "\n",
        "Track_table = ''  # @param {type: \"string\"}\n",
        "Spot_table = ''  # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ###Provide the path to your Result folder\n",
        "\n",
        "Results_Folder = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "if not Results_Folder:\n",
        "    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n",
        "\n",
        "if not os.path.exists(Results_Folder):\n",
        "    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Print the location of the result folder\n",
        "print(f\"Result folder is located at: {Results_Folder}\")\n",
        "\n",
        "def validate_tracks_df(df):\n",
        "    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n",
        "    required_columns = ['TRACK_ID']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n",
        "            return False\n",
        "\n",
        "    # Additional data type checks or value ranges can be added here\n",
        "    return True\n",
        "\n",
        "def validate_spots_df(df):\n",
        "    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n",
        "    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n",
        "            return False\n",
        "\n",
        "    # Additional data type checks or value ranges can be added here\n",
        "    return True\n",
        "\n",
        "def check_unique_id_match(df1, df2):\n",
        "    df1_ids = set(df1['Unique_ID'])\n",
        "    df2_ids = set(df2['Unique_ID'])\n",
        "\n",
        "    # Check if the IDs in the two dataframes match\n",
        "    if df1_ids == df2_ids:\n",
        "        print(\"The Unique_ID values in both dataframes match perfectly!\")\n",
        "    else:\n",
        "        missing_in_df1 = df2_ids - df1_ids\n",
        "        missing_in_df2 = df1_ids - df2_ids\n",
        "\n",
        "        if missing_in_df1:\n",
        "            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n",
        "            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n",
        "\n",
        "        if missing_in_df2:\n",
        "            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n",
        "            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n",
        "\n",
        "# For existing dataframes\n",
        "if Track_table:\n",
        "    print(\"Loading track table file....\")\n",
        "    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n",
        "    if not validate_tracks_df(merged_tracks_df):\n",
        "        print(\"Error: Validation failed for loaded tracks dataframe.\")\n",
        "\n",
        "if Spot_table:\n",
        "    print(\"Loading spot table file....\")\n",
        "    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n",
        "    if not validate_spots_df(merged_spots_df):\n",
        "        print(\"Error: Validation failed for loaded spots dataframe.\")\n",
        "\n",
        "check_for_nans(merged_spots_df, \"merged_spots_df\")\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CpBVp_H-Ee0c"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check Metadata\n",
        "\n",
        "\n",
        "# Define the metadata columns that are expected to have identical values for each filename\n",
        "metadata_columns = ['Cells', 'Flow_speed', 'ILbeta', 'Condition', 'experiment_nb', 'Repeat']\n",
        "\n",
        "# Group the DataFrame by 'File_name' and then check if all entries within each group are identical\n",
        "consistent_metadata = True\n",
        "for name, group in merged_tracks_df.groupby('File_name'):\n",
        "    for col in metadata_columns:\n",
        "        if not group[col].nunique() == 1:\n",
        "            consistent_metadata = False\n",
        "            print(f\"Inconsistency found for file: {name} in column: {col}\")\n",
        "            break  # Stop checking other columns for this group and move to the next file\n",
        "    if not consistent_metadata:\n",
        "        break  # Stop the entire process if any inconsistency is found\n",
        "\n",
        "if consistent_metadata:\n",
        "    print(\"All files have consistent metadata across the specified columns.\")\n",
        "else:\n",
        "    print(\"There are inconsistencies in the metadata. Please check the output for details.\")\n",
        "\n",
        "# Drop duplicates based on the 'File_name' to get a unique list of filenames and their metadata\n",
        "unique_files_df = merged_tracks_df.drop_duplicates(subset=['File_name'])[['File_name', 'Cells', 'Flow_speed', 'ILbeta', 'Condition', 'experiment_nb', 'Repeat']]\n",
        "\n",
        "# Reset the index to clean up the DataFrame\n",
        "unique_files_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the resulting DataFrame in a nicely formatted HTML table\n",
        "unique_files_df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it already contains 'Conditions' and 'Repeats' columns.\n",
        "\n",
        "# Group by 'Conditions' and 'Repeats' and count the occurrences\n",
        "grouped = unique_files_df.groupby(['Condition', 'Repeat']).size().reset_index(name='counts')\n",
        "\n",
        "# Check if any combinations have a count greater than 1, which means they are not unique\n",
        "non_unique_combinations = grouped[grouped['counts'] > 1]\n",
        "\n",
        "# Print the non-unique combinations\n",
        "if not non_unique_combinations.empty:\n",
        "    print(\"There are non-unique combinations of Conditions and Repeats:\")\n",
        "    print(non_unique_combinations)\n",
        "else:\n",
        "    print(\"All combinations of Conditions and Repeats are unique.\")\n",
        "\n",
        "check_unique_id_match(merged_spots_df, merged_tracks_df)\n",
        "\n",
        "\n",
        "# Group the DataFrame by 'Cells', 'ILbeta', 'Repeat' and then check if there are 4 unique 'Flow_speed' values for each group\n",
        "consistent_flow_speeds = True\n",
        "for (cells, ilbeta, repeat), group in merged_tracks_df.groupby(['Cells', 'ILbeta', 'Repeat']):\n",
        "    if group['Flow_speed'].nunique() != 4:\n",
        "        consistent_flow_speeds = False\n",
        "        print(f\"Inconsistency found for Cells: {cells}, ILbeta: {ilbeta}, Repeat: {repeat} - Expected 4 Flow_speeds, found {group['Flow_speed'].nunique()}\")\n",
        "        break  # Stop the entire process if any inconsistency is found\n",
        "\n",
        "if consistent_flow_speeds:\n",
        "    print(\"Each combination of 'Cells', 'ILbeta', 'Repeat' has exactly 4 different 'Flow_speed' values.\")\n",
        "else:\n",
        "    print(\"There are inconsistencies in 'Flow_speed' values. Please check the output for details.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5YJ9V468HwJ"
      },
      "source": [
        "## **1.4. Filter tracks shorter than 50 spots**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8GBhFy8vd6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ##Filter tracks shorter than 50 spots\n",
        "\n",
        "\n",
        "merged_tracks_df = merged_tracks_df[merged_tracks_df['NUMBER_SPOTS'] >= 50]\n",
        "merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52STmnv43d45"
      },
      "source": [
        "## **1.5. Visualise your tracks**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AE881uJW5ukQ"
      },
      "outputs": [],
      "source": [
        "# @title ##Run the cell and choose the file you want to inspect\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
        "    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# Create a Dropdown widget with the filenames\n",
        "filename_dropdown = widgets.Dropdown(\n",
        "    options=filenames,\n",
        "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
        "    description='File Name:',\n",
        ")\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "# Link the Dropdown widget to the plotting function\n",
        "interact(plot_coordinates, filename=filename_dropdown)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 2: Count number of arrested tracks per time points**\n",
        "--------------------------------------------------------"
      ],
      "metadata": {
        "id": "bKWEyCSTUqos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uSAPhbQ1gWpI"
      },
      "outputs": [],
      "source": [
        "# @title #Count number of tracks per time points\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Define the window size for rolling average\n",
        "window_size = 20  # Adjust this value as needed\n",
        "\n",
        "speed_threshold = 5\n",
        "\n",
        "# Check and create necessary directories\n",
        "if not os.path.exists(f\"{Results_Folder}/Track_Counts\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Track_Counts\")\n",
        "\n",
        "# Modified function to count slow tracks with rolling average\n",
        "def count_slow_tracks(spots_df, speed_threshold):\n",
        "    slow_tracks = spots_df[spots_df['Speed'] < speed_threshold]\n",
        "    count_df = slow_tracks.groupby(['Condition','Cells','Flow_speed','ILbeta', 'Repeat', 'POSITION_T'])['Unique_ID'].nunique().reset_index()\n",
        "        # Sort the DataFrame by 'POSITION_T' within each group\n",
        "    count_df.sort_values(by=['Condition','Cells','Flow_speed','ILbeta', 'Repeat', 'POSITION_T'], inplace=True)\n",
        "    count_df['Slow_Track_Count_Rolling'] = count_df.groupby(['Condition','Cells','Flow_speed','ILbeta', 'Repeat'])['Unique_ID'].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
        "    return count_df\n",
        "\n",
        "# Modified function to count total tracks with rolling average\n",
        "def count_total_tracks(spots_df):\n",
        "    total_count_df = spots_df.groupby(['Condition','Cells','Flow_speed','ILbeta', 'Repeat', 'POSITION_T'])['Unique_ID'].nunique().reset_index()\n",
        "    total_count_df.sort_values(by=['Condition','Cells','Flow_speed','ILbeta', 'Repeat', 'POSITION_T'], inplace=True)\n",
        "    total_count_df['Total_Track_Count_Rolling'] = total_count_df.groupby(['Condition','Cells','Flow_speed','ILbeta', 'Repeat'])['Unique_ID'].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
        "    return total_count_df\n",
        "\n",
        "# Plotting function with percentages and save figures as PDF\n",
        "def plot_tracks_with_percentage(count_df, total_count_df, window_size):\n",
        "    conditions = count_df['Condition'].unique()\n",
        "    repeats = count_df['Repeat'].unique()\n",
        "    fit_results = []\n",
        "\n",
        "    for condition in conditions:\n",
        "        for repeat in repeats:\n",
        "            condition_repeat_df = count_df[(count_df['Condition'] == condition) & (count_df['Repeat'] == repeat)]\n",
        "\n",
        "            if not condition_repeat_df.empty:\n",
        "                merged_df = pd.merge(\n",
        "                    total_count_df[(total_count_df['Condition'] == condition) & (total_count_df['Repeat'] == repeat)],\n",
        "                    condition_repeat_df,\n",
        "                    on=['Condition', 'Repeat', 'POSITION_T'],\n",
        "                    how='left',\n",
        "                    suffixes=('_total', '_slow')\n",
        "                ).fillna(0)\n",
        "\n",
        "                # Determine the start and end times based on window size\n",
        "                start_time = merged_df['POSITION_T'].min() + window_size -1\n",
        "                end_time = merged_df['POSITION_T'].max() - window_size -1\n",
        "\n",
        "                # Trim data to the specified time window\n",
        "                trimmed_df = merged_df[(merged_df['POSITION_T'] >= start_time) & (merged_df['POSITION_T'] <= end_time)]\n",
        "\n",
        "                percentage_slow = (trimmed_df['Slow_Track_Count_Rolling'] / trimmed_df['Total_Track_Count_Rolling']) * 100\n",
        "                m, b = np.polyfit(trimmed_df['POSITION_T'], trimmed_df['Slow_Track_Count_Rolling'], 1)\n",
        "                y_fit = m * trimmed_df['POSITION_T'] + b\n",
        "                r2 = r2_score(trimmed_df['Slow_Track_Count_Rolling'], y_fit)\n",
        "\n",
        "                fit_results.append({'Condition': condition, 'Repeat': repeat, 'm': m, 'b': b, 'R2': r2})\n",
        "\n",
        "                # Plotting\n",
        "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "                sns.lineplot(data=trimmed_df, x='POSITION_T', y='Slow_Track_Count_Rolling', ax=ax1, label='Slow Tracks')\n",
        "                sns.lineplot(data=trimmed_df, x='POSITION_T', y='Total_Track_Count_Rolling', ax=ax1, label='Total Tracks')\n",
        "                ax1.plot(trimmed_df['POSITION_T'], y_fit, '--', label=f'Linear fit for Slow Tracks: y = {m:.2f}x + {b:.2f}')\n",
        "                ax1.set_title(f'Number of Tracks for {condition} - Repeat {repeat}\\nR^2 for Slow Tracks = {r2:.2f}')\n",
        "\n",
        "                sns.lineplot(data=trimmed_df, x='POSITION_T', y=percentage_slow, ax=ax2, label='% Slow Tracks')\n",
        "                ax2.set_title(f'Percentage of Slow Tracks for {condition} - Repeat {repeat}')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(Results_Folder + f'/Track_Counts/{condition}_Repeat{repeat}_plot.pdf', format='pdf')\n",
        "                plt.show()\n",
        "\n",
        "    return pd.DataFrame(fit_results)\n",
        "\n",
        "\n",
        "# Use the functions\n",
        "count_df = count_slow_tracks(merged_spots_df, speed_threshold)\n",
        "total_count_df = count_total_tracks(merged_spots_df)\n",
        "\n",
        "# Save dataframes\n",
        "save_dataframe_with_progress(count_df, Results_Folder + '/Track_Counts/' +\"slow_tracks_count.csv\", desc=\"Saving slow tracks count\")\n",
        "save_dataframe_with_progress(total_count_df, Results_Folder + '/Track_Counts/' +\"total_tracks_count.csv\", desc=\"Saving total tracks count\")\n",
        "\n",
        "# Plot and save fit results\n",
        "fit_df = plot_tracks_with_percentage(count_df, total_count_df, window_size)\n",
        "save_dataframe_with_progress(fit_df, Results_Folder + '/Track_Counts/' +\"fit_results.csv\", desc=\"Saving fit results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O2OzeG6Kkeza"
      },
      "outputs": [],
      "source": [
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# @title #Plot the fits\n",
        "\n",
        "def plot_combined_fits(fit_df, save_path):\n",
        "    conditions = fit_df['Condition'].unique()\n",
        "\n",
        "    # Create a PDF file for the plots\n",
        "    pdf_filename = \"combined_fits.pdf\"\n",
        "    pdf_filepath = os.path.join(save_path, pdf_filename)\n",
        "    pdf = PdfPages(pdf_filepath)\n",
        "\n",
        "    for condition in conditions:\n",
        "        condition_df = fit_df[fit_df['Condition'] == condition]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "        for index, row in condition_df.iterrows():\n",
        "            x_vals = np.linspace(0, 100, 400)  # Modify as needed\n",
        "            y_vals = row['m'] * x_vals + row['b']\n",
        "\n",
        "            ax.plot(x_vals, y_vals, label=f'Fit for {row[\"Repeat\"]}: y = {row[\"m\"]:.2f}x + {row[\"b\"]:.2f}')\n",
        "\n",
        "        ax.set_title(f'Combined Fits for {condition}')\n",
        "        ax.legend(loc='upper left')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the current plot to the PDF\n",
        "        pdf.savefig(fig)\n",
        "        plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Close the PDF object\n",
        "    pdf.close()\n",
        "    print(f\"All combined fit plots saved to {pdf_filepath}\")\n",
        "\n",
        "# Example usage\n",
        "save_path = Results_Folder + '/Track_Counts/'\n",
        "plot_combined_fits(fit_df, save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O1nSZxHXpq8Q"
      },
      "outputs": [],
      "source": [
        "# @title #Plot the fits as bars\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Barplot to display the mean value for each condition\n",
        "sns.barplot(data=fit_df, x='Condition', y='m', color='lightgray', estimator=np.mean, errorbar=None, alpha=0.7, label='Mean')\n",
        "\n",
        "# Swarmplot to show individual repeats for each condition\n",
        "filtered_df = fit_df[fit_df['m'] >= 0]\n",
        "sns.stripplot(data=filtered_df, x='Condition', y='m', hue='Repeat', palette=\"Set3\", size=7)\n",
        "\n",
        "# Annotating the bars with the mean values\n",
        "for index, value in enumerate(fit_df.groupby('Condition')['m'].mean()):\n",
        "    if value >= 0:  # Only annotate bars with non-negative means\n",
        "        plt.text(index, value + 0.01, f'{value:.2f}', ha='center')\n",
        "\n",
        "plt.title(\"Variation of 'm' value for each condition\")\n",
        "plt.ylabel(\"Slope (m)\")\n",
        "plt.xlabel(\"Condition\")\n",
        "plt.legend(title='Legend')\n",
        "\n",
        "# Set y-axis to start from 0\n",
        "plt.ylim(bottom=0)\n",
        "\n",
        "# Rotate x-axis labels\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout to make room for the x-tick labels\n",
        "plt.gcf().subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "\n",
        "plt.savefig(Results_Folder + '/Track_Counts/m_value_variation_per_condition_combined.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QWs5GLHa6Drk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# @title #Plot the profiles together\n",
        "\n",
        "window_size = 10  # Adjust this value as needed\n",
        "\n",
        "# Define the order for 'Flow_speed'\n",
        "flow_speed_order = [\"300\", \"200\", \"100\", \"wash\"]\n",
        "\n",
        "# Convert 'Flow_speed' to string\n",
        "count_df['Flow_speed'] = count_df['Flow_speed'].astype(str)\n",
        "\n",
        "count_df['unique_combinations'] = count_df['Cells'].astype(str) + '_' + count_df['ILbeta'].astype(str) + '_' + count_df['Repeat'].astype(str)\n",
        "\n",
        "# Function to create POSITION_T_REPEAT values\n",
        "def create_position_t_repeat(group):\n",
        "    # Apply custom sorting within the group\n",
        "    group = group.copy()\n",
        "    group['Flow_speed'] = pd.Categorical(group['Flow_speed'], categories=flow_speed_order, ordered=True)\n",
        "    group = group.sort_values(by=['Flow_speed', 'POSITION_T'])\n",
        "\n",
        "    # Calculate POSITION_T_REPEAT\n",
        "    position_t_repeat = 0.04\n",
        "    for i in range(len(group)):\n",
        "        if i != 0:\n",
        "            position_t_repeat += 0.04\n",
        "        group.iat[i, group.columns.get_loc('POSITION_T_REPEAT')] = position_t_repeat\n",
        "\n",
        "    return group\n",
        "\n",
        "# Add a placeholder column for POSITION_T_REPEAT\n",
        "count_df['POSITION_T_REPEAT'] = 0\n",
        "\n",
        "# Apply the function to each unique combination of 'Cells', 'ILbeta', 'Repeat'\n",
        "updated_dfs = []\n",
        "unique_combinations = count_df[['Cells', 'ILbeta', 'Repeat']].drop_duplicates()\n",
        "for _, row in unique_combinations.iterrows():\n",
        "    combo_df = count_df[(count_df['Cells'] == row['Cells']) &\n",
        "                        (count_df['ILbeta'] == row['ILbeta']) &\n",
        "                        (count_df['Repeat'] == row['Repeat'])]\n",
        "    updated_combo_df = create_position_t_repeat(combo_df)\n",
        "    updated_dfs.append(updated_combo_df)\n",
        "\n",
        "# Concatenate the updated DataFrames\n",
        "count_df = pd.concat(updated_dfs, ignore_index=True)\n",
        "\n",
        "# Function to apply rolling average on 'Unique_ID' column\n",
        "def apply_rolling_average(group):\n",
        "    group['Unique_ID_Rolling'] = group['Unique_ID'].rolling(window=window_size, min_periods=1).mean()\n",
        "    return group\n",
        "\n",
        "# Apply the rolling average function to each group\n",
        "count_df = count_df.groupby('unique_combinations', group_keys=False).apply(apply_rolling_average)\n",
        "\n",
        "save_dataframe_with_progress(count_df, Results_Folder + '/Track_Counts/' +\"slow_tracks_count.csv\", desc=\"Saving slow tracks count\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "\n",
        "# Get unique combinations\n",
        "unique_combinations = count_df['unique_combinations'].unique()\n",
        "\n",
        "# Create a PDF file for the plots\n",
        "pdf_filename = \"Profiles_together.pdf\"\n",
        "pdf_filepath = os.path.join(f\"{Results_Folder}/Track_Counts/\", pdf_filename)\n",
        "pdf = PdfPages(pdf_filepath)\n",
        "\n",
        "# Loop through each unique combination and plot\n",
        "for combination in unique_combinations:\n",
        "    # Filter data for the specific combination\n",
        "    combo_df = count_df[count_df['unique_combinations'] == combination]\n",
        "\n",
        "    # Define the filename based on the combination\n",
        "    filename = f\"{combination}_data.csv\"\n",
        "    filepath = os.path.join(f\"{Results_Folder}/Track_Counts/\", filename)\n",
        "\n",
        "    # Save the dataframe\n",
        "    save_dataframe_with_progress(combo_df, filepath)\n",
        "    print(f\"Dataframe for {combination} saved to {filepath}\")\n",
        "\n",
        "    # Create a new figure for each plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=combo_df, x='POSITION_T_REPEAT', y='Unique_ID_Rolling', label=combination)\n",
        "    plt.title(f'Track Count over Time - {combination}')\n",
        "    plt.xlabel('Time (Continuous)')\n",
        "    plt.ylabel('Number of Tracks')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the current plot to the PDF\n",
        "    pdf.savefig()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Close the PDF object\n",
        "pdf.close()\n",
        "print(f\"All plots saved to {pdf_filepath}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LReJr1Z1CQJs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# @title #Make the attachment profiles the same length\n",
        "\n",
        "# Function to adjust series length\n",
        "def adjust_series_length(group, target_length=8750, interval=0.04, window_size=10):\n",
        "    # Sort by POSITION_T_REPEAT\n",
        "    group = group.sort_values(by='POSITION_T_REPEAT')\n",
        "\n",
        "    # Metadata columns to be copied\n",
        "    metadata_columns = ['Cells', 'ILbeta', 'Repeat', 'Flow_speed', 'Condition','unique_combinations']  # Add other relevant columns here\n",
        "\n",
        "    # Check the length of the current series\n",
        "    current_length = len(group)\n",
        "\n",
        "    if current_length > target_length:\n",
        "        # If the series is too long, trim it\n",
        "        return group.head(target_length)\n",
        "    elif current_length < target_length:\n",
        "        # If the series is too short, evenly distribute and fill\n",
        "        distributed_indices = np.linspace(0, target_length - 1, len(group)).astype(int)\n",
        "        new_df = pd.DataFrame({'POSITION_T_REPEAT': np.arange(0, target_length) * interval})\n",
        "        new_df['Slow_Track_Count_Rolling'] = np.nan  # Initialize with NaN\n",
        "        new_df.loc[distributed_indices, 'Slow_Track_Count_Rolling'] = group['Slow_Track_Count_Rolling'].values\n",
        "        new_df['Slow_Track_Count_Rolling'] = new_df['Slow_Track_Count_Rolling'].rolling(window=window_size, min_periods=1, center=True).mean()\n",
        "\n",
        "        new_df['Unique_ID'] = np.nan  # Initialize with NaN\n",
        "        new_df.loc[distributed_indices, 'Unique_ID'] = group['Unique_ID'].values\n",
        "        new_df['Unique_ID'] = new_df['Unique_ID'].rolling(window=window_size, min_periods=1, center=True).mean()\n",
        "\n",
        "        new_df['Unique_ID_Rolling'] = np.nan  # Initialize with NaN\n",
        "        new_df.loc[distributed_indices, 'Unique_ID_Rolling'] = group['Unique_ID_Rolling'].values\n",
        "        new_df['Unique_ID_Rolling'] = new_df['Unique_ID_Rolling'].rolling(window=window_size, min_periods=1, center=True).mean()\n",
        "\n",
        "        # Copy metadata columns\n",
        "        for col in metadata_columns:\n",
        "            new_df[col] = group[col].iloc[0]\n",
        "\n",
        "        return new_df\n",
        "    else:\n",
        "        # If the series is already the right length, return as is\n",
        "        return group\n",
        "\n",
        "# Apply the function to each unique combination\n",
        "adjusted_dfs = []\n",
        "for unique_combination in count_df['unique_combinations'].unique():\n",
        "    group = count_df[count_df['unique_combinations'] == unique_combination]\n",
        "    adjusted_df = adjust_series_length(group)\n",
        "    adjusted_dfs.append(adjusted_df)\n",
        "\n",
        "# Combine all adjusted dataframes\n",
        "adjusted_df = pd.concat(adjusted_dfs, ignore_index=True)\n",
        "\n",
        "# Check for NaNs in the adjusted dataframe\n",
        "def check_for_nans(df, df_name):\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "    if nan_columns:\n",
        "        print(f\"NaNs found in {df_name} in columns: {nan_columns}\")\n",
        "    else:\n",
        "        print(f\"No NaNs found in {df_name}.\")\n",
        "\n",
        "\n",
        "# Drop 'POSITION_T' column from adjusted_df\n",
        "adjusted_df.drop(columns='POSITION_T', inplace=True)\n",
        "\n",
        "# Group by unique_combinations, sort by POSITION_T_REPEAT, and renumber\n",
        "adjusted_df = adjusted_df.groupby('unique_combinations', group_keys=False).apply(lambda x: x.sort_values(by='POSITION_T_REPEAT'))\n",
        "adjusted_df['POSITION_T_REPEAT'] = adjusted_df.groupby('unique_combinations').cumcount() * 0.04\n",
        "\n",
        "check_for_nans(adjusted_df, \"adjusted_df\")\n",
        "\n",
        "save_dataframe_with_progress(adjusted_df, Results_Folder + '/Track_Counts/' +\"slow_tracks_count_adjusted.csv\", desc=\"Saving slow tracks count adjusted\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot all original and interpolated time series in a single plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Loop through each unique combination and add both original and interpolated series to the plot\n",
        "for unique_id in count_df['unique_combinations'].unique():\n",
        "    original_series = count_df[count_df['unique_combinations'] == unique_id]\n",
        "    processed_series = adjusted_df[adjusted_df['unique_combinations'] == unique_id]\n",
        "\n",
        "    # Plot original series\n",
        "    sns.lineplot(data=original_series, x='POSITION_T_REPEAT', y='Unique_ID_Rolling', label=f'{unique_id} Original', linestyle='--')\n",
        "\n",
        "    # Plot interpolated series\n",
        "    sns.lineplot(data=processed_series, x='POSITION_T_REPEAT', y='Unique_ID_Rolling', label=f'{unique_id} Interpolated')\n",
        "\n",
        "plt.title('Original vs Interpolated Time Series for All Combinations')\n",
        "plt.xlabel('POSITION_T_REPEAT')\n",
        "plt.ylabel('Slow_Track_Count_Rolling')\n",
        "plt.legend(title='Series Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uAu7HcfcZOLz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# @title #Plot altogether\n",
        "\n",
        "\n",
        "# Get unique combinations of 'Cells' and 'ILbeta'\n",
        "unique_cells_ilbeta = adjusted_df[['Cells', 'ILbeta']].drop_duplicates()\n",
        "\n",
        "# Adjust figure size and layout\n",
        "fig, ax = plt.subplots(figsize=(12, 8))  # Adjusted figure size\n",
        "\n",
        "for _, row in unique_cells_ilbeta.iterrows():\n",
        "    cells, ilbeta = row['Cells'], row['ILbeta']\n",
        "    combo_df = adjusted_df[(adjusted_df['Cells'] == cells) & (adjusted_df['ILbeta'] == ilbeta)]\n",
        "\n",
        "    filename = f\"{cells}_{ilbeta}_data.csv\"\n",
        "    filepath = os.path.join(Results_Folder + '/Track_Counts/', filename)\n",
        "    combo_df.to_csv(filepath, index=False)\n",
        "    print(f\"Dataframe for {cells}, {ilbeta} saved to {filepath}\")\n",
        "\n",
        "    sns.lineplot(data=combo_df, x='POSITION_T_REPEAT', y='Unique_ID_Rolling', label=f\"{cells}, {ilbeta}\", errorbar=\"se\")\n",
        "\n",
        "# Manually adjust y-axis limits\n",
        "current_ylim = ax.get_ylim()\n",
        "ax.set_ylim(current_ylim[0], current_ylim[1] * 1.1)\n",
        "\n",
        "# Add horizontal lines for different Flow_speed segments\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=0, xmax=87, colors='gray', linestyles='solid', lw=5)\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=88, xmax=175, colors='gray', linestyles='solid', lw=5)\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=176, xmax=263, colors='gray', linestyles='solid', lw=5)\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=264, xmax=350, colors='gray', linestyles='solid', lw=5)\n",
        "\n",
        "ax.text(40, current_ylim[1]*1.03, '300', horizontalalignment='center')\n",
        "ax.text(130, current_ylim[1]*1.03, '200', horizontalalignment='center')\n",
        "ax.text(220, current_ylim[1]*1.03, '100', horizontalalignment='center')\n",
        "ax.text(310, current_ylim[1]*1.03, 'Wash', horizontalalignment='center')\n",
        "\n",
        "ax.set_title('Track Count over Time')\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Number of Tracks')\n",
        "\n",
        "# Place the legend outside the plot on the right\n",
        "ax.legend(title='Conditions', loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot as a PDF\n",
        "plt.savefig(pdf_filepath)\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(f\"Plot saved to {pdf_filepath}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 3: Filter and plot your data**\n",
        "--------------------------------------------------------"
      ],
      "metadata": {
        "id": "87sHyVUjS-ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# @title #Filter the data\n",
        "\n",
        "count_df = adjusted_df\n",
        "\n",
        "\n",
        "# Global variables to store the selected options\n",
        "global filtered_df\n",
        "filtered_df = pd.DataFrame()\n",
        "\n",
        "global selected_cells, selected_speeds, selected_ilbetas\n",
        "selected_cells, selected_speeds, selected_ilbetas = [], [], []\n",
        "\n",
        "# Function to summarize selected options into a string\n",
        "def summarize_options(options):\n",
        "    return \"_\".join([str(option) for option in options if option])  # Filters out any 'falsy' values like empty strings or None\n",
        "\n",
        "# Function to create a filename based on selected options\n",
        "def create_filename(selected_cells, selected_speeds, selected_ilbetas):\n",
        "    # Join the summarized options for each parameter with an underscore\n",
        "    selected_options = \"_\".join([\n",
        "        summarize_options(selected_cells),\n",
        "        summarize_options(selected_speeds),\n",
        "        summarize_options(selected_ilbetas)\n",
        "    ])\n",
        "\n",
        "    # Replace spaces with underscores and return the filename\n",
        "    filename = f\"{selected_options}\"\n",
        "    return filename.replace(\" \", \"_\")\n",
        "\n",
        "# Create checkboxes for each category\n",
        "cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in count_df['Cells'].unique()]\n",
        "flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in count_df['Flow_speed'].unique()]\n",
        "ilbeta_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in count_df['ILbeta'].unique()]\n",
        "\n",
        "# Function to filter dataframe and update global variables based on selected checkbox values\n",
        "def filter_dataframe(button):\n",
        "    global filtered_df, selected_cells, selected_speeds, selected_ilbetas\n",
        "\n",
        "    # Trim whitespace and correct cases if necessary\n",
        "    count_df['Cells'] = count_df['Cells'].str.strip()\n",
        "    count_df['Flow_speed'] = count_df['Flow_speed'].str.strip()\n",
        "    count_df['ILbeta'] = count_df['ILbeta'].str.strip()\n",
        "\n",
        "    selected_cells = [box.description for box in cells_checkboxes if box.value]\n",
        "    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n",
        "    selected_ilbetas = [box.description for box in ilbeta_checkboxes if box.value]\n",
        "\n",
        "    # Debugging output\n",
        "    print(\"Selected Cells:\", selected_cells)\n",
        "    print(\"Selected Speeds:\", selected_speeds)\n",
        "    print(\"Selected ILbetas:\", selected_ilbetas)\n",
        "    print(\"Original DF length:\", len(count_df))\n",
        "\n",
        "    filtered_df = count_df[\n",
        "        (count_df['Cells'].isin(selected_cells)) &\n",
        "        (count_df['Flow_speed'].isin(selected_speeds)) &\n",
        "        (count_df['ILbeta'].isin(selected_ilbetas))\n",
        "    ]\n",
        "\n",
        "    # More debugging output\n",
        "    print(\"Filtered DF length:\", len(filtered_df))\n",
        "    if len(filtered_df) == 0:\n",
        "        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n",
        "        print(\"Unique 'Cells' in DataFrame:\", count_df['Cells'].unique())\n",
        "        print(\"Unique 'Flow_speed' in DataFrame:\", count_df['Flow_speed'].unique())\n",
        "        print(\"Unique 'ILbeta' in DataFrame:\", count_df['ILbeta'].unique())\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Now call the filter function or trigger the button to filter the dataframe and see the output.\n",
        "\n",
        "\n",
        "# Button to trigger dataframe filtering\n",
        "filter_button = widgets.Button(description=\"Filter Dataframe\")\n",
        "filter_button.on_click(filter_dataframe)\n",
        "\n",
        "# Display checkboxes and button\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Select Cells:'),\n",
        "    widgets.HBox(cells_checkboxes),\n",
        "    widgets.Label('Select Flow Speed:'),\n",
        "    widgets.HBox(flow_speed_checkboxes),\n",
        "    widgets.Label('Select ILbeta:'),\n",
        "    widgets.HBox(ilbeta_checkboxes),\n",
        "    filter_button\n",
        "]))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5SPbRERi1VLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# @title #Plot selected conditions\n",
        "\n",
        "# Check and create necessary directories\n",
        "if not os.path.exists(f\"{Results_Folder}/Track_Counts\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Track_Counts\")\n",
        "\n",
        "filename = create_filename(selected_cells, selected_speeds, selected_ilbetas)\n",
        "\n",
        "\n",
        "pdf_filepath = os.path.join(Results_Folder + '/Track_Counts/', filename+'_plot.pdf')\n",
        "\n",
        "# Get unique combinations of 'Cells' and 'ILbeta'\n",
        "unique_cells_ilbeta = filtered_df[['Cells', 'ILbeta']].drop_duplicates()\n",
        "\n",
        "# Adjust figure size and layout\n",
        "fig, ax = plt.subplots(figsize=(12, 8))  # Adjusted figure size\n",
        "\n",
        "for _, row in unique_cells_ilbeta.iterrows():\n",
        "    cells, ilbeta = row['Cells'], row['ILbeta']\n",
        "    combo_df = filtered_df[(filtered_df['Cells'] == cells) & (filtered_df['ILbeta'] == ilbeta)]\n",
        "\n",
        "    filepath = os.path.join(Results_Folder + '/Track_Counts/', filename +'_data.csv')\n",
        "    combo_df.to_csv(filepath, index=False)\n",
        "    print(f\"Dataframe for {cells}, {ilbeta} saved to {filepath}\")\n",
        "\n",
        "    sns.lineplot(data=combo_df, x='POSITION_T_REPEAT', y='Unique_ID_Rolling', label=f\"{cells}, {ilbeta}\", errorbar=\"se\")\n",
        "\n",
        "# Manually adjust y-axis limits\n",
        "current_ylim = ax.get_ylim()\n",
        "ax.set_ylim(current_ylim[0], current_ylim[1] * 1.1)\n",
        "\n",
        "# Add horizontal lines for different Flow_speed segments\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=0, xmax=87, colors='gray', linestyles='solid', lw=5)\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=88, xmax=175, colors='gray', linestyles='solid', lw=5)\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=176, xmax=263, colors='gray', linestyles='solid', lw=5)\n",
        "ax.hlines(y=current_ylim[1]*1.00, xmin=264, xmax=350, colors='gray', linestyles='solid', lw=5)\n",
        "\n",
        "ax.text(40, current_ylim[1]*1.03, '300', horizontalalignment='center')\n",
        "ax.text(130, current_ylim[1]*1.03, '200', horizontalalignment='center')\n",
        "ax.text(220, current_ylim[1]*1.03, '100', horizontalalignment='center')\n",
        "ax.text(310, current_ylim[1]*1.03, 'Wash', horizontalalignment='center')\n",
        "\n",
        "ax.set_title('Track Count over Time')\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Number of Tracks')\n",
        "\n",
        "# Place the legend outside the plot on the right\n",
        "ax.legend(title='Conditions', loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot as a PDF\n",
        "plt.savefig(pdf_filepath)\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(f\"Plot saved to {pdf_filepath}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ghJ5ELxj1A1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}