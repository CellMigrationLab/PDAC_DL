{"cells":[{"cell_type":"markdown","metadata":{"id":"xF4zYMmXULP7"},"source":["# PDAC CellTracksColab - Landing Tracks\n","---\n","\n","This notebook focus on the analysis of cell tracks exhibiting distinct landing patterns. Through a detailed examination of track dynamics, this tool provides deep insights into the mechanisms of cell landing, arrest, and interaction with the endothelium. Here's an overview of its functionalities:\n","\n","### Key Features\n","\n","- **Track Filtering Based on Instantaneous Speed:** The initial step involves segregating tracks that demonstrate a clear landing pattern by analyzing their instantaneous speed. This process ensures that only tracks relevant to the landing behavior are included for detailed analysis.\n","\n","- **Measurement of Track Parameters:** Once filtered, the notebook facilitates the measurement of a range of track parameters.\n","\n","- **Proximity Analysis to Endothelial Features:** A unique feature of this notebook is its ability to measure the shortest distance of each track from previously segmented features, including endothelial cell nuclei and cell junctions. This analysis is pivotal in understanding the spatial relationships and interactions between circulating cells and the endothelium.\n","\n","- **Visualization of Track Parameters:** To aid in the interpretation and presentation of findings, the notebook includes functionality for plotting the computed parameters of the tracks. These visualizations facilitate a clear and intuitive understanding of the data, highlighting key trends and patterns in cell behavior.\n","\n","\n","<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JrkfFr7mgZmA"},"outputs":[],"source":["# @title #MIT License\n","\n","print(\"\"\"\n","**MIT License**\n","\n","Copyright (c) 2023 Guillaume Jacquemet\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"Y4-Ft-yNRVCc"},"source":["--------------------------------------------------------\n","# **Part 1: Prepare the session and load your data**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9h0prdayn0qG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rAP0ahCzn1V6"},"outputs":[],"source":["#@markdown ##Play to install\n","!pip -q install pandas scikit-learn\n","!pip -q install plotly\n","!pip -q install tqdm\n","\n","!git clone https://github.com/CellMigrationLab/CellTracksColab.git\n","\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import numpy as np\n","import itertools\n","from matplotlib.gridspec import GridSpec\n","import requests\n","\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import sys\n","import matplotlib.colors as mcolors\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","import itertools\n","import requests\n","import ipywidgets as widgets\n","import warnings\n","import scipy.stats as stats\n","\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.gridspec import GridSpec\n","from ipywidgets import Dropdown, interact,Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","from tqdm.notebook import tqdm\n","from IPython.display import display, clear_output\n","from scipy.spatial import ConvexHull\n","from scipy.spatial.distance import cosine, pdist\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","from sklearn.metrics import pairwise_distances\n","from scipy.stats import zscore, ks_2samp\n","from sklearn.preprocessing import MinMaxScaler\n","from multiprocessing import Pool\n","from matplotlib.ticker import FixedLocator\n","from matplotlib.ticker import FuncFormatter\n","from matplotlib.colors import LogNorm\n","sys.path.append(\"../\")\n","sys.path.append(\"CellTracksColab/\")\n","\n","import celltracks\n","from celltracks import *\n","from celltracks.Track_Plots import *\n","from celltracks.BoxPlots_Statistics import *\n","from celltracks.Track_Metrics import *\n","\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=500000):\n","    \"\"\"Save a DataFrame with a progress bar and gzip compression.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing with gzip compression\n","        with gzip.open(path, \"wt\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3Kzd_8GUnpbw"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GA1wCrkoV4i5"},"outputs":[],"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bsDAwkSOo1gV"},"source":["## **1.3. Compile your data or load existing dataframes**\n","---\n","\n","<font size = 4> Please ensure that your data is properly organised (see above)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CQKXq3giI3nX"},"outputs":[],"source":["#@markdown ##Provide the path to your dataset (chunk):\n","\n","\n","import os\n","import re\n","import glob\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import requests\n","import zipfile\n","\n","#@markdown ###You have existing dataframes, provide the path to your:\n","\n","Track_table = ''  # @param {type: \"string\"}\n","Spot_table = ''  # @param {type: \"string\"}\n","\n","#@markdown ###Provide the path to your Result folder\n","\n","Results_Folder = \"\"  # @param {type: \"string\"}\n","\n","\n","Results_Folder = Results_Folder + \"/Landing\"\n","\n","if not os.path.exists(Results_Folder):\n","    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n","\n","# Print the location of the result folder\n","print(f\"Result folder is located at: {Results_Folder}\")\n","\n","def validate_tracks_df(df):\n","    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def validate_spots_df(df):\n","    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def check_unique_id_match(df1, df2):\n","    df1_ids = set(df1['Unique_ID'])\n","    df2_ids = set(df2['Unique_ID'])\n","\n","    # Check if the IDs in the two dataframes match\n","    if df1_ids == df2_ids:\n","        print(\"The Unique_ID values in both dataframes match perfectly!\")\n","    else:\n","        missing_in_df1 = df2_ids - df1_ids\n","        missing_in_df2 = df1_ids - df2_ids\n","\n","        if missing_in_df1:\n","            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n","\n","        if missing_in_df2:\n","            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n","\n","# For existing dataframes\n","if Track_table:\n","    print(\"Loading track table file....\")\n","    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n","    if not validate_tracks_df(merged_tracks_df):\n","        print(\"Error: Validation failed for loaded tracks dataframe.\")\n","\n","if Spot_table:\n","    print(\"Loading spot table file....\")\n","    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n","    if not validate_spots_df(merged_spots_df):\n","        print(\"Error: Validation failed for loaded spots dataframe.\")\n","\n","check_for_nans(merged_spots_df, \"merged_spots_df\")\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CvgA4JhC3HcH"},"outputs":[],"source":["#@markdown ##Check Metadata\n","\n","\n","# Define the metadata columns that are expected to have identical values for each filename\n","metadata_columns = ['Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']\n","\n","# Group the DataFrame by 'File_name' and then check if all entries within each group are identical\n","consistent_metadata = True\n","for name, group in merged_tracks_df.groupby('File_name'):\n","    for col in metadata_columns:\n","        if not group[col].nunique() == 1:\n","            consistent_metadata = False\n","            print(f\"Inconsistency found for file: {name} in column: {col}\")\n","            break  # Stop checking other columns for this group and move to the next file\n","    if not consistent_metadata:\n","        break  # Stop the entire process if any inconsistency is found\n","\n","if consistent_metadata:\n","    print(\"All files have consistent metadata across the specified columns.\")\n","else:\n","    print(\"There are inconsistencies in the metadata. Please check the output for details.\")\n","\n","# Drop duplicates based on the 'File_name' to get a unique list of filenames and their metadata\n","unique_files_df = merged_tracks_df.drop_duplicates(subset=['File_name'])[['File_name', 'Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']]\n","\n","# Reset the index to clean up the DataFrame\n","unique_files_df.reset_index(drop=True, inplace=True)\n","\n","# Display the resulting DataFrame in a nicely formatted HTML table\n","unique_files_df\n","\n","import pandas as pd\n","\n","# Assuming 'df' is your DataFrame and it already contains 'Conditions' and 'Repeats' columns.\n","\n","# Group by 'Conditions' and 'Repeats' and count the occurrences\n","grouped = unique_files_df.groupby(['Condition', 'Repeat']).size().reset_index(name='counts')\n","\n","# Check if any combinations have a count greater than 1, which means they are not unique\n","non_unique_combinations = grouped[grouped['counts'] > 1]\n","\n","# Print the non-unique combinations\n","if not non_unique_combinations.empty:\n","    print(\"There are non-unique combinations of Conditions and Repeats:\")\n","    print(non_unique_combinations)\n","else:\n","    print(\"All combinations of Conditions and Repeats are unique.\")\n","\n","check_unique_id_match(merged_spots_df, merged_tracks_df)\n","\n","\n","# Group the DataFrame by 'Cells', 'Treatment', 'Repeat' and then check if there are 4 unique 'Flow_speed' values for each group\n","consistent_flow_speeds = True\n","for (cells, ilbeta, repeat), group in merged_tracks_df.groupby(['Cells', 'Treatment', 'Repeat']):\n","    if group['Flow_speed'].nunique() != 4:\n","        consistent_flow_speeds = False\n","        print(f\"Inconsistency found for Cells: {cells}, Treatment: {Treatment_conditions}, Repeat: {repeat} - Expected 4 Flow_speeds, found {group['Flow_speed'].nunique()}\")\n","        break  # Stop the entire process if any inconsistency is found\n","\n","if consistent_flow_speeds:\n","    print(\"Each combination of 'Cells', 'Treatment', 'Repeat' has exactly 4 different 'Flow_speed' values.\")\n","else:\n","    print(\"There are inconsistencies in 'Flow_speed' values. Please check the output for details.\")\n","\n","\n","unique_cells = unique_files_df['Cells'].unique()\n","unique_flow_speeds = unique_files_df['Flow_speed'].unique()\n","unique_Treatment = unique_files_df['Treatment'].unique()\n","unique_conditions = unique_files_df['Condition'].unique()\n","\n","print(\"Unique Cells:\", unique_cells)\n","print(\"Unique Flow Speeds:\", unique_flow_speeds)\n","print(\"Unique Silencing:\", unique_Treatment)\n","print(\"Unique Conditions:\", unique_conditions)\n"]},{"cell_type":"markdown","metadata":{"id":"t5YJ9V468HwJ"},"source":["## **1.4. Filter tracks**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mA8GBhFy8vd6"},"outputs":[],"source":["# @title ##Filter tracks shorter than 50 spots\n","\n","\n","merged_tracks_df = merged_tracks_df[merged_tracks_df['NUMBER_SPOTS'] >= 50]\n","merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]\n"]},{"cell_type":"markdown","metadata":{"id":"52STmnv43d45"},"source":["## **1.5. Visualise your tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AE881uJW5ukQ"},"outputs":[],"source":["# @title ##Run the cell and choose the file you want to inspect\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CQ2P8sw1izGb"},"outputs":[],"source":["# @title ##Speed density plots\n","\n","\n","# Updated code to visualize distributions using the 'fill' parameter in sns.kdeplot\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_distribution_by_condition_updated(df):\n","    conditions = df['Condition'].unique()\n","\n","    # Setting up the plotting environment\n","    sns.set_style(\"whitegrid\")\n","    plt.figure(figsize=(18, 20))  # Increased height to fit the fourth plot\n","\n","    # Plotting histograms for TRACK_MEAN_SPEED\n","    plt.subplot(4, 1, 1)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TRACK_MEAN_SPEED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TRACK_MEAN_SPEED by Condition')\n","    plt.legend()\n","\n","    # Plotting histograms for TRACK_MAX_SPEED\n","    plt.subplot(4, 1, 2)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TRACK_MAX_SPEED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TRACK_MAX_SPEED by Condition')\n","    plt.legend()\n","\n","    # Plotting histograms for TRACK_MIN_SPEED\n","    plt.subplot(4, 1, 3)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TRACK_MIN_SPEED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TRACK_MIN_SPEED by Condition')\n","    plt.legend()\n","\n","    # Plotting histograms for TOTAL_DISTANCE_TRAVELED\n","    plt.subplot(4, 1, 4)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TOTAL_DISTANCE_TRAVELED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TOTAL_DISTANCE_TRAVELED by Condition')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# You can call this function with your dataframe like this:\n","plot_distribution_by_condition_updated(merged_tracks_df)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jWasqb467Z65"},"outputs":[],"source":["# @title ##Time points per tracks\n","\n","\n","import matplotlib.pyplot as plt\n","\n","\n","# Calculate the count of time points per track\n","time_points_per_track = merged_spots_df.groupby('Unique_ID').size()\n","\n","# Plotting\n","plt.figure(figsize=(10, 6))\n","time_points_per_track.hist(bins=30, edgecolor='black')\n","plt.title('Distribution of Time Points per Track')\n","plt.xlabel('Number of Time Points')\n","plt.ylabel('Count of Tracks')\n","plt.grid(False)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"49LdP6Mp2HF_"},"source":["# **Part 2: Analyse only the tracks that arrest**"]},{"cell_type":"markdown","metadata":{"id":"rNVrG_XN9bez"},"source":["## **2.1. Filter the data and save the dataframe**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mRHSNfVfEfJT"},"outputs":[],"source":["# @title ##Categorise the tracks based on Start speed and Min speed and End speed\n","\n","from tqdm.notebook import tqdm\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","import pandas as pd\n","import numpy as np\n","\n","def categorize_tracks(dataframe, max_speed_threshold, min_speed_threshold, end_speed_threshold):\n","    # Categorization based on the criteria for Flow_arrested\n","    condition = (dataframe['AvgSpeedFirstN'] > max_speed_threshold) & \\\n","                (dataframe['Min Speed'] < min_speed_threshold) & \\\n","                (dataframe['AvgSpeedLastN'] < end_speed_threshold)\n","\n","    dataframe['Behaviour'] = np.where(condition, 'Flow_arrested', 'Other')\n","\n","    # Keep only rows where Behaviour is 'Flow_arrested'\n","    return dataframe[dataframe['Behaviour'] == 'Flow_arrested']\n","\n","def on_button_click(button):\n","    with output:\n","        clear_output(wait=True)\n","\n","        filtered_df = categorize_tracks(\n","            merged_tracks_df,  # Make sure this DataFrame is correctly referenced\n","            max_speed_threshold=max_speed_input.value,\n","            min_speed_threshold=min_speed_input.value,\n","            end_speed_threshold=end_speed_input.value  # New end speed threshold parameter\n","        )\n","\n","        # Calculating count for Flow_arrested\n","        flow_arrested_count = len(filtered_df)\n","\n","        # Printing the results\n","        print(\"Count of 'Flow_arrested' tracks:\")\n","        print(f\"Flow_arrested: {flow_arrested_count}\")\n","\n","        print(\"\\nSaving the results\")\n","        save_dataframe_with_progress(filtered_df, Results_Folder + '/' + 'Flow_Arrested_Tracks.csv')\n","        print(\"Done\")\n","\n","# Define widgets for user input\n","max_speed_input = widgets.FloatText(value=20, description='Max Speed Threshold:', step=0.1)\n","min_speed_input = widgets.FloatText(value=1, description='Min Speed Threshold:', step=0.1)\n","end_speed_input = widgets.FloatText(value=5, description='End Speed Threshold:', step=0.1)  # New widget for end speed threshold\n","\n","apply_button = widgets.Button(description=\"Categorize Tracks\")\n","output = widgets.Output()\n","\n","apply_button.on_click(on_button_click)\n","\n","# Display the widgets\n","display(widgets.VBox([max_speed_input, min_speed_input, end_speed_input, apply_button, output]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9v7_EhO-GS-X"},"outputs":[],"source":["# @title ##Filter the data to keep only the Flow_arrested\n","\n","\n","# Filter merged_tracks_df for 'Flow_arrested' behaviour only\n","Filtered_merged_tracks_df = merged_tracks_df[merged_tracks_df['Behaviour'] == 'Flow_arrested']\n","\n","# Filter merged_spots_df to include only spots from 'Flow_arrested' tracks\n","Filtered_merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(Filtered_merged_tracks_df['Unique_ID'])]\n","\n","check_unique_id_match(Filtered_merged_spots_df, Filtered_merged_tracks_df)\n","\n","# Save the updated DataFrame\n","save_dataframe_with_progress(Filtered_merged_tracks_df, Results_Folder + '/' + 'Filtered_Merged_Tracks.csv.gz')\n","\n","save_dataframe_with_progress(Filtered_merged_spots_df, Results_Folder + '/' + 'Filtered_Spots_Tracks.csv.gz')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LUOlo1n79hNh"},"source":["## **2.2. Compute track metrics**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"T8MVe3RHDWl9"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# @title ##Compute track metrics from slow down to arrest, slow down to end, and arrest to end\n","\n","\n","# Function to identify the slowing point and return its coordinates and time\n","def identify_and_get_slowing_point_details(track, slowdown_threshold=10):\n","    track = track.sort_values(by='POSITION_T')\n","    slowing_point_candidates = track[track['Speed'] < slowdown_threshold]\n","    slowing_start_index = slowing_point_candidates.index.min() if not slowing_point_candidates.empty else None\n","\n","    if slowing_start_index is not None:\n","        slowing_point_details = track.loc[slowing_start_index, ['POSITION_X', 'POSITION_Y', 'POSITION_T']]\n","        return slowing_start_index, slowing_point_details\n","    else:\n","        return None, pd.Series({'POSITION_X': np.nan, 'POSITION_Y': np.nan, 'POSITION_T': np.nan})\n","\n","def identify_and_get_stopping_point_details(track):\n","    track = track.sort_values(by='POSITION_T')\n","    # Identify the minimum speed\n","    min_speed = track['Speed'].min()\n","    # Find all points where speed equals the minimum speed\n","    stopping_point_candidates = track[track['Speed'] == min_speed]\n","    # Sort these candidates by time and take the first one\n","    stopping_point = stopping_point_candidates.sort_values(by='POSITION_T').head(1)\n","\n","    if not stopping_point.empty:\n","        stopping_start_index = stopping_point.index[0]\n","        stopping_point_details = stopping_point.iloc[0][['POSITION_X', 'POSITION_Y', 'POSITION_T']]\n","        return stopping_start_index, stopping_point_details\n","    else:\n","        return None, pd.Series({'POSITION_X': np.nan, 'POSITION_Y': np.nan, 'POSITION_T': np.nan})\n","\n","# Function to compute the total distance traveled from the slowing point\n","def compute_total_distance_from_slowing_point(track):\n","    distances = np.sqrt(track['POSITION_X'].diff()**2 + track['POSITION_Y'].diff()**2)\n","    return distances.sum()\n","\n","# Function to calculate Directionality\n","def calculate_directionality(group):\n","    group = group.sort_values('POSITION_T')\n","    start_point = group.iloc[0][['POSITION_X', 'POSITION_Y']].to_numpy()\n","    end_point = group.iloc[-1][['POSITION_X', 'POSITION_Y']].to_numpy()\n","\n","    euclidean_distance = np.linalg.norm(end_point - start_point)\n","    deltas = np.linalg.norm(np.diff(group[['POSITION_X', 'POSITION_Y']].values, axis=0), axis=1)\n","    total_path_length = deltas.sum()\n","\n","    D = euclidean_distance / total_path_length if total_path_length != 0 else 0\n","    return pd.Series({'Directionality': D})\n","\n","# Function to calculate FMI\n","def calculate_fmi(group):\n","    group = group.sort_values('POSITION_T')\n","    total_forward_displacement = group['POSITION_X'].diff().fillna(0).sum()\n","    total_path_length = np.linalg.norm(np.diff(group[['POSITION_X', 'POSITION_Y']].values, axis=0), axis=1).sum()\n","\n","    FMI = total_forward_displacement / total_path_length if total_path_length != 0 else 0\n","    return pd.Series({'FMI': FMI})\n","\n","def compute_track_segments_metrics(track, slowdown_threshold=10):\n","    # Sort the track by time\n","    track = track.sort_values(by='POSITION_T')\n","\n","    # Identify slowing, stopping, and end points\n","    slowing_point_index, slowing_point_details = identify_and_get_slowing_point_details(track, slowdown_threshold)\n","    stopping_point_index, stopping_point_details = identify_and_get_stopping_point_details(track)\n","    end_point_index = track.index.max()\n","    end_point_details = track.loc[end_point_index, ['POSITION_X', 'POSITION_Y', 'POSITION_T']]\n","\n","    # Check if the points are identified\n","    if slowing_point_index is None or stopping_point_index is None or end_point_index is None:\n","        return pd.Series({'Metrics_Slowdown_to_Arrest': np.nan, 'Metrics_Slowdown_to_End': np.nan, 'Metrics_Arrest_to_End': np.nan,\n","                          'Euclidean_Distance_Slowdown_to_End': np.nan,\n","                          'Slowing_Point_X': np.nan, 'Slowing_Point_Y': np.nan, 'Slowing_Point_T': np.nan,\n","                          'Stopping_Point_X': np.nan, 'Stopping_Point_Y': np.nan, 'Stopping_Point_T': np.nan,\n","                          'End_Point_X': np.nan, 'End_Point_Y': np.nan, 'End_Point_T': np.nan})\n","\n","    # Calculate metrics for each segment\n","    metrics_slowdown_to_arrest = calculate_metrics(track, slowing_point_index, stopping_point_index)\n","    metrics_slowdown_to_end = calculate_metrics(track, slowing_point_index, end_point_index)\n","    metrics_arrest_to_end = calculate_metrics(track, stopping_point_index, end_point_index)\n","\n","    # Compute Euclidean distance from slowdown to track end\n","    start_point = track.loc[slowing_point_index, ['POSITION_X', 'POSITION_Y']].to_numpy()\n","    end_point = track.loc[end_point_index, ['POSITION_X', 'POSITION_Y']].to_numpy()\n","    euclidean_distance_slowdown_to_end = np.linalg.norm(end_point - start_point)\n","\n","    # Construct the result\n","    result = {\n","        **{'Slowdown_to_Arrest_' + k: v for k, v in metrics_slowdown_to_arrest.items()},\n","        **{'Slowdown_to_End_' + k: v for k, v in metrics_slowdown_to_end.items()},\n","        **{'Arrest_to_End_' + k: v for k, v in metrics_arrest_to_end.items()},\n","        'Euclidean_Distance_Slowdown_to_End': euclidean_distance_slowdown_to_end,\n","        'Slowing_Point_X': slowing_point_details['POSITION_X'],\n","        'Slowing_Point_Y': slowing_point_details['POSITION_Y'],\n","        'Slowing_Point_T': slowing_point_details['POSITION_T'],\n","        'Stopping_Point_X': stopping_point_details['POSITION_X'],\n","        'Stopping_Point_Y': stopping_point_details['POSITION_Y'],\n","        'Stopping_Point_T': stopping_point_details['POSITION_T'],\n","        'End_Point_X': end_point_details['POSITION_X'],\n","        'End_Point_Y': end_point_details['POSITION_Y'],\n","        'End_Point_T': end_point_details['POSITION_T']\n","    }\n","    return pd.Series(result)\n","\n","\n","def calculate_metrics(track, start_index, end_index):\n","    # Subset the track for the given segment\n","    subset_track = track.loc[start_index:end_index]\n","\n","    # Compute required metrics for the track segment\n","    # Add any additional metrics calculation here as needed\n","    total_distance = compute_total_distance_from_slowing_point(subset_track)\n","    directionality = calculate_directionality(subset_track)['Directionality']\n","    fmi = calculate_fmi(subset_track)['FMI']\n","\n","    # Return a dictionary of calculated metrics\n","    return {'Total_Distance': total_distance, 'Directionality': directionality, 'FMI': fmi}\n","\n","# Apply the function to the grouped DataFrame\n","grouped_df = Filtered_merged_spots_df.groupby('Unique_ID')\n","track_segments_metrics_df = grouped_df.apply(compute_track_segments_metrics).reset_index()\n","\n","# Save the new DataFrame\n","save_dataframe_with_progress(track_segments_metrics_df, Results_Folder + '/' + 'Track_Segments_Metrics.csv.gz')\n","\n","# Find overlapping columns and remove them from the original DataFrame\n","overlapping_columns = Filtered_merged_tracks_df.columns.intersection(track_segments_metrics_df.columns).drop('Unique_ID')\n","Filtered_merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge the new data into the original DataFrame\n","Filtered_merged_tracks_df = pd.merge(Filtered_merged_tracks_df, track_segments_metrics_df, on='Unique_ID', how='left')\n","\n","# Save the updated DataFrame\n","save_dataframe_with_progress(Filtered_merged_tracks_df, Results_Folder + '/' + 'Filtered_Merged_Tracks.csv.gz')\n","\n","# Check for NaNs in the updated DataFrame\n","check_for_nans(Filtered_merged_tracks_df, \"Filtered_merged_tracks_df\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"vqUHBOZyGzLc"},"outputs":[],"source":["# @title ##Plot examples\n","\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","\n","if not os.path.exists(Results_Folder+\"/Track_speed\"):\n","    os.makedirs(Results_Folder+\"/Track_speed\")  # Create Results_Folder if it doesn't exist\n","\n","def plot_flow_arrested_tracks(tracks_df, spots_df, num_tracks=15, save_path='plots'):\n","    save_path = Results_Folder+\"/Track_speed\"\n","\n","    arrested_track_ids = tracks_df['Unique_ID'].unique()\n","\n","    plotted_tracks = 0\n","    for track_id in arrested_track_ids:\n","        if plotted_tracks >= num_tracks:\n","            break\n","\n","        track = spots_df[spots_df['Unique_ID'] == track_id]\n","        if track.empty:\n","            continue\n","        track = track.sort_values(by='POSITION_T')\n","\n","        # Get the recorded slowdown, stopping, and end time from the tracks dataframe\n","        recorded_slowdown_time = tracks_df[tracks_df['Unique_ID'] == track_id]['Slowing_Point_T'].iloc[0]\n","        recorded_stopping_time = tracks_df[tracks_df['Unique_ID'] == track_id]['Stopping_Point_T'].iloc[0]\n","        recorded_end_time = tracks_df[tracks_df['Unique_ID'] == track_id]['End_Point_T'].iloc[0]\n","\n","        if pd.isna(recorded_slowdown_time) or pd.isna(recorded_stopping_time) or pd.isna(recorded_end_time):\n","            # Skip plotting if no slowdown, stopping, or end time is recorded\n","            continue\n","\n","        # Find the points in the track data corresponding to the slowdown, stopping, and end time\n","        slowdown_point = track[track['POSITION_T'] == recorded_slowdown_time].iloc[0]\n","        stopping_point = track[track['POSITION_T'] == recorded_stopping_time].iloc[0]\n","        end_point = track[track['POSITION_T'] == recorded_end_time].iloc[0]\n","\n","        # Plotting\n","        plt.figure(figsize=(12, 6))\n","        plt.plot(track['POSITION_T'], track['Speed'], label=f'Track {track_id}', linestyle='-', marker=None)\n","\n","        # Highlight the recorded slowdown point\n","        plt.scatter(slowdown_point['POSITION_T'], slowdown_point['Speed'], color='red', zorder=5)\n","        plt.text(slowdown_point['POSITION_T'], slowdown_point['Speed'], ' Slowdown', color='red')\n","\n","        # Highlight the recorded stopping point\n","        plt.scatter(stopping_point['POSITION_T'], stopping_point['Speed'], color='blue', zorder=5)\n","        plt.text(stopping_point['POSITION_T'], stopping_point['Speed'], ' Stopping', color='blue')\n","\n","        # Highlight the recorded end point\n","        plt.scatter(end_point['POSITION_T'], end_point['Speed'], color='green', zorder=5)\n","        plt.text(end_point['POSITION_T'], end_point['Speed'], ' End', color='green')\n","\n","        plt.xlabel('Time')\n","        plt.ylabel('Instantaneous Speed')\n","        plt.title(f'Instantaneous Speed Over Time for Track {track_id} (Flow Arrested)')\n","        plt.legend()\n","\n","        # Save the plot as a PDF file\n","        plt.savefig(f'{save_path}/Track_{track_id}.pdf')\n","        plt.show()\n","        plt.close()  # Close the plot to free up memory\n","\n","        plotted_tracks += 1\n","\n","# Example usage\n","plot_flow_arrested_tracks(Filtered_merged_tracks_df, Filtered_merged_spots_df)\n"]},{"cell_type":"markdown","metadata":{"id":"COlUzNLC9n_u"},"source":["## **2.3. Check the tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"q_iuC8ARRp_G"},"outputs":[],"source":["# @title ##Plot track examples\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Ensure the Results_Folder exists\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")\n","\n","# Extract unique filenames from the dataframe\n","filenames = Filtered_merged_tracks_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrames based on the selected filename\n","        filtered_df = Filtered_merged_spots_df[Filtered_merged_spots_df['File_name'] == filename]\n","        points_df = Filtered_merged_tracks_df[Filtered_merged_tracks_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=1)\n","\n","            # Plot the slowdown point if it exists\n","            if unique_id in points_df['Unique_ID'].values:\n","                point = points_df[points_df['Unique_ID'] == unique_id]\n","                if not pd.isna(point['Slowing_Point_X'].values[0]):\n","                    plt.scatter(point['Slowing_Point_X'].values[0], point['Slowing_Point_Y'].values[0], color='red', s=50, label='Slowdown Point' if unique_id == filtered_df['Unique_ID'].unique()[0] else \"\")\n","\n","                # Plot the stopping point if it exists\n","                if not pd.isna(point['Stopping_Point_X'].values[0]):\n","                    plt.scatter(point['Stopping_Point_X'].values[0], point['Stopping_Point_Y'].values[0], color='blue', s=50, label='Stopping Point' if unique_id == filtered_df['Unique_ID'].unique()[0] else \"\")\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.legend()\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dfYTUNplYoEB"},"outputs":[],"source":["# @title ##Run to plot the tracks for all FOV\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Ensure the output directory exists\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")\n","\n","# Extract unique filenames from the DataFrame\n","filenames = Filtered_merged_tracks_df['File_name'].unique()\n","\n","def plot_coordinates(filename):\n","    # Filter the DataFrames based on the filename\n","    filtered_df = Filtered_merged_spots_df[Filtered_merged_spots_df['File_name'] == filename]\n","    points_df = Filtered_merged_tracks_df[Filtered_merged_tracks_df['File_name'] == filename]\n","\n","    plt.figure(figsize=(10, 8))\n","    for unique_id in filtered_df['Unique_ID'].unique():\n","        unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","        plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=1)\n","\n","        # Plot the slowdown point if it exists\n","        if unique_id in points_df['Unique_ID'].values:\n","            point = points_df[points_df['Unique_ID'] == unique_id]\n","            if not pd.isna(point['Slowing_Point_X'].values[0]):\n","                plt.scatter(point['Slowing_Point_X'].values[0], point['Slowing_Point_Y'].values[0], color='red', s=50)\n","\n","            # Plot the stopping point if it exists\n","            if not pd.isna(point['Stopping_Point_X'].values[0]):\n","                plt.scatter(point['Stopping_Point_X'].values[0], point['Stopping_Point_Y'].values[0], color='blue', s=50)\n","\n","    # Set the plot axes limits\n","    plt.xlim(0, 650)\n","    plt.ylim(0, 650)\n","\n","    plt.xlabel('POSITION_X')\n","    plt.ylabel('POSITION_Y')\n","    plt.title(f'Coordinates for {filename}')\n","    plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","    plt.close()\n","\n","# Generate and save plots for all filenames\n","for filename in filenames:\n","    plot_coordinates(filename)\n"]},{"cell_type":"markdown","metadata":{"id":"Hlp-NuZn9tyx"},"source":["## **2.4. Plot tracks only from slowdown to (first) arrest**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"w5HMc4wHchHz"},"outputs":[],"source":["# @title ##Plot track example\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Ensure the Results_Folder exists\n","if not os.path.exists(Results_Folder+\"/Tracks_slowdown\"):\n","    os.makedirs(Results_Folder+\"/Tracks_slowdown\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = Filtered_merged_tracks_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrames based on the selected filename\n","        filtered_df = Filtered_merged_spots_df[Filtered_merged_spots_df['File_name'] == filename]\n","        points_df = Filtered_merged_tracks_df[Filtered_merged_tracks_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            if unique_id in points_df['Unique_ID'].values:\n","                point = points_df[points_df['Unique_ID'] == unique_id]\n","                slowdown_time = point['Slowing_Point_T'].values[0]\n","                stopping_time = point['Stopping_Point_T'].values[0]\n","\n","                # Plot only the track segment between the slowdown and stopping points\n","                if not pd.isna(slowdown_time) and not pd.isna(stopping_time):\n","                    unique_df = filtered_df[(filtered_df['Unique_ID'] == unique_id) & (filtered_df['POSITION_T'] >= slowdown_time) & (filtered_df['POSITION_T'] <= stopping_time)].sort_values(by='POSITION_T')\n","                    plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=1)\n","\n","                    # Mark the slowdown and stopping points\n","                    plt.scatter(point['Slowing_Point_X'].values[0], point['Slowing_Point_Y'].values[0], color='red', s=50, label='Slowdown Point')\n","                    plt.scatter(point['Stopping_Point_X'].values[0], point['Stopping_Point_Y'].values[0], color='blue', s=50, label='Stopping Point')\n","\n","        plt.xlim(0, 650)\n","        plt.ylim(0, 650)\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BURFOXbId2qD"},"outputs":[],"source":["# @title ##Run to generate tracks for all FOV\n","\n","\n","\n","if not os.path.exists(Results_Folder+\"/Tracks_slowdown\"):\n","    os.makedirs(Results_Folder+\"/Tracks_slowdown\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = Filtered_merged_tracks_df['File_name'].unique()\n","\n","def plot_coordinates(filename):\n","    # Filter the DataFrames based on the filename\n","    filtered_df = Filtered_merged_spots_df[Filtered_merged_spots_df['File_name'] == filename]\n","    points_df = Filtered_merged_tracks_df[Filtered_merged_tracks_df['File_name'] == filename]\n","\n","    plt.figure(figsize=(10, 8))\n","    for unique_id in filtered_df['Unique_ID'].unique():\n","        if unique_id in points_df['Unique_ID'].values:\n","            point = points_df[points_df['Unique_ID'] == unique_id]\n","            slowdown_time = point['Slowing_Point_T'].values[0]\n","            stopping_time = point['Stopping_Point_T'].values[0]\n","\n","            # Plot only the track segment between the slowdown and stopping points\n","            if not pd.isna(slowdown_time) and not pd.isna(stopping_time):\n","                unique_df = filtered_df[(filtered_df['Unique_ID'] == unique_id) & (filtered_df['POSITION_T'] >= slowdown_time) & (filtered_df['POSITION_T'] <= stopping_time)].sort_values(by='POSITION_T')\n","                plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=1)\n","\n","                # Optionally, mark the slowdown and stopping points\n","                plt.scatter(point['Slowing_Point_X'].values[0], point['Slowing_Point_Y'].values[0], color='red', s=50)\n","                plt.scatter(point['Stopping_Point_X'].values[0], point['Stopping_Point_Y'].values[0], color='blue', s=50)\n","\n","    # Set the plot axes limits\n","    plt.xlim(0, 650)\n","    plt.ylim(0, 650)\n","\n","    plt.xlabel('POSITION_X')\n","    plt.ylabel('POSITION_Y')\n","    plt.title(f'Coordinates for {filename}')\n","    plt.savefig(f\"{Results_Folder}/Tracks_slowdown/Tracks_{filename}.pdf\")\n","    plt.close()  # Close the figure to free memory\n","\n","# Process each file\n","for filename in filenames:\n","    plot_coordinates(filename)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CjF0cU7pc-7g"},"source":["--------------------------------------------------------\n","# **Part 3. Compute the distance to the nearest junction and nuclei**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"sj9mk_6zEVlI"},"source":["## **3.1. Load your Nuclei and Junction segmentation maps**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xz1dqlkgdHod"},"outputs":[],"source":["# @title #Load your Nuclei and Junction segmentation maps\n","\n","\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from skimage import io\n","import matplotlib.pyplot as plt\n","from tifffile import imread\n","from skimage.measure import label, regionprops, find_contours\n","from scipy.ndimage import distance_transform_edt\n","\n","Video_path = ''  # @param {type: \"string\"}\n","\n","Pixel_calibration = 0.6496139\n","\n","# @title #Process the dataset\n","\n","def compute_distances_using_distance_transform(df, image_dir):\n","    \"\"\"\n","    Compute distances to the nearest labeled pixel for each spot using the distance transform method.\n","    \"\"\"\n","    for file_name in tqdm(df['File_name'].unique(), desc=\"Processing files\"):\n","        # Paths to the label images\n","        junctions_img_path = f\"{image_dir}/{file_name}_HUVEC_junctions.tif\"\n","        nuclei_img_path = f\"{image_dir}/{file_name}_HUVEC_nuclei.tif\"\n","\n","        try:\n","            junctions_img = io.imread(junctions_img_path)\n","            nuclei_img = io.imread(nuclei_img_path)\n","        except FileNotFoundError:\n","            print(f\"Error: Images for {file_name} not found. Skipping...\")\n","            continue\n","\n","        # Compute distance transform\n","        distance_transform_junctions = distance_transform_edt(junctions_img == 0) * Pixel_calibration\n","        distance_transform_nuclei = distance_transform_edt(nuclei_img == 0) * Pixel_calibration\n","\n","        # Process coordinates and update the distances\n","        for idx, row in tqdm(df[df['File_name'] == file_name].iterrows(), total=df[df['File_name'] == file_name].shape[0], desc=f\"Processing coordinates for {file_name}\", leave=False):\n","            y, x = int(row['POSITION_Y'] / Pixel_calibration), int(row['POSITION_X'] / Pixel_calibration)\n","            df.loc[idx, 'DistanceToJunctions'] = distance_transform_junctions[y, x]\n","            df.loc[idx, 'DistanceToNuclei'] = distance_transform_nuclei[y, x]\n","\n","    return df\n","\n","compute_distances_using_distance_transform(Filtered_merged_spots_df, Video_path)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hvC5dtQFEegw"},"source":["## **3.2. Visual validation**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rEedcUEle1HT"},"outputs":[],"source":["# @title #Visual validation\n","from ipywidgets import Button, interactive, IntSlider, widgets\n","from ipywidgets import Output\n","from IPython.display import clear_output\n","from tifffile import imread\n","\n","\n","Pixel_calibration = 0.6496139\n","error_output = Output()\n","\n","def display_error_message(message):\n","    print(f\"Error: {message}\")\n","\n","filename_dropdown = widgets.Dropdown(\n","    options=merged_spots_df['File_name'].unique(),\n","    description='Filename:',\n","    disabled=False\n",")\n","\n","# Function to visualize distances for a given filename using pre-computed distances from merged_spot_df\n","def visualize_precomputed_distances_for_filename(filename):\n","    # Construct paths for the label images\n","    junctions_img_path = f\"{Video_path}/{filename}_HUVEC_junctions.tif\"\n","    nuclei_img_path = f\"{Video_path}/{filename}_HUVEC_nuclei.tif\"\n","\n","    try:\n","        junctions_img = imread(junctions_img_path)\n","        nuclei_img = imread(nuclei_img_path)\n","\n","        # Convert images to binary\n","        junctions_img[junctions_img > 0] = 255\n","        nuclei_img[nuclei_img > 0] = 255\n","\n","    except FileNotFoundError:\n","        display_error_message(f\"Images for {filename} not found.\")\n","        return\n","\n","    # Combine images into an RGB image\n","    combined_img = np.zeros((nuclei_img.shape[0], nuclei_img.shape[1], 3), dtype=np.uint8)\n","    combined_img[:, :, 0] = junctions_img  # Red channel\n","    combined_img[:, :, 1] = nuclei_img  # Green channel\n","    combined_img[:, :, 2] = junctions_img  # Blue channel\n","\n","    # Fetch the coordinates and precomputed distances for the selected filename\n","    data_for_frame = Filtered_merged_spots_df[Filtered_merged_spots_df['File_name'] == filename]\n","\n","    # Define a function to update the display based on the frame slider\n","    def update_display(frame_number):\n","        plt.figure(figsize=(10, 10))\n","\n","        # Use combined RGB image for visualization\n","        frame = combined_img.copy()\n","        coords_for_frame = data_for_frame[data_for_frame['POSITION_T'] == frame_number]\n","        for idx, row in coords_for_frame.iterrows():\n","            x, y = int(row['POSITION_X']/Pixel_calibration), int(row['POSITION_Y']/Pixel_calibration)\n","\n","            distance_to_junction = row['DistanceToJunctions']/Pixel_calibration\n","            distance_to_nuclei = row['DistanceToNuclei']/Pixel_calibration\n","\n","            circle_junction = plt.Circle((x, y), distance_to_junction, color='red', fill=False)\n","            circle_nuclei = plt.Circle((x, y), distance_to_nuclei, color='blue', fill=False)\n","\n","            plt.gca().add_patch(circle_junction)\n","            plt.gca().add_patch(circle_nuclei)\n","            plt.scatter(x, y, c='yellow')  # Coordinate point\n","\n","        plt.imshow(frame)\n","        plt.title(f\"Frame {frame_number} for {filename}\")\n","        plt.show()\n","\n","# Assuming 'Filtered_merged_spots_df' is your DataFrame\n","    unique_position_t_values = sorted(Filtered_merged_spots_df['POSITION_T'].unique())\n","\n","    frame_slider = widgets.SelectionSlider(\n","    options=unique_position_t_values,\n","    description='Frame',\n","    orientation='horizontal',\n","    readout=True)\n","    # Display the visualization with interactive for more reactive updates\n","    w = interactive(update_display, frame_number=frame_slider)\n","    display(w)\n","\n","\n","# Button to trigger visualization\n","plot_button_filename = Button(description=\"Visualize Distances\")\n","\n","# Function to handle button click for filename visualization\n","def on_plot_button_filename_click(b):\n","    filename = filename_dropdown.value\n","    # Clear the previous output\n","    clear_output()\n","\n","    display(filename_dropdown)\n","    display(plot_button_filename)\n","    display(error_output)\n","    visualize_precomputed_distances_for_filename(filename)\n","\n","# Bind the function to the button click event\n","plot_button_filename.on_click(on_plot_button_filename_click)\n","\n","# Display the widgets for filename visualization\n","display(filename_dropdown)\n","display(plot_button_filename)\n","display(error_output)"]},{"cell_type":"markdown","metadata":{"id":"ZEZJlOq9HCMI"},"source":["## **3.3. Extract distance and speed at landing, (first) arrest and end**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YPIK8F0qCnbu"},"outputs":[],"source":["# @title #Extract distance and speed at landing, arrest and end\n","\n","\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import pandas as pd\n","\n","def get_distances_and_speeds(track_df, spots_df):\n","    results = []\n","\n","    for _, track in tqdm(track_df.iterrows(), total=track_df.shape[0], desc=\"Processing Tracks\"):\n","        unique_id = track['Unique_ID']\n","        slowing_down_time = track['Slowing_Point_T']\n","        stopping_time = track['Stopping_Point_T']\n","\n","        # Filter spots for this track\n","        track_spots = spots_df[spots_df['Unique_ID'] == unique_id]\n","\n","        def get_spot_values_at_time(spot_df, time):\n","            spot = spot_df[spot_df['POSITION_T'] == time]\n","            if not spot.empty:\n","                return {\n","                    'distance_to_nuclei': spot['DistanceToNuclei'].iloc[0],\n","                    'distance_to_junctions': spot['DistanceToJunctions'].iloc[0],\n","                    'speed': spot['Speed'].iloc[0]\n","                }\n","            else:\n","                return {\n","                    'distance_to_nuclei': np.nan,\n","                    'distance_to_junctions': np.nan,\n","                    'speed': np.nan\n","                }\n","\n","        # Get distances and speed at the slowing down time\n","        slowing_values = get_spot_values_at_time(track_spots, slowing_down_time)\n","\n","        # Get distances and speed at the stopping time\n","        stopping_values = get_spot_values_at_time(track_spots, stopping_time)\n","\n","        # Get distances and speed at the end of the track\n","        end_time = track_spots['POSITION_T'].max()\n","        end_values = get_spot_values_at_time(track_spots, end_time)\n","\n","        # Append results\n","        results.append({\n","            'Unique_ID': unique_id,\n","            'DistanceToNuclei_Slowing': slowing_values['distance_to_nuclei'],\n","            'DistanceToJunctions_Slowing': slowing_values['distance_to_junctions'],\n","            'Speed_Slowing': slowing_values['speed'],\n","            'DistanceToNuclei_Stopping': stopping_values['distance_to_nuclei'],\n","            'DistanceToJunctions_Stopping': stopping_values['distance_to_junctions'],\n","            'Speed_Stopping': stopping_values['speed'],\n","            'DistanceToNuclei_End': end_values['distance_to_nuclei'],\n","            'DistanceToJunctions_End': end_values['distance_to_junctions'],\n","            'Speed_End': end_values['speed']\n","        })\n","\n","    return pd.DataFrame(results)\n","\n","# Usage example\n","distances_and_speeds_df = get_distances_and_speeds(Filtered_merged_tracks_df, Filtered_merged_spots_df)\n","\n","# Merging process\n","overlapping_columns = Filtered_merged_tracks_df.columns.intersection(distances_and_speeds_df.columns).drop('Unique_ID')\n","Filtered_merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","Filtered_merged_tracks_df = pd.merge(Filtered_merged_tracks_df, distances_and_speeds_df, on='Unique_ID', how='left')\n","\n","# Save the updated DataFrame\n","save_dataframe_with_progress(Filtered_merged_tracks_df, Results_Folder + '/' + 'Filtered_Merged_Tracks.csv.gz')\n"]},{"cell_type":"markdown","metadata":{"id":"0vLxEpLdEOeW"},"source":["## **3.4. Plot your results**\n"]},{"cell_type":"markdown","metadata":{"id":"fwrALabkE2zl"},"source":["### **3.4.1 Combine the flow speeds**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0dxZQnxXPUqr"},"outputs":[],"source":["# @title #Combine the flow speeds (optional)\n","\n","\n","def categorize_flow_speed(speed):\n","    if speed in ['100','200', '300', 'wash']:\n","        return 'Combined'\n","    else:\n","        return speed\n","\n","# Apply the function to create the Flow_speed_category column\n","Filtered_merged_tracks_df['Flow_speed_category'] = Filtered_merged_tracks_df['Flow_speed'].apply(categorize_flow_speed)\n","\n","# Print unique Flow_speed_category values\n","unique_flow_speed_categories = Filtered_merged_tracks_df['Flow_speed_category'].unique()\n","print(unique_flow_speed_categories)\n","\n","Filtered_merged_tracks_df['Condition_category'] = Filtered_merged_tracks_df['Cells'] + '_' + Filtered_merged_tracks_df['Flow_speed_category'].astype(str) + '_' + Filtered_merged_tracks_df['Treatment']\n"]},{"cell_type":"markdown","metadata":{"id":"bso0THJgE_Gy"},"source":["### **3.4.2 Plots the various distances per cells**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eNjnCW8j_Zw-"},"outputs":[],"source":["# @title #Plots the various distances per cells\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import os\n","\n","# Variables to compare (replace these with your actual column names)\n","variables_to_compare = ['DistanceToNuclei_Slowing', 'DistanceToJunctions_Slowing', 'DistanceToNuclei_Stopping', 'DistanceToJunctions_Stopping', 'DistanceToNuclei_End', 'DistanceToJunctions_End' ]\n","\n","# Unique condition categories\n","unique_conditions = Filtered_merged_tracks_df['Condition_category'].unique()\n","\n","# Directory to save plots and data\n","save_dir = Results_Folder+\"/Distance_nucleus_junctions\"\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)  # Create Results_Folder if it doesn't exist\n","\n","# Loop through each unique condition\n","for condition in unique_conditions:\n","    # Filter DataFrame for the current condition\n","    df_filtered = Filtered_merged_tracks_df[Filtered_merged_tracks_df['Condition_category'] == condition]\n","\n","    # Prepare data for plotting\n","    plot_data = pd.melt(df_filtered, id_vars=['Condition_category'], value_vars=variables_to_compare, var_name='Variable', value_name='Value')\n","\n","    # Create a figure for the plot\n","    plt.figure(figsize=(10, 6))\n","\n","    # Create a plot comparing the variables\n","    sns.boxplot(x='Variable', y='Value', data=plot_data)\n","    plt.title(f'Comparison of Variables for {condition}')\n","    plt.xlabel('Variable')\n","    plt.ylabel('Value')\n","\n","    # Save the figure as a PDF\n","    plot_filename = f\"{save_dir}/{condition}_comparison_plot.pdf\"\n","    plt.savefig(plot_filename)\n","    plt.show()\n","    plt.close()\n","\n","    # Save the data used for the plot as a CSV\n","    data_filename = f\"{save_dir}/{condition}_data.csv\"\n","    plot_data.to_csv(data_filename, index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"gydc2ejbJKZO"},"source":["# **Part 4. Count the number of arrest using peak detection**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7CyRbV5-jC3V"},"outputs":[],"source":["# @title ##Count the number of arrest using peak detection\n","\n","\n","from scipy.signal import find_peaks\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def detect_peaks(spots_df, tracks_df, track_id, height=None, threshold=None, distance=None):\n","    # Extract and sort the track data by 'POSITION_T'\n","    track_data = spots_df[spots_df['Unique_ID'] == track_id].sort_values(by='POSITION_T')\n","\n","    # Retrieve the first arrest time from tracks_df\n","    first_arrest_time = tracks_df[tracks_df['Unique_ID'] == track_id]['Stopping_Point_T'].iloc[0]\n","\n","    # Segment the data after the first arrest\n","    post_arrest_data = track_data[track_data['POSITION_T'] > first_arrest_time]\n","\n","    # Apply peak detection with additional parameters\n","    peaks, _ = find_peaks(post_arrest_data['Speed'], height=height, threshold=threshold, distance=distance)\n","\n","    return post_arrest_data, peaks\n","\n","\n","def batch_process_peak_detection(spots_df, tracks_df, height=None, threshold=None, distance=None, limit=10):\n","    peak_info = []\n","\n","    # Limit the number of tracks processed\n","    #for track_id in tracks_df['Unique_ID'].unique()[:limit]:\n","    for track_id in tracks_df['Unique_ID'].unique():\n","\n","        post_arrest_data, peaks = detect_peaks(spots_df, tracks_df, track_id, height, threshold, distance)\n","        peak_times = post_arrest_data.iloc[peaks]['POSITION_T'].values\n","        peak_count = len(peaks)\n","        peak_info.append({'Unique_ID': track_id, 'Number_of_Peaks': peak_count, 'Peak_Times': peak_times})\n","\n","\n","    return pd.DataFrame(peak_info)\n","\n","# Parameters for peak detection\n","height = 10\n","threshold = 0.1\n","distance = 5\n","\n","# Apply the peak detection and visual validation for the first 10 tracks\n","peak_results_df = batch_process_peak_detection(Filtered_merged_spots_df, Filtered_merged_tracks_df, height, threshold, distance)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ipqfbEOliyC","cellView":"form"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","\n","\n","# @title ##Plot\n","\n","\n","Speed_peaks_Results_Folder = Results_Folder + \"/Landing/Speed_peaks\"\n","\n","if not os.path.exists(Speed_peaks_Results_Folder):\n","    os.makedirs(Speed_peaks_Results_Folder)\n","\n","# Ensure the Results_Folder exists\n","if not os.path.exists(Speed_peaks_Results_Folder+\"/Tracks\"):\n","    os.makedirs(Speed_peaks_Results_Folder+\"/Tracks\")\n","\n","if not os.path.exists(Speed_peaks_Results_Folder+\"/Track_speed\"):\n","    os.makedirs(Speed_peaks_Results_Folder+\"/Track_speed\")  # Create Results_Folder if it doesn't exist\n","\n","\n","def plot_flow_arrested_tracks_with_peaks(tracks_df, spots_df, peak_results_df, num_tracks=10, Results_Folder=Speed_peaks_Results_Folder):\n","    save_path = os.path.join(Results_Folder, \"Track_speed\")\n","    plotted_tracks = 0\n","    for track_id in tracks_df['Unique_ID'].unique():\n","        if plotted_tracks >= num_tracks:\n","            break\n","\n","        track = spots_df[spots_df['Unique_ID'] == track_id]\n","        if track.empty or not any(peak_results_df['Unique_ID'] == track_id):\n","            continue\n","        track = track.sort_values(by='POSITION_T')\n","\n","        peaks_row = peak_results_df[peak_results_df['Unique_ID'] == track_id].iloc[0]\n","        peak_times = peaks_row['Peak_Times']\n","\n","        plt.figure(figsize=(12, 6))\n","        plt.plot(track['POSITION_T'], track['Speed'], label=f'Track {track_id}', linestyle='-', marker=None)\n","\n","        # Plot detected peaks\n","        for peak_time in peak_times:\n","            if peak_time in track['POSITION_T'].values:  # Check if the peak_time exactly matches any POSITION_T\n","                peak_point = track[track['POSITION_T'] == peak_time].iloc[0]\n","                plt.scatter(peak_point['POSITION_T'], peak_point['Speed'], color='orange', zorder=5, marker='*', s=100)\n","                plt.text(peak_point['POSITION_T'], peak_point['Speed'], ' Peak', color='orange', horizontalalignment='right')\n","\n","        plt.xlabel('Time')\n","        plt.ylabel('Instantaneous Speed')\n","        plt.title(f'Instantaneous Speed Over Time for Track {track_id}')\n","        plt.legend()\n","\n","        plt.savefig(f'{save_path}/Track_Peaks_{track_id}.pdf')\n","        plt.show()\n","        plt.close()\n","\n","        plotted_tracks += 1\n","\n","\n","# Example usage\n","plot_flow_arrested_tracks_with_peaks(Filtered_merged_tracks_df, Filtered_merged_spots_df, peak_results_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxcQ2d_wqLGF","cellView":"form"},"outputs":[],"source":["# @title ##Save the results\n","\n","# Identify overlapping columns except 'Unique_ID' to avoid duplications\n","overlapping_columns = Filtered_merged_tracks_df.columns.intersection(peak_results_df.columns).drop('Unique_ID')\n","\n","# Drop overlapping columns from 'Filtered_merged_tracks_df' if necessary\n","Filtered_merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge 'Filtered_merged_tracks_df' with only the 'Number_of_Peaks' column from 'peak_results_df'\n","Filtered_merged_tracks_df = pd.merge(\n","    Filtered_merged_tracks_df,\n","    peak_results_df[['Unique_ID', 'Number_of_Peaks']],\n","    on='Unique_ID',\n","    how='left'\n",")\n","\n","# Fill NaN values with 0 for 'Number_of_Peaks' if some tracks do not have detected peaks\n","Filtered_merged_tracks_df['Number_of_Peaks'].fillna(0, inplace=True)\n","\n","save_dataframe_with_progress(Filtered_merged_tracks_df, Results_Folder + '/' + 'Filtered_Merged_Tracks.csv.gz')\n"]},{"cell_type":"markdown","metadata":{"id":"joRI14WVUPuM"},"source":["-------------------------------------------\n","\n","# **Part 5. Plot track parameters**\n","-------------------------------------------\n","\n","##**Statistical analyses**\n","### Cohen's d (Effect Size):\n","<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n","\n","### Randomization Test:\n","<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n","\n","### Bonferroni Correction:\n","<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gMdpokV-Pgzm"},"outputs":[],"source":["# @title #Plot track parameters\n","\n","# Import necessary libraries\n","import os\n","import itertools\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","from matplotlib.backends.backend_pdf import PdfPages\n","import ipywidgets as widgets\n","from matplotlib.ticker import FixedLocator\n","\n","# Check and create necessary directories\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots\")\n","\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/pdf\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots/pdf\")\n","\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/csv\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots/csv\")\n","\n","\n","def get_selectable_columns(df):\n","    \"\"\"Get columns that can be plotted.\"\"\"\n","    exclude_cols = ['Condition', 'File_name', 'Flow_speed', 'Cells', 'Treatment', 'Repeat', 'Unique_ID',\n","                    'experiment_nb', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION',\n","                    'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    \"\"\"Display checkboxes for selecting variables.\"\"\"\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3))\n","    ]))\n","    return variable_checkboxes\n","\n","\n","def create_filename(base, selected_cells, selected_speeds, selected_Treatment, var):\n","    \"\"\"Create a unique filename based on selected options.\"\"\"\n","    def summarize_options(options):\n","        if len(options) > 3:\n","            return f\"{len(options)}options\"\n","        return \"_\".join(options)\n","\n","    selected_options = \"_\".join([\n","        summarize_options(selected_cells),\n","        summarize_options(selected_speeds),\n","        summarize_options(selected_Treatment)\n","    ])\n","\n","    filename = f\"{base}_{selected_options}_{var}.pdf\"\n","    return filename.replace(\" \", \"_\")  # Replace spaces with underscores for file compatibility\n","\n","\n","# Create checkboxes for various attributes\n","cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in merged_tracks_df['Cells'].unique()]\n","flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in merged_tracks_df['Flow_speed'].unique()]\n","Treatment_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in merged_tracks_df['Treatment'].unique()]\n","\n","\n","# Display checkboxes\n","display(widgets.VBox([\n","    widgets.Label('Cells:'),\n","    widgets.GridBox(cells_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n","    widgets.Label('Flow Speed:'),\n","    widgets.GridBox(flow_speed_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n","    widgets.Label('Treatment:'),\n","    widgets.GridBox(Treatment_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4))\n","\n","]))\n","\n","# Convert Flow_speed to string for checkbox matching\n","merged_tracks_df['Flow_speed'] = merged_tracks_df['Flow_speed'].astype(str)\n","\n","# Define the plotting function\n","def plot_selected_vars(button, variable_checkboxes):\n","    print(\"Plotting in progress...\")\n","\n","    # Fetch selected values\n","    selected_cells = [box.description for box in cells_checkboxes if box.value]\n","    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n","    selected_Treatment = [box.description for box in Treatment_checkboxes if box.value]\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","\n","    # Filter dataframe\n","    filtered_df = merged_tracks_df.copy()\n","    filtered_df = filtered_df[filtered_df['Cells'].isin(selected_cells)]\n","    filtered_df = filtered_df[filtered_df['Flow_speed'].isin(selected_speeds)]\n","    filtered_df = filtered_df[filtered_df['Treatment'].isin(selected_Treatment)]\n","\n","    # Initialize matrices for statistics\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df['Condition'].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      filename = create_filename(\"track_parameters_plots\", selected_cells, selected_speeds, selected_Treatment, var)\n","      pdf_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"pdf\", filename)\n","      csv_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"csv\", f\"{filename[:-4]}.csv\")  # Remove '.pdf' and add '.csv'\n","\n","      pdf_pages = PdfPages(pdf_path)\n","\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = filtered_df[filtered_df['Condition'] == cond1][var]\n","        group2 = filtered_df[filtered_df['Condition'] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = cohen_d(new_group1, new_group2)\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = (count_extreme + 1) / (n_iterations + 1)\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(csv_path)\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = filtered_df[['Condition', var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/track_parameters_plots/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = filtered_df[var].quantile(0.25)\n","      Q3 = filtered_df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","\n","    # Plotting\n","      sns.boxplot(x='Condition', y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x='Condition', y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","# Close the PDF\n","      pdf_pages.close()\n","\n","# Display variable checkboxes and button\n","selectable_columns = get_selectable_columns(merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes))\n","display(button)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"smzCyMVMFWwr"},"outputs":[],"source":["# @title #Combine the flow speeds (optional)\n","\n","\n","def categorize_flow_speed(speed):\n","    if speed in ['100','200', '300', 'wash']:\n","        return 'Combined'\n","    else:\n","        return speed\n","\n","# Apply the function to create the Flow_speed_category column\n","Filtered_merged_tracks_df['Flow_speed_category'] = Filtered_merged_tracks_df['Flow_speed'].apply(categorize_flow_speed)\n","\n","# Print unique Flow_speed_category values\n","unique_flow_speed_categories = Filtered_merged_tracks_df['Flow_speed_category'].unique()\n","print(unique_flow_speed_categories)\n","\n","Filtered_merged_tracks_df['Condition_category'] = Filtered_merged_tracks_df['Cells'] + '_' + Filtered_merged_tracks_df['Flow_speed_category'].astype(str) + '_' + Filtered_merged_tracks_df['Treatment']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MtOEIAqcqfmG"},"outputs":[],"source":["# @title ##Plot track parameters (Combined flow speeds)\n","\n","# Import necessary libraries\n","import os\n","import itertools\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","from matplotlib.backends.backend_pdf import PdfPages\n","import ipywidgets as widgets\n","from matplotlib.ticker import FixedLocator\n","\n","# Check and create necessary directories\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots\")\n","\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/pdf\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots/pdf\")\n","\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/csv\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots/csv\")\n","\n","\n","def get_selectable_columns(df):\n","    \"\"\"Get columns that can be plotted.\"\"\"\n","    exclude_cols = ['Condition', 'File_name', 'Flow_speed', 'Cells', 'Treatment', 'Repeat', 'Unique_ID',\n","                    'experiment_nb', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION',\n","                    'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    \"\"\"Display checkboxes for selecting variables.\"\"\"\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3))\n","    ]))\n","    return variable_checkboxes\n","\n","\n","def create_filename(base, selected_cells, selected_speeds, selected_Treatment, var):\n","    \"\"\"Create a unique filename based on selected options.\"\"\"\n","    def summarize_options(options):\n","        if len(options) > 3:\n","            return f\"{len(options)}options\"\n","        return \"_\".join(options)\n","\n","    selected_options = \"_\".join([\n","        summarize_options(selected_cells),\n","        summarize_options(selected_speeds),\n","        summarize_options(selected_Treatment)\n","    ])\n","\n","    filename = f\"{base}_{selected_options}_{var}.pdf\"\n","    return filename.replace(\" \", \"_\")  # Replace spaces with underscores for file compatibility\n","\n","\n","# Create checkboxes for various attributes\n","cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in Filtered_merged_tracks_df['Cells'].unique()]\n","flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in Filtered_merged_tracks_df['Flow_speed_category'].unique()]\n","Treatment_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in Filtered_merged_tracks_df['Treatment'].unique()]\n","\n","\n","# Display checkboxes\n","display(widgets.VBox([\n","    widgets.Label('Cells:'),\n","    widgets.GridBox(cells_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n","    widgets.Label('Flow Speed:'),\n","    widgets.GridBox(flow_speed_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n","    widgets.Label('Treatment:'),\n","    widgets.GridBox(Treatment_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4))\n","\n","]))\n","\n","# Convert Flow_speed to string for checkbox matching\n","Filtered_merged_tracks_df['Flow_speed_category'] = Filtered_merged_tracks_df['Flow_speed_category'].astype(str)\n","\n","# Define the plotting function\n","def plot_selected_vars(button, variable_checkboxes):\n","    print(\"Plotting in progress...\")\n","\n","    # Fetch selected values\n","    selected_cells = [box.description for box in cells_checkboxes if box.value]\n","    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n","    selected_Treatment = [box.description for box in Treatment_checkboxes if box.value]\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","\n","    # Filter dataframe\n","    filtered_df = Filtered_merged_tracks_df.copy()\n","    filtered_df = filtered_df[filtered_df['Cells'].isin(selected_cells)]\n","    filtered_df = filtered_df[filtered_df['Flow_speed_category'].isin(selected_speeds)]\n","    filtered_df = filtered_df[filtered_df['Treatment'].isin(selected_Treatment)]\n","\n","    # Initialize matrices for statistics\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df['Condition_category'].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      filename = create_filename(\"track_parameters_plots\", selected_cells, selected_speeds, selected_Treatment, var)\n","      pdf_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"pdf\", filename)\n","      csv_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"csv\", f\"{filename[:-4]}.csv\")  # Remove '.pdf' and add '.csv'\n","\n","      pdf_pages = PdfPages(pdf_path)\n","\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = filtered_df[filtered_df['Condition_category'] == cond1][var]\n","        group2 = filtered_df[filtered_df['Condition_category'] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = cohen_d(new_group1, new_group2)\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = (count_extreme + 1) / (n_iterations + 1)\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(csv_path)\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = filtered_df[['Condition_category', var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/track_parameters_plots/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = filtered_df[var].quantile(0.25)\n","      Q3 = filtered_df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","\n","    # Plotting\n","      sns.boxplot(x='Condition_category', y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x='Condition_category', y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition_category')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","# Close the PDF\n","      pdf_pages.close()\n","\n","# Display variable checkboxes and button\n","selectable_columns = get_selectable_columns(Filtered_merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes))\n","display(button)\n"]}],"metadata":{"colab":{"collapsed_sections":["Y4-Ft-yNRVCc","49LdP6Mp2HF_","CjF0cU7pc-7g","gydc2ejbJKZO","joRI14WVUPuM"],"machine_shape":"hm","provenance":[{"file_id":"1mj0t_5qwHRtMsiDv6kl8AGwksWsSxJ97","timestamp":1698490066065}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}