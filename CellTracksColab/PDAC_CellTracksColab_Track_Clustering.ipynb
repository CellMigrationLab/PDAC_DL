{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **PDAC CellTracksColab — Track Clustering & Co-Arrest**\n",
        "---\n",
        "\n",
        "<font size=4>This notebook is part of the **CellTracksColab** suite for analyzing cancer cell–endothelium interactions in flow. It provides two complementary, lightweight modules you can run end-to-end on your tracked data.\n",
        "\n",
        "**Project:** [CellMigrationLab/CellTracksColab](https://github.com/CellMigrationLab/CellTracksColab)\n",
        "\n",
        "---\n",
        "\n",
        "### 1) Spatial Clustering (Ripley’s L)\n",
        "- **Goal:** Test whether arrested cells are spatially clustered within each FOV.\n",
        "- **Input:** Arrest points per FOV.\n",
        "- **Output:** L(r) curves, per-FOV and grouped summaries by metadata (Cells / Treatment / Condition), and compact plots.\n",
        "\n",
        "### 2) Space–Time Co-Arrest\n",
        "- **Goal:** Quantify arrests that occur **together** in space and time and characterize **cluster size**.\n",
        "- **Definition:** For each track, take the first frame where speed ≤ threshold as the arrest event. An event is “together” if ≥1 other event lies within a space–time neighborhood (**ΔT**, **r**), with **r = k × diameter(Cells)** to normalize across cell sizes.\n",
        "- **Output:** Co-arrest fraction per FOV, event-level cluster sizes (size≥2), distributions by Cells / Treatment / Condition / Cells×Treatment, time-colored arrest maps, and QC (k–ΔT sensitivity; flag vs. component agreement).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Notebook creation:** [Guillaume Jacquemet](https://cellmig.org/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1: Prepare the session and load your data**\n",
        "--------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rAP0ahCzn1V6"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install\n",
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install hdbscan\n",
        "!pip -q install umap-learn\n",
        "!pip -q install plotly\n",
        "!pip -q install tqdm\n",
        "\n",
        "!git clone https://github.com/CellMigrationLab/CellTracksColab.git\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import itertools\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import requests\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import requests\n",
        "import ipywidgets as widgets\n",
        "import warnings\n",
        "import scipy.stats as stats\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from ipywidgets import Dropdown, interact,Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, clear_output\n",
        "from scipy.spatial import ConvexHull\n",
        "from scipy.spatial.distance import cosine, pdist\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.stats import zscore, ks_2samp\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from multiprocessing import Pool\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib.colors import LogNorm\n",
        "sys.path.append(\"../\")\n",
        "sys.path.append(\"CellTracksColab/\")\n",
        "\n",
        "import celltracks\n",
        "from celltracks import *\n",
        "from celltracks.Track_Plots import *\n",
        "from celltracks.BoxPlots_Statistics import *\n",
        "from celltracks.Track_Metrics import *\n",
        "\n",
        "\n",
        "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=500000):\n",
        "    \"\"\"Save a DataFrame with a progress bar and gzip compression.\"\"\"\n",
        "\n",
        "    # Estimating the number of chunks based on the provided chunk size\n",
        "    num_chunks = int(len(df) / chunk_size) + 1\n",
        "\n",
        "    # Create a tqdm instance for progress tracking\n",
        "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
        "        # Open the file for writing with gzip compression\n",
        "        with gzip.open(path, \"wt\") as f:\n",
        "            # Write the header once at the beginning\n",
        "            df.head(0).to_csv(f, index=False)\n",
        "\n",
        "            for chunk in np.array_split(df, num_chunks):\n",
        "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
        "                pbar.update(len(chunk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDAwkSOo1gV"
      },
      "source": [
        "## **1.3. Compile your data or load existing dataframes**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQKXq3giI3nX"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Provide the path to the dataset:\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "#@markdown ###You have existing dataframes, provide the path to your:\n",
        "\n",
        "Track_table = ''  # @param {type: \"string\"}\n",
        "Spot_table = ''  # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ###Provide the path to your Result folder\n",
        "\n",
        "Results_Folder = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "if not Results_Folder:\n",
        "    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n",
        "\n",
        "if not os.path.exists(Results_Folder):\n",
        "    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Print the location of the result folder\n",
        "print(f\"Result folder is located at: {Results_Folder}\")\n",
        "\n",
        "def validate_tracks_df(df):\n",
        "    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n",
        "    required_columns = ['TRACK_ID']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n",
        "            return False\n",
        "\n",
        "    # Additional data type checks or value ranges can be added here\n",
        "    return True\n",
        "\n",
        "def validate_spots_df(df):\n",
        "    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n",
        "    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n",
        "            return False\n",
        "\n",
        "    # Additional data type checks or value ranges can be added here\n",
        "    return True\n",
        "\n",
        "def check_unique_id_match(df1, df2):\n",
        "    df1_ids = set(df1['Unique_ID'])\n",
        "    df2_ids = set(df2['Unique_ID'])\n",
        "\n",
        "    # Check if the IDs in the two dataframes match\n",
        "    if df1_ids == df2_ids:\n",
        "        print(\"The Unique_ID values in both dataframes match perfectly!\")\n",
        "    else:\n",
        "        missing_in_df1 = df2_ids - df1_ids\n",
        "        missing_in_df2 = df1_ids - df2_ids\n",
        "\n",
        "        if missing_in_df1:\n",
        "            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n",
        "            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n",
        "\n",
        "        if missing_in_df2:\n",
        "            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n",
        "            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n",
        "\n",
        "# For existing dataframes\n",
        "if Track_table:\n",
        "    print(\"Loading track table file....\")\n",
        "    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n",
        "    if not validate_tracks_df(merged_tracks_df):\n",
        "        print(\"Error: Validation failed for loaded tracks dataframe.\")\n",
        "\n",
        "if Spot_table:\n",
        "    print(\"Loading spot table file....\")\n",
        "    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n",
        "    if not validate_spots_df(merged_spots_df):\n",
        "        print(\"Error: Validation failed for loaded spots dataframe.\")\n",
        "\n",
        "check_for_nans(merged_spots_df, \"merged_spots_df\")\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nT0t5jqsGRoG"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check Metadata\n",
        "\n",
        "\n",
        "# Define the metadata columns that are expected to have identical values for each filename\n",
        "metadata_columns = ['Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']\n",
        "\n",
        "# Group the DataFrame by 'File_name' and then check if all entries within each group are identical\n",
        "consistent_metadata = True\n",
        "for name, group in merged_tracks_df.groupby('File_name'):\n",
        "    for col in metadata_columns:\n",
        "        if not group[col].nunique() == 1:\n",
        "            consistent_metadata = False\n",
        "            print(f\"Inconsistency found for file: {name} in column: {col}\")\n",
        "            break  # Stop checking other columns for this group and move to the next file\n",
        "    if not consistent_metadata:\n",
        "        break  # Stop the entire process if any inconsistency is found\n",
        "\n",
        "if consistent_metadata:\n",
        "    print(\"All files have consistent metadata across the specified columns.\")\n",
        "else:\n",
        "    print(\"There are inconsistencies in the metadata. Please check the output for details.\")\n",
        "\n",
        "# Drop duplicates based on the 'File_name' to get a unique list of filenames and their metadata\n",
        "unique_files_df = merged_tracks_df.drop_duplicates(subset=['File_name'])[['File_name', 'Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']]\n",
        "\n",
        "# Reset the index to clean up the DataFrame\n",
        "unique_files_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the resulting DataFrame in a nicely formatted HTML table\n",
        "unique_files_df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it already contains 'Conditions' and 'Repeats' columns.\n",
        "\n",
        "# Group by 'Conditions' and 'Repeats' and count the occurrences\n",
        "grouped = unique_files_df.groupby(['Condition', 'Repeat']).size().reset_index(name='counts')\n",
        "\n",
        "# Check if any combinations have a count greater than 1, which means they are not unique\n",
        "non_unique_combinations = grouped[grouped['counts'] > 1]\n",
        "\n",
        "# Print the non-unique combinations\n",
        "if not non_unique_combinations.empty:\n",
        "    print(\"There are non-unique combinations of Conditions and Repeats:\")\n",
        "    print(non_unique_combinations)\n",
        "else:\n",
        "    print(\"All combinations of Conditions and Repeats are unique.\")\n",
        "\n",
        "check_unique_id_match(merged_spots_df, merged_tracks_df)\n",
        "\n",
        "\n",
        "# Group the DataFrame by 'Cells', 'ILbeta', 'Repeat' and then check if there are 4 unique 'Flow_speed' values for each group\n",
        "consistent_flow_speeds = True\n",
        "for (cells, ilbeta, repeat), group in merged_tracks_df.groupby(['Cells', 'Treatment', 'Repeat']):\n",
        "    if group['Flow_speed'].nunique() != 4:\n",
        "        consistent_flow_speeds = False\n",
        "        print(f\"Inconsistency found for Cells: {cells}, Treatment: {Treatment_conditions}, Repeat: {repeat} - Expected 4 Flow_speeds, found {group['Flow_speed'].nunique()}\")\n",
        "        break  # Stop the entire process if any inconsistency is found\n",
        "\n",
        "if consistent_flow_speeds:\n",
        "    print(\"Each combination of 'Cells', 'Treatment', 'Repeat' has exactly 4 different 'Flow_speed' values.\")\n",
        "else:\n",
        "    print(\"There are inconsistencies in 'Flow_speed' values. Please check the output for details.\")\n",
        "\n",
        "\n",
        "unique_cells = unique_files_df['Cells'].unique()\n",
        "unique_flow_speeds = unique_files_df['Flow_speed'].unique()\n",
        "unique_Treatment = unique_files_df['Treatment'].unique()\n",
        "unique_conditions = unique_files_df['Condition'].unique()\n",
        "\n",
        "print(\"Unique Cells:\", unique_cells)\n",
        "print(\"Unique Flow Speeds:\", unique_flow_speeds)\n",
        "print(\"Unique Treatment:\", unique_Treatment)\n",
        "print(\"Unique Conditions:\", unique_conditions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5YJ9V468HwJ"
      },
      "source": [
        "## **1.4. Filter tracks shorter than 50 spots**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mA8GBhFy8vd6"
      },
      "outputs": [],
      "source": [
        "# @title ##Filter tracks shorter than 50 spots\n",
        "\n",
        "\n",
        "merged_tracks_df = merged_tracks_df[merged_tracks_df['NUMBER_SPOTS'] >= 50]\n",
        "merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52STmnv43d45"
      },
      "source": [
        "## **1.5. Visualise your tracks**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AE881uJW5ukQ"
      },
      "outputs": [],
      "source": [
        "# @title ##Run the cell and choose the file you want to inspect\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
        "    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# Create a Dropdown widget with the filenames\n",
        "filename_dropdown = widgets.Dropdown(\n",
        "    options=filenames,\n",
        "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
        "    description='File Name:',\n",
        ")\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "# Link the Dropdown widget to the plotting function\n",
        "interact(plot_coordinates, filename=filename_dropdown)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtIm-c77u5ha"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 2: Assess spatial clustering using Ripley's L function**\n",
        "--------------------------------------------------------\n",
        "\n",
        "<font size = 4>**Spatial Clustering Analyses:** For a comprehensive guide on performing spatial clustering analysis with CellTracksColab, visit the project's [Spatial Clustering analyses wiki page](https://github.com/CellMigrationLab/CellTracksColab/wiki/Spatial-Clustering-analyses).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6I305XNQL69"
      },
      "source": [
        "## **2.1. Filter tracks with Track_MIN_speed**\n",
        "\n",
        "<font size = 4>This section enables to filter the dataset so that we only keep arresting tracks.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-mbDA3xfNRqH"
      },
      "outputs": [],
      "source": [
        "# @title ##Filter tracks using Min Speed\n",
        "\n",
        "\n",
        "merged_tracks_df = merged_tracks_df[merged_tracks_df['Min Speed'] <= 5]\n",
        "merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Py1TcJwqwB"
      },
      "source": [
        "## **2.2. Visualise where cells slow down in each tracks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JVheHy0OVlhG"
      },
      "outputs": [],
      "source": [
        "# @title ##Run the cell and choose the file you want to inspect to visualise track and choosen point\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
        "    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# Create a Dropdown widget with the filenames\n",
        "filename_dropdown = widgets.Dropdown(\n",
        "    options=filenames,\n",
        "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
        "    description='File Name:',\n",
        ")\n",
        "\n",
        "# User-defined speed threshold\n",
        "speed_threshold = 3  # Replace with the desired threshold value\n",
        "\n",
        "def find_point_below_threshold(track):\n",
        "    below_threshold = track[track['Speed'] < speed_threshold]\n",
        "    return below_threshold.iloc[0] if not below_threshold.empty else None\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "            # Find and mark the slowdown point\n",
        "            slowdown_point = find_point_below_threshold(unique_df)\n",
        "            if slowdown_point is not None:\n",
        "                plt.scatter(slowdown_point['POSITION_X'], slowdown_point['POSITION_Y'], color='red', s=50)\n",
        "            #else:\n",
        "                #print(f\"No slowdown point found for track {unique_id}\")\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "\n",
        "# Link the Dropdown widget to the plotting function\n",
        "interact(plot_coordinates, filename=filename_dropdown)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiY-Iq5HcwA0"
      },
      "source": [
        "## **2.3 Identify the coordinates where cells slow down**\n",
        "\n",
        "<font size = 4>Here we identify the where circulating cells slow down on the endothelial monolayer to identify possible hotspots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "Fn-E8ZoIQG2m"
      },
      "outputs": [],
      "source": [
        "# @title ##Run the cell to identify the coordinates to use for the clustering analysis\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "if not os.path.exists(Results_Folder + \"/Slow_down_coordinates\"):\n",
        "    os.makedirs(Results_Folder + \"/Slow_down_coordinates\")  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# User-defined speed threshold\n",
        "speed_threshold = 5  # Replace with the desired threshold value\n",
        "\n",
        "def find_point_below_threshold(track):\n",
        "    below_threshold = track[track['Speed'] < speed_threshold]\n",
        "    return below_threshold.iloc[0] if not below_threshold.empty else None\n",
        "\n",
        "def plot_slowdown_points(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "\n",
        "            # Find and mark the slowdown point\n",
        "            slowdown_point = find_point_below_threshold(unique_df)\n",
        "            if slowdown_point is not None:\n",
        "                plt.scatter(slowdown_point['POSITION_X'], slowdown_point['POSITION_Y'], color='red', s=50, label=f'Track {unique_id}')\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Slowdown Points for {filename}')\n",
        "        plt.savefig(f\"{Results_Folder}/Slow_down_coordinates/Slowdown_Points_{filename}.pdf\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "# Loop through each file and generate the plot for slowdown points\n",
        "for filename in filenames:\n",
        "    plot_slowdown_points(filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ldnL9HwA51"
      },
      "source": [
        "## **2.4 Compute the Ripley's L function for each FOV**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "Y0Ikco9nMopc"
      },
      "outputs": [],
      "source": [
        "# @title ##Compute Ripley's L function for each FOV\n",
        "\n",
        "# User-defined speed threshold\n",
        "speed_threshold = 5\n",
        "\n",
        "# Check and create necessary directories\n",
        "if not os.path.exists(f\"{Results_Folder}/Track_Clustering\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Track_Clustering\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import distance_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define Ripley's K function\n",
        "def ripley_k(points, r, area):\n",
        "    n = len(points)\n",
        "    d_matrix = distance_matrix(points, points)\n",
        "    sum_indicator = np.sum(d_matrix < r) - n  # Subtract n to exclude self-pairs\n",
        "\n",
        "    K_r = (area / (n ** 2)) * sum_indicator\n",
        "\n",
        "    # Check if K_r is negative and print relevant information\n",
        "    if K_r < 0:\n",
        "        print(\"Negative K_r encountered!\")\n",
        "        print(\"Distance matrix:\", d_matrix)\n",
        "        print(\"Sum indicator:\", sum_indicator)\n",
        "        print(\"Area:\", area, \"Number of points:\", n, \"Distance threshold r:\", r)\n",
        "\n",
        "    return K_r\n",
        "\n",
        "\n",
        "# Define Ripley's L function\n",
        "\n",
        "def ripley_l(points, r, area):\n",
        "    K_r = ripley_k(points, r, area)\n",
        "    # Check if K_r has negative values\n",
        "    if np.any(K_r < 0):\n",
        "        print(\"Warning: Negative value encountered in K_r\")\n",
        "\n",
        "    L_r = np.sqrt(K_r / np.pi) - r\n",
        "    return L_r\n",
        "\n",
        "def find_point_below_threshold(track):\n",
        "  below_threshold = track[track['Speed'] < speed_threshold]\n",
        "  if not below_threshold.empty:\n",
        "    return below_threshold.iloc[0][['POSITION_X', 'POSITION_Y']]\n",
        "  return pd.Series([np.nan, np.nan], index=['POSITION_X', 'POSITION_Y'])\n",
        "\n",
        "# Define area based on your dataset's extent\n",
        "area = (merged_spots_df['POSITION_X'].max() - merged_spots_df['POSITION_X'].min()) * \\\n",
        "       (merged_spots_df['POSITION_Y'].max() - merged_spots_df['POSITION_Y'].min())\n",
        "\n",
        "# Define r values\n",
        "r_values = np.linspace(1, 250, 250)  # Adjust as needed\n",
        "\n",
        "# Compute Ripley's L function for each FOV\n",
        "l_values_per_fov_slow = {}\n",
        "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc=\"Processing FOVs\"):\n",
        "    # Sort each track by POSITION_T\n",
        "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
        "\n",
        "    representative_points = group.groupby('TRACK_ID').apply(find_point_below_threshold).dropna()\n",
        "    if not representative_points.empty:\n",
        "        l_values = [ripley_l(representative_points.values, r, area) for r in tqdm(r_values, desc=f\"Calculating L for {file_name}\")]\n",
        "        l_values_per_fov_slow[file_name] = l_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE8uWDxswKBC"
      },
      "source": [
        "## **2.5 Compute Monte Carlo simulations for each FOV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "INwTjl0JyM70"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# @title ##Compute Monte Carlo simulations for each FOV\n",
        "\n",
        "\n",
        "# Simulate random points for Monte Carlo simulations\n",
        "def simulate_random_points(num_points, x_range, y_range):\n",
        "    x_coords = np.random.uniform(x_range[0], x_range[1], num_points)\n",
        "    y_coords = np.random.uniform(y_range[0], y_range[1], num_points)\n",
        "    return np.column_stack((x_coords, y_coords))\n",
        "\n",
        "# Initialize simulated_l_values as an empty dictionary\n",
        "simulated_l_values_dict_slow = {}\n",
        "\n",
        "# Perform Monte Carlo simulations for significance testing\n",
        "confidence_envelopes_slow = {}\n",
        "for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc='Processing FOVs'):\n",
        "\n",
        "    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n",
        "    representative_points = group.groupby('TRACK_ID').apply(find_point_below_threshold).dropna()\n",
        "\n",
        "    simulations = [simulate_random_points(len(representative_points),\n",
        "                                          (merged_spots_df['POSITION_X'].min(), merged_spots_df['POSITION_X'].max()),\n",
        "                                          (merged_spots_df['POSITION_Y'].min(), merged_spots_df['POSITION_Y'].max()))\n",
        "                   for _ in tqdm(range(10), desc=f'Simulating for {file_name}', leave=False)]\n",
        "\n",
        "    simulated_l_values = [[ripley_l(points, r, area) for r in r_values] for points in simulations]\n",
        "    simulated_l_values_dict_slow[file_name] = simulated_l_values  # Store the simulated values in the dictionary\n",
        "\n",
        "    lower_bound = np.percentile(simulated_l_values, 2.5, axis=0)\n",
        "    upper_bound = np.percentile(simulated_l_values, 97.5, axis=0)\n",
        "    confidence_envelopes_slow[file_name] = (lower_bound, upper_bound)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EYrmDpzwXR2"
      },
      "source": [
        "## **2.6 Plot the results for each FOV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FYkImji3TPOg"
      },
      "outputs": [],
      "source": [
        "# @title ##Plots for each FOV - Slow down\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualization of Ripley's L function with confidence envelopes\n",
        "for file_name, l_values in l_values_per_fov_slow.items():\n",
        "    # Retrieve the confidence envelope for the current file\n",
        "    lower_bound, upper_bound = confidence_envelopes_slow.get(file_name, (None, None))\n",
        "\n",
        "    # Only proceed if the confidence envelope exists\n",
        "    if lower_bound is not None and upper_bound is not None:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(r_values, l_values, label=f'L(r) for {file_name}')\n",
        "        plt.fill_between(r_values, lower_bound, upper_bound, color='gray', alpha=0.5)\n",
        "        plt.xlabel('Radius (r)')\n",
        "        plt.ylabel(\"Ripley's L Function\")\n",
        "        plt.title(f\"Ripley's L Function - {file_name}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Save the plot as a PDF in the specified folder\n",
        "        pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/{file_name}.pdf\")\n",
        "        plt.savefig(pdf_path,bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()  # Close the plot to free memory\n",
        "    else:\n",
        "        print(f\"No confidence envelope data available for {file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPxKl7HNwd-q"
      },
      "source": [
        "## **2.7 Define a specific radius and save as dataframe**\n",
        "\n",
        "This is performed to compare FOV and conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "hQlY8X-nJqYb"
      },
      "outputs": [],
      "source": [
        "# @title ##Define a specific radius and save as dataframe - Slow down\n",
        "\n",
        "\n",
        "# Define the specific radius for comparison\n",
        "specific_radius = 50  # Replace with your chosen radius\n",
        "\n",
        "# Extract L values at the specific radius\n",
        "specific_radius_index = np.argmin(np.abs(r_values - specific_radius))  # Find the index of the closest radius value\n",
        "l_values_at_specific_radius_slow = {fov: l_values[specific_radius_index] for fov, l_values in l_values_per_fov_slow.items()}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(l_values_at_specific_radius_slow.keys(), l_values_at_specific_radius_slow.values())\n",
        "plt.xlabel('Field of View')\n",
        "plt.ylabel(f\"Ripley's L at radius {specific_radius}\")\n",
        "plt.title(f\"Comparison of Ripley's L Function at Radius {specific_radius} Across Different FOVs\")\n",
        "plt.xticks(rotation=45)\n",
        "# Save the plot as a PDF in the specified folder\n",
        "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_at_specific_radius_slow.pdf\")\n",
        "plt.savefig(pdf_path, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Create DataFrame with confidence envelopes, median, and L values at the specific radius\n",
        "rows = []\n",
        "for fov, (lower_bound, upper_bound) in confidence_envelopes_slow.items():\n",
        "    l_value = l_values_per_fov_slow[fov][specific_radius_index]\n",
        "    lower = lower_bound[specific_radius_index]\n",
        "    upper = upper_bound[specific_radius_index]\n",
        "\n",
        "    # Retrieve simulated L values for the FOV\n",
        "    simulated_l_values_for_fov_slow = simulated_l_values_dict_slow.get(fov, [])\n",
        "\n",
        "    # Calculate median if simulated L values are available for the FOV\n",
        "    if simulated_l_values_for_fov_slow:\n",
        "        median_vals = [l_vals[specific_radius_index] for l_vals in simulated_l_values_for_fov_slow]\n",
        "        median = np.median(median_vals) if median_vals else np.nan\n",
        "    else:\n",
        "        median = np.nan\n",
        "\n",
        "    rows.append([fov, l_value, lower, upper, median])\n",
        "\n",
        "confidence_df = pd.DataFrame(rows, columns=['File_name', 'Ripley_L_at_Specific_Radius_slow', 'Lower_Bound_slow', 'Upper_Bound_slow', 'Median_slow'])\n",
        "\n",
        "# Merge with additional information\n",
        "additional_info_df = merged_tracks_df[['File_name', 'Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']].drop_duplicates('File_name')\n",
        "merged_df = pd.merge(confidence_df, additional_info_df, left_on='File_name', right_on='File_name')\n",
        "\n",
        "# Save the merged DataFrame to a CSV file\n",
        "merged_df.to_csv(f\"{Results_Folder}/Track_Clustering/ripleys_l_values.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j465mCknws6q"
      },
      "source": [
        "## **2.8 Ripley's L Values Across conditions and cells**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "hN0CAyN3Kr43"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# @title ##Comparison of Ripley\\'s L Values Across Conditions\n",
        "\n",
        "# Convert 'Condition' to string if it's not already\n",
        "merged_df['Condition'] = merged_df['Condition'].astype(str)\n",
        "\n",
        "# Create the box plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=merged_df, x='Condition', y='Ripley_L_at_Specific_Radius_slow')\n",
        "\n",
        "# Overlay the Monte Carlo simulation results\n",
        "for condition in merged_df['Condition'].unique():\n",
        "    condition_data = merged_df[merged_df['Condition'] == condition]\n",
        "\n",
        "    # Plot median values\n",
        "    medians = condition_data['Median_slow']\n",
        "    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n",
        "\n",
        "    # Handle NaN values and calculate mean and error only for non-NaN values\n",
        "    valid_data = condition_data.dropna(subset=['Median_slow', 'Lower_Bound_slow', 'Upper_Bound_slow'])\n",
        "    if not valid_data.empty:\n",
        "        median_mean = valid_data['Median_slow'].mean()\n",
        "        lower_mean = valid_data['Lower_Bound_slow'].mean()\n",
        "        upper_mean = valid_data['Upper_Bound_slow'].mean()\n",
        "        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n",
        "\n",
        "        # Check if yerr contains valid data before plotting\n",
        "        if not any(np.isnan(yerr)):\n",
        "            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Ripley\\'s L at Specific Radius')\n",
        "plt.title('Comparison of Ripley\\'s L Values Across Conditions with Monte Carlo Simulation Results')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the figure before showing it\n",
        "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_Conditions_slow.pdf\")\n",
        "plt.savefig(pdf_path, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "himdQhR-Kr4_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# @title ##Comparison of Ripley\\'s L Values Across Cells\n",
        "\n",
        "# Convert 'Condition' to string if it's not already\n",
        "merged_df['Cells'] = merged_df['Cells'].astype(str)\n",
        "\n",
        "# Create the box plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=merged_df, x='Cells', y='Ripley_L_at_Specific_Radius_slow')\n",
        "\n",
        "# Overlay the Monte Carlo simulation results\n",
        "for condition in merged_df['Cells'].unique():\n",
        "    condition_data = merged_df[merged_df['Cells'] == condition]\n",
        "\n",
        "    # Plot median values\n",
        "    medians = condition_data['Median_slow']\n",
        "    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n",
        "\n",
        "    # Handle NaN values and calculate mean and error only for non-NaN values\n",
        "    valid_data = condition_data.dropna(subset=['Median_slow', 'Lower_Bound_slow', 'Upper_Bound_slow'])\n",
        "    if not valid_data.empty:\n",
        "        median_mean = valid_data['Median_slow'].mean()\n",
        "        lower_mean = valid_data['Lower_Bound_slow'].mean()\n",
        "        upper_mean = valid_data['Upper_Bound_slow'].mean()\n",
        "        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n",
        "\n",
        "        # Check if yerr contains valid data before plotting\n",
        "        if not any(np.isnan(yerr)):\n",
        "            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Ripley\\'s L at Specific Radius')\n",
        "plt.title('Comparison of Ripley\\'s L Values Across Conditions with Monte Carlo Simulation Results')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the figure before showing it\n",
        "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_Cells_slow.pdf\")\n",
        "plt.savefig(pdf_path, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fooHjAmK7I8O"
      },
      "outputs": [],
      "source": [
        "# @title ##Comparison of Ripley\\'s L Values Across Cells and Treatment\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Convert 'Cells' and 'Treatment' to string if they are not already\n",
        "merged_df['Cells'] = merged_df['Cells'].astype(str)\n",
        "merged_df['Treatment'] = merged_df['Treatment'].astype(str)\n",
        "\n",
        "# Create a combined factor for Cells and Silencing\n",
        "merged_df['Cells_Treatment'] = merged_df['Cells'] + \"_\" + merged_df['Treatment']\n",
        "\n",
        "# Create the box plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(data=merged_df, x='Cells_Treatment', y='Ripley_L_at_Specific_Radius_slow')\n",
        "\n",
        "# Overlay the Monte Carlo simulation results\n",
        "for condition in merged_df['Cells_Treatment'].unique():\n",
        "    condition_data = merged_df[merged_df['Cells_Treatment'] == condition]\n",
        "\n",
        "    # Plot median values\n",
        "    medians = condition_data['Median_slow']\n",
        "    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n",
        "\n",
        "    # Handle NaN values and calculate mean and error only for non-NaN values\n",
        "    valid_data = condition_data.dropna(subset=['Median_slow', 'Lower_Bound_slow', 'Upper_Bound_slow'])\n",
        "    if not valid_data.empty:\n",
        "        median_mean = valid_data['Median_slow'].mean()\n",
        "        lower_mean = valid_data['Lower_Bound_slow'].mean()\n",
        "        upper_mean = valid_data['Upper_Bound_slow'].mean()\n",
        "        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n",
        "\n",
        "        # Check if yerr contains valid data before plotting\n",
        "        if not any(np.isnan(yerr)):\n",
        "            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Cells and Treatment')\n",
        "plt.ylabel('Ripley\\'s L at Specific Radius')\n",
        "plt.title('Comparison of Ripley\\'s L Values Across Cells and Silencing Conditions with Monte Carlo Simulation Results')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the figure before showing it\n",
        "pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_Cells_Treatment_slow.pdf\")\n",
        "plt.savefig(pdf_path, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 3: Cluster vs single cell arrest (space–time co-arrest)**\n",
        "--------------------------------------------------------\n",
        "\n",
        "<font size = 4>"
      ],
      "metadata": {
        "id": "Vr12mvbniP7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. Filter tracks with Track_MIN_speed**\n",
        "\n",
        "<font size = 4>This section enables to filter the dataset so that we only keep arresting tracks."
      ],
      "metadata": {
        "id": "ePDMLp0MoYo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Filter tracks using Min Speed\n",
        "\n",
        "\n",
        "merged_tracks_df = merged_tracks_df[merged_tracks_df['Min Speed'] <= 5]\n",
        "merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "30RYCLA8iaP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 Identify the coordinates where cells slow down**\n",
        "\n",
        "<font size = 4>Here we identify the where circulating cells slow down on the endothelial monolayer to identify possible hotspots.\n"
      ],
      "metadata": {
        "id": "4jz1FXgbitgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Slowdown maps colored by time\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# --- Output folder (different from Slow_down_coordinates) ---\n",
        "OUT_DIR = os.path.join(Results_Folder, \"Slowdown_TimeColored\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- User-defined threshold (keep consistent with your filter above) ---\n",
        "speed_threshold = 5  # same units as merged_spots_df['Speed']\n",
        "\n",
        "def _sanitize_filename(s):\n",
        "    \"\"\"Make a safe filename fragment from the FOV name.\"\"\"\n",
        "    return str(s).replace(os.sep, \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "def get_first_slowdown_points_for_fov(filename, df_spots, threshold):\n",
        "    \"\"\"\n",
        "    For a given FOV (File_name), return a dataframe with the first point\n",
        "    where each track's Speed drops below `threshold`.\n",
        "    Columns: Unique_ID, File_name, t0 (POSITION_T), x0 (POSITION_X), y0 (POSITION_Y)\n",
        "    \"\"\"\n",
        "    fov_df = df_spots[df_spots['File_name'] == filename]\n",
        "    events = []\n",
        "    # groupby Unique_ID to find first slowdown per track\n",
        "    for uid, track in fov_df.groupby('Unique_ID', sort=False):\n",
        "        track = track.sort_values('POSITION_T')\n",
        "        below = track[track['Speed'] <= threshold]\n",
        "        if not below.empty:\n",
        "            row = below.iloc[0]\n",
        "            events.append({\n",
        "                'Unique_ID': uid,\n",
        "                'File_name': filename,\n",
        "                't0': float(row['POSITION_T']),\n",
        "                'x0': float(row['POSITION_X']),\n",
        "                'y0': float(row['POSITION_Y'])\n",
        "            })\n",
        "    return pd.DataFrame(events)\n",
        "\n",
        "# --- Build slowdown events for all FOVs & save time-colored plots ---\n",
        "all_events = []\n",
        "filenames = merged_spots_df['File_name'].dropna().unique()\n",
        "\n",
        "# If you prefer a consistent color scale across FOVs, set this to True.\n",
        "USE_GLOBAL_TIME_SCALE = False\n",
        "\n",
        "# First pass (optional) to compute global t-range for a consistent colorbar\n",
        "tmin_global, tmax_global = None, None\n",
        "if USE_GLOBAL_TIME_SCALE:\n",
        "    tmp_list = []\n",
        "    for fn in filenames:\n",
        "        ev = get_first_slowdown_points_for_fov(fn, merged_spots_df, speed_threshold)\n",
        "        if not ev.empty:\n",
        "            tmp_list.append(ev['t0'])\n",
        "    if tmp_list:\n",
        "        tmin_global = float(pd.concat(tmp_list).min())\n",
        "        tmax_global = float(pd.concat(tmp_list).max())\n",
        "\n",
        "for fn in filenames:\n",
        "    ev = get_first_slowdown_points_for_fov(fn, merged_spots_df, speed_threshold)\n",
        "    all_events.append(ev)\n",
        "\n",
        "    # Save per-FOV CSV (handy for later clustering step)\n",
        "    if not ev.empty:\n",
        "        ev_out = os.path.join(OUT_DIR, f\"slowdown_events_{_sanitize_filename(fn)}.csv\")\n",
        "        ev.to_csv(ev_out, index=False)\n",
        "\n",
        "        # Plot: dots colored by slowdown time\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        if USE_GLOBAL_TIME_SCALE and (tmin_global is not None):\n",
        "            norm = mpl.colors.Normalize(vmin=tmin_global, vmax=tmax_global)\n",
        "        else:\n",
        "            norm = None\n",
        "\n",
        "        sc = ax.scatter(ev['x0'], ev['y0'],\n",
        "                        c=ev['t0'],\n",
        "                        cmap='viridis',\n",
        "                        norm=norm,\n",
        "                        s=40,\n",
        "                        edgecolors='none')\n",
        "        cbar = plt.colorbar(sc, ax=ax)\n",
        "        cbar.set_label(\"First slowdown time (POSITION_T)\")\n",
        "\n",
        "        ax.set_xlabel(\"POSITION_X\")\n",
        "        ax.set_ylabel(\"POSITION_Y\")\n",
        "        ax.set_title(f\"Slowdown points colored by time (< {speed_threshold})\\n{fn}\")\n",
        "\n",
        "        fig.tight_layout()\n",
        "        out_pdf = os.path.join(OUT_DIR, f\"Slowdown_TimeColored_{_sanitize_filename(fn)}.pdf\")\n",
        "        fig.savefig(out_pdf)\n",
        "        plt.close(fig)\n",
        "\n",
        "# Save an all-FOV events table (optional convenience)\n",
        "if len(all_events):\n",
        "    slowdown_events_df = pd.concat(all_events, ignore_index=True)\n",
        "    slowdown_events_df.to_csv(os.path.join(OUT_DIR, \"slowdown_events_all.csv\"), index=False)\n",
        "\n",
        "print(f\"Saved time-colored slowdown maps and CSVs to: {OUT_DIR}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jFJzf_q4if0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 Identification of co-arrest**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-bLHBM3zoeft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Co-arrest fraction (radius = k × cell diameter per FOV)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- PARAMETERS ----\n",
        "DELTA_T_UNITS = 0.08   # time window; SAME UNITS as POSITION_T\n",
        "# One diameter (µm) per cell type in merged_tracks_df['Cells']  <-- FILL THESE\n",
        "DIAMETER_MAP_UM = {\n",
        "     \"AsPc1\": 17,\n",
        "     \"MiaPaca-2\": 20,\n",
        "    \"Panc10\": 20,\n",
        "     \"Monocyte\": 15,\n",
        "     \"Neutrophil\": 12\n",
        "}\n",
        "K_RADIUS = 1.0  # neighborhood size in cell diameters (try 0.75 / 1.0 / 1.5)\n",
        "\n",
        "# ---- INPUT EVENTS ----\n",
        "EVENTS_DIR = os.path.join(Results_Folder, \"Slowdown_TimeColored\")\n",
        "OUT_DIR_CO = os.path.join(Results_Folder, \"CoArrest_TimeWindow\")\n",
        "os.makedirs(OUT_DIR_CO, exist_ok=True)\n",
        "\n",
        "def load_slowdown_events(events_dir: str) -> pd.DataFrame:\n",
        "    all_path = os.path.join(events_dir, \"slowdown_events_all.csv\")\n",
        "    if os.path.exists(all_path):\n",
        "        return pd.read_csv(all_path)\n",
        "    frames = []\n",
        "    for fn in os.listdir(events_dir):\n",
        "        if fn.startswith(\"slowdown_events_\") and fn.endswith(\".csv\"):\n",
        "            frames.append(pd.read_csv(os.path.join(events_dir, fn)))\n",
        "    if frames:\n",
        "        return pd.concat(frames, ignore_index=True)\n",
        "    raise RuntimeError(\"No slowdown events found. Run the slowdown export first.\")\n",
        "\n",
        "def co_arrest_flags_for_fov(ev_fov: pd.DataFrame, r: float, dt: float) -> np.ndarray:\n",
        "    n = len(ev_fov)\n",
        "    if n <= 1:\n",
        "        return np.zeros(n, dtype=bool)\n",
        "    X = ev_fov[['x0','y0']].to_numpy(float)\n",
        "    T = ev_fov['t0'].to_numpy(float)\n",
        "    D = cdist(X, X)\n",
        "    dT = np.abs(T[:, None] - T[None, :])\n",
        "    A = (D <= r) & (dT <= dt)\n",
        "    np.fill_diagonal(A, False)\n",
        "    return A.any(axis=1)\n",
        "\n",
        "# ---- Build per-FOV radius from cell diameter ----\n",
        "meta_by_fov_cells = (\n",
        "    merged_tracks_df\n",
        "    .groupby('File_name', as_index=False)[['Cells']]\n",
        "    .agg(lambda s: s.iloc[0])\n",
        ")\n",
        "meta_by_fov_cells['Cells'] = meta_by_fov_cells['Cells'].astype(str)\n",
        "\n",
        "unknown = sorted(set(meta_by_fov_cells['Cells'].unique()) - set(DIAMETER_MAP_UM.keys()))\n",
        "if unknown:\n",
        "    raise ValueError(f\"Please add diameters for these Cells labels in DIAMETER_MAP_UM: {unknown}\")\n",
        "\n",
        "meta_by_fov_cells['diam_um'] = meta_by_fov_cells['Cells'].map(DIAMETER_MAP_UM).astype(float)\n",
        "meta_by_fov_cells['r_fov_um'] = K_RADIUS * meta_by_fov_cells['diam_um']\n",
        "r_map = dict(zip(meta_by_fov_cells['File_name'], meta_by_fov_cells['r_fov_um']))\n",
        "\n",
        "# ---- Load events, compute flags, summarize ----\n",
        "events_df = load_slowdown_events(EVENTS_DIR).copy()\n",
        "required = {'File_name','t0','x0','y0'}\n",
        "missing = required - set(events_df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in slowdown events: {missing}\")\n",
        "\n",
        "per_fov_rows, flag_series = [], []\n",
        "for fov, g in events_df.groupby('File_name'):\n",
        "    r_fov = float(r_map[fov])\n",
        "    flags = co_arrest_flags_for_fov(g, r_fov, DELTA_T_UNITS)\n",
        "    flag_series.append(pd.Series(flags, index=g.index))\n",
        "\n",
        "    total = len(g)\n",
        "    together = int(flags.sum())\n",
        "    alone = total - together\n",
        "    frac_together = (together / total) if total else np.nan\n",
        "\n",
        "    per_fov_rows.append({\n",
        "        \"File_name\": fov,\n",
        "        \"Cells\": str(meta_by_fov_cells.loc[meta_by_fov_cells['File_name']==fov, 'Cells'].iloc[0]),\n",
        "        \"diam_um\": float(meta_by_fov_cells.loc[meta_by_fov_cells['File_name']==fov, 'diam_um'].iloc[0]),\n",
        "        \"r_fov_um\": r_fov,\n",
        "        \"k_radius\": K_RADIUS,\n",
        "        \"delta_t_units\": DELTA_T_UNITS,\n",
        "        \"total_events\": total,\n",
        "        \"together_events\": together,\n",
        "        \"alone_events\": alone,\n",
        "        \"frac_together\": frac_together,\n",
        "        \"frac_alone\": 1.0 - frac_together if total else np.nan,\n",
        "        \"pct_together\": 100.0 * frac_together if total else np.nan,\n",
        "        \"pct_alone\": 100.0 * (1.0 - frac_together) if total else np.nan,\n",
        "    })\n",
        "\n",
        "events_df[\"r_fov_um\"] = events_df[\"File_name\"].map(r_map)\n",
        "events_df[\"k_radius\"] = K_RADIUS\n",
        "events_df[\"delta_t_units\"] = DELTA_T_UNITS\n",
        "\n",
        "TAG = f\"rk{K_RADIUS}_dt{DELTA_T_UNITS}\"\n",
        "summary_df = pd.DataFrame(per_fov_rows).sort_values(\"File_name\")\n",
        "\n",
        "summary_path = os.path.join(OUT_DIR_CO, f\"co_arrest_summary_by_FOV_{TAG}.csv\")\n",
        "events_path  = os.path.join(OUT_DIR_CO, f\"slowdown_events_with_coarrestflags_{TAG}.csv\")\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "events_df.to_csv(events_path, index=False)\n",
        "\n",
        "# ---- Overall headline (weighted by events) ----\n",
        "overall_total = int(summary_df[\"total_events\"].sum())\n",
        "overall_together = int(summary_df[\"together_events\"].sum())\n",
        "overall_alone = int(summary_df[\"alone_events\"].sum())\n",
        "overall_frac_together = (overall_together / overall_total) if overall_total else np.nan\n",
        "overall_frac_alone    = (overall_alone / overall_total) if overall_total else np.nan\n",
        "\n",
        "overall_path = os.path.join(OUT_DIR_CO, f\"overall_co_arrest_totals_{TAG}.csv\")\n",
        "pd.DataFrame([{\n",
        "    \"overall_total_events\": overall_total,\n",
        "    \"overall_together_events\": overall_together,\n",
        "    \"overall_alone_events\": overall_alone,\n",
        "    \"overall_frac_together\": overall_frac_together,\n",
        "    \"overall_frac_alone\": overall_frac_alone,\n",
        "    \"overall_pct_together\": 100.0 * overall_frac_together if overall_total else np.nan,\n",
        "    \"overall_pct_alone\": 100.0 * overall_frac_alone if overall_total else np.nan,\n",
        "    \"k_radius\": K_RADIUS,\n",
        "    \"delta_t_units\": DELTA_T_UNITS,\n",
        "}]).to_csv(overall_path, index=False)\n",
        "\n",
        "# ---- Quick overall bar chart (optional) ----\n",
        "fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "ax.bar([\"Alone\",\"Together\"], [overall_frac_alone, overall_frac_together])\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_ylabel(\"Fraction of arrests\")\n",
        "ax.set_title(f\"Co-arrest fraction (r = k×diameter; k={K_RADIUS}, ΔT={DELTA_T_UNITS})\")\n",
        "fig.tight_layout()\n",
        "bar_path = os.path.join(OUT_DIR_CO, f\"overall_co_arrest_fraction_{TAG}.png\")\n",
        "fig.savefig(bar_path, dpi=200)\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"Saved:\", summary_path)\n",
        "print(\"Saved:\", events_path)\n",
        "print(\"Saved:\", overall_path)\n",
        "print(\"Saved:\", bar_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vxZdBMR2b6E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## QC: Sensitivity across k (radius in cell diameters) and ΔT\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Locate a per-event table with t0/x0/y0 (prefer the one from Cell 3) ---\n",
        "try:\n",
        "    base_events = pd.read_csv(events_path)[['File_name','Unique_ID','t0','x0','y0']]\n",
        "    print(f\"[QC] Loaded events from events_path: {events_path}\")\n",
        "except Exception:\n",
        "    # Fallback 1: most recent flagged events in CoArrest_TimeWindow\n",
        "    co_dir = os.path.join(Results_Folder, \"CoArrest_TimeWindow\")\n",
        "    cand = [fn for fn in os.listdir(co_dir)\n",
        "            if fn.startswith(\"slowdown_events_with_coarrestflags_\") and fn.endswith(\".csv\")]\n",
        "    if cand:\n",
        "        cand.sort()\n",
        "        fp = os.path.join(co_dir, cand[-1])\n",
        "        base_events = pd.read_csv(fp)[['File_name','Unique_ID','t0','x0','y0']]\n",
        "        print(f\"[QC] Loaded events from fallback: {fp}\")\n",
        "    else:\n",
        "        # Fallback 2: raw slowdown events\n",
        "        fp = os.path.join(Results_Folder, \"Slowdown_TimeColored\", \"slowdown_events_all.csv\")\n",
        "        base_events = pd.read_csv(fp)[['File_name','Unique_ID','t0','x0','y0']]\n",
        "        print(f\"[QC] Loaded events from raw slowdown file: {fp}\")\n",
        "\n",
        "# --- Build a per-FOV diameter map (µm) ---\n",
        "# Prefer the table made earlier; otherwise rebuild from merged_tracks_df + DIAMETER_MAP_UM\n",
        "if 'meta_by_fov_cells' in globals() and 'diam_um' in meta_by_fov_cells.columns:\n",
        "    diam_map = dict(zip(meta_by_fov_cells['File_name'], meta_by_fov_cells['diam_um']))\n",
        "else:\n",
        "    if 'DIAMETER_MAP_UM' not in globals():\n",
        "        raise RuntimeError(\"DIAMETER_MAP_UM not defined. Define it as {Cells_label: diameter_um}.\")\n",
        "    meta_by_fov_cells = (\n",
        "        merged_tracks_df\n",
        "        .groupby('File_name', as_index=False)[['Cells']]\n",
        "        .agg(lambda s: s.iloc[0])\n",
        "    )\n",
        "    meta_by_fov_cells['Cells'] = meta_by_fov_cells['Cells'].astype(str)\n",
        "    unknown = sorted(set(meta_by_fov_cells['Cells'].unique()) - set(DIAMETER_MAP_UM.keys()))\n",
        "    if unknown:\n",
        "        raise ValueError(f\"Please add diameters for these Cells labels in DIAMETER_MAP_UM: {unknown}\")\n",
        "    meta_by_fov_cells['diam_um'] = meta_by_fov_cells['Cells'].map(DIAMETER_MAP_UM).astype(float)\n",
        "    diam_map = dict(zip(meta_by_fov_cells['File_name'], meta_by_fov_cells['diam_um']))\n",
        "\n",
        "# --- Helper: fraction 'together' for a single FOV given k & ΔT ---\n",
        "def frac_together_k(ev_fov: pd.DataFrame, k: float, dt: float, diam_um: float) -> float:\n",
        "    n = len(ev_fov)\n",
        "    if n <= 1 or not np.isfinite(diam_um):\n",
        "        return np.nan\n",
        "    r = k * float(diam_um)  # radius in µm (positions already calibrated)\n",
        "    X = ev_fov[['x0','y0']].to_numpy(float)\n",
        "    T = ev_fov['t0'].to_numpy(float)\n",
        "    D = cdist(X, X)\n",
        "    dT = np.abs(T[:, None] - T[None, :])\n",
        "    A = (D <= r) & (dT <= dt)\n",
        "    np.fill_diagonal(A, False)\n",
        "    return float(A.any(axis=1).mean())\n",
        "\n",
        "# --- Grid centered on your current K_RADIUS from Cell 3 ---\n",
        "if 'K_RADIUS' not in globals():\n",
        "    K_RADIUS = 1.0  # fallback\n",
        "grid_k  = [0.75 * K_RADIUS, K_RADIUS, 1.5 * K_RADIUS]\n",
        "grid_dt = [0.5 * DELTA_T_UNITS, DELTA_T_UNITS, 2.0 * DELTA_T_UNITS]\n",
        "\n",
        "rows = []\n",
        "for k in grid_k:\n",
        "    for dt in grid_dt:\n",
        "        vals = []\n",
        "        for fov, g in base_events.groupby('File_name'):\n",
        "            diam = diam_map.get(fov, np.nan)\n",
        "            vals.append(frac_together_k(g, k, dt, diam))\n",
        "        rows.append({'k': k, 'dt': dt, 'mean_frac_together': np.nanmean(vals)})\n",
        "\n",
        "sens_df = pd.DataFrame(rows)\n",
        "pivot = sens_df.pivot(index='k', columns='dt', values='mean_frac_together').round(3)\n",
        "print(pivot)\n",
        "\n",
        "# --- Save results (CSV + small heatmap PDF) ---\n",
        "qc_csv = os.path.join(OUT_DIR_CO, \"qc_sensitivity_k_dt.csv\")\n",
        "sens_df.to_csv(qc_csv, index=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "im = ax.imshow(pivot.values, aspect='auto', origin='lower')\n",
        "ax.set_xticks(range(len(pivot.columns)))\n",
        "ax.set_yticks(range(len(pivot.index)))\n",
        "ax.set_xticklabels([f\"{v:.3g}\" for v in pivot.columns], rotation=45, ha='right')\n",
        "ax.set_yticklabels([f\"{v:.3g}\" for v in pivot.index])\n",
        "ax.set_xlabel(\"ΔT (same units as POSITION_T)\")\n",
        "ax.set_ylabel(\"k (× cell diameter)\")\n",
        "ax.set_title(\"Sensitivity of mean co-arrest fraction\")\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label(\"Mean frac_together\")\n",
        "fig.tight_layout()\n",
        "qc_pdf = os.path.join(OUT_DIR_CO, \"qc_sensitivity_k_dt_heatmap.pdf\")\n",
        "fig.savefig(qc_pdf)\n",
        "plt.close(fig)\n",
        "\n",
        "print(\"Saved sensitivity table to:\", qc_csv)\n",
        "print(\"Saved sensitivity heatmap to:\", qc_pdf)\n"
      ],
      "metadata": {
        "id": "LdD3tmBwW8E4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Plot Co-arrest fraction by Cells / Condition / Treatment / Cells_Treatment (FOV-level)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---- INPUTS ----\n",
        "META_COLS = ['Cells', 'Treatment', 'Condition', 'Flow_speed', 'experiment_nb', 'Repeat']\n",
        "\n",
        "COARREST_DIR = os.path.join(Results_Folder, \"CoArrest_TimeWindow\")\n",
        "cand = [fn for fn in os.listdir(COARREST_DIR) if fn.startswith(\"co_arrest_summary_by_FOV_\") and fn.endswith(\".csv\")]\n",
        "if not cand:\n",
        "    raise RuntimeError(\"No co_arrest_summary_by_FOV_*.csv found. Run the co-arrest cell first.\")\n",
        "cand.sort()\n",
        "summary_path = os.path.join(COARREST_DIR, cand[-1])\n",
        "summary_df = pd.read_csv(summary_path)\n",
        "\n",
        "# Try to fetch k and dt from the summary (for plot titles); fallback to globals if present\n",
        "k_in = summary_df['k_radius'].iloc[0] if 'k_radius' in summary_df.columns else (K_RADIUS if 'K_RADIUS' in globals() else None)\n",
        "dt_in = summary_df['delta_t_units'].iloc[0] if 'delta_t_units' in summary_df.columns else (DELTA_T_UNITS if 'DELTA_T_UNITS' in globals() else None)\n",
        "\n",
        "def _title_suffix():\n",
        "    bits = []\n",
        "    if k_in is not None: bits.append(f\"k={k_in:g}\")\n",
        "    if dt_in is not None: bits.append(f\"ΔT={dt_in}\")\n",
        "    return f\" ({', '.join(bits)})\" if bits else \"\"\n",
        "\n",
        "# ---- OUTPUT FOLDER ----\n",
        "OUT_DIR = os.path.join(Results_Folder, \"CoArrest_TimeWindow_ByCondition\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Merge metadata (per-FOV) ----\n",
        "meta_by_fov = (\n",
        "    merged_tracks_df\n",
        "    .groupby('File_name', as_index=False)[META_COLS]\n",
        "    .agg(lambda s: s.iloc[0])\n",
        ")\n",
        "\n",
        "coarrest_with_meta = summary_df.merge(meta_by_fov, on='File_name', how='left')\n",
        "\n",
        "# --- Helper to coalesce duplicate columns produced by merge (e.g., Cells_x / Cells_y -> Cells)\n",
        "def _coalesce_suffix(df, col):\n",
        "    if col in df.columns:\n",
        "        return df\n",
        "    x, y = f\"{col}_x\", f\"{col}_y\"\n",
        "    if x in df.columns or y in df.columns:\n",
        "        left = df[x] if x in df.columns else pd.Series(index=df.index, dtype=object)\n",
        "        right = df[y] if y in df.columns else pd.Series(index=df.index, dtype=object)\n",
        "        df[col] = left.where(~left.isna(), right)\n",
        "        for c in (x, y):\n",
        "            if c in df.columns:\n",
        "                df.drop(columns=c, inplace=True)\n",
        "    return df\n",
        "\n",
        "for col in META_COLS:\n",
        "    coarrest_with_meta = _coalesce_suffix(coarrest_with_meta, col)\n",
        "\n",
        "# Sanity checks\n",
        "if coarrest_with_meta.empty:\n",
        "    raise RuntimeError(\"Merged table is empty. Check that 'File_name' matches between summary and metadata.\")\n",
        "for col in ['Cells','Treatment','Condition']:\n",
        "    if col not in coarrest_with_meta.columns:\n",
        "        raise RuntimeError(f\"Column '{col}' not found after merge. \"\n",
        "                           f\"Available columns: {list(coarrest_with_meta.columns)}\")\n",
        "\n",
        "# Cast to string categories and make the composite Cells_Treatment\n",
        "for col in ['Cells', 'Treatment', 'Condition']:\n",
        "    coarrest_with_meta[col] = coarrest_with_meta[col].astype(str)\n",
        "coarrest_with_meta['Cells_Treatment'] = coarrest_with_meta['Cells'] + \"_\" + coarrest_with_meta['Treatment']\n",
        "\n",
        "# Save merged table (optional; helpful to inspect once)\n",
        "per_fov_out = os.path.join(OUT_DIR, \"co_arrest_by_FOV_with_metadata.csv\")\n",
        "coarrest_with_meta.to_csv(per_fov_out, index=False)\n",
        "print(\"Saved:\", per_fov_out)\n",
        "\n",
        "# ---- Helper to plot and save a boxplot+strip for a grouping column ----\n",
        "def plot_box(group_col: str, label: str, fname_stub: str):\n",
        "    plt.figure(figsize=(max(10, min(18, 1.2*coarrest_with_meta[group_col].nunique())), 6))\n",
        "    sns.boxplot(data=coarrest_with_meta, x=group_col, y='frac_together')\n",
        "    sns.stripplot(data=coarrest_with_meta, x=group_col, y='frac_together',\n",
        "                  color='black', alpha=0.5, jitter=True)\n",
        "    plt.ylabel('Co-arrest fraction (per FOV)')\n",
        "    plt.title(f'Co-arrest fraction by {label} (FOV-level){_title_suffix()}')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    out_pdf = os.path.join(OUT_DIR, f\"boxplot_frac_together_by_{fname_stub}.pdf\")\n",
        "    plt.savefig(out_pdf)\n",
        "    plt.close()\n",
        "    print(\"Saved:\", out_pdf)\n",
        "\n",
        "# ---- Make all four plots ----\n",
        "plot_box('Cells', 'Cells', 'Cells')\n",
        "plot_box('Condition', 'Condition', 'Condition')\n",
        "plot_box('Treatment', 'Treatment', 'Treatment')\n",
        "plot_box('Cells_Treatment', 'Cells_Treatment', 'Cells_Treatment')\n"
      ],
      "metadata": {
        "id": "pVG5r-dOnOUm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4 Identification of the number of cells in each co-arrested cluster**\n"
      ],
      "metadata": {
        "id": "OaFVNIF6onVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Co-arrest group size per event (space–time components; r = k × diameter)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Reuse: OUT_DIR_CO (CoArrest_TimeWindow), DELTA_T_UNITS, K_RADIUS, meta_by_fov_cells (with diam_um, r_fov_um)\n",
        "# from the previous cell. If not present, rebuild meta_by_fov_cells as in Patch A.\n",
        "\n",
        "# --- Helper: Union-Find for connected components ---\n",
        "class UnionFind:\n",
        "    def __init__(self, n):\n",
        "        self.parent = np.arange(n)\n",
        "        self.rank = np.zeros(n, dtype=int)\n",
        "    def find(self, x):\n",
        "        while self.parent[x] != x:\n",
        "            self.parent[x] = self.parent[self.parent[x]]\n",
        "            x = self.parent[x]\n",
        "        return x\n",
        "    def union(self, a, b):\n",
        "        ra, rb = self.find(a), self.find(b)\n",
        "        if ra == rb: return\n",
        "        if self.rank[ra] < self.rank[rb]:\n",
        "            self.parent[ra] = rb\n",
        "        elif self.rank[rb] < self.rank[ra]:\n",
        "            self.parent[rb] = ra\n",
        "        else:\n",
        "            self.parent[rb] = ra\n",
        "            self.rank[ra] += 1\n",
        "\n",
        "def label_group_sizes_for_fov(ev_fov: pd.DataFrame, r: float, dt: float) -> pd.DataFrame:\n",
        "    ev = ev_fov.reset_index(drop=True).copy()\n",
        "    n = len(ev)\n",
        "    if n == 0:\n",
        "        return ev.assign(\n",
        "            co_arrest_neighbors=pd.Series(dtype=int),\n",
        "            co_arrest_group_id=pd.Series(dtype=int),\n",
        "            co_arrest_group_size=pd.Series(dtype=int)\n",
        "        )\n",
        "    X = ev[['x0','y0']].to_numpy(float)\n",
        "    T = ev['t0'].to_numpy(float)\n",
        "    D = cdist(X, X)\n",
        "    dT = np.abs(T[:, None] - T[None, :])\n",
        "    A = (D <= r) & (dT <= dt)\n",
        "    np.fill_diagonal(A, False)\n",
        "\n",
        "    neighbors = A.sum(axis=1).astype(int)\n",
        "\n",
        "    uf = UnionFind(n)\n",
        "    ii, jj = np.where(A)\n",
        "    for a, b in zip(ii, jj):\n",
        "        uf.union(a, b)\n",
        "    roots = np.array([uf.find(i) for i in range(n)])\n",
        "    _, comp_ids = np.unique(roots, return_inverse=True)\n",
        "    comp_sizes = pd.Series(comp_ids).value_counts().to_dict()\n",
        "    group_size = np.array([comp_sizes[c] for c in comp_ids], dtype=int)\n",
        "\n",
        "    ev['co_arrest_neighbors'] = neighbors\n",
        "    ev['co_arrest_group_id'] = comp_ids\n",
        "    ev['co_arrest_group_size'] = group_size\n",
        "    ev['co_arrest_together'] = ev['co_arrest_group_size'] >= 2\n",
        "    return ev\n",
        "\n",
        "# ---- Load the slowdown events ----\n",
        "EVENTS_DIR = os.path.join(Results_Folder, \"Slowdown_TimeColored\")\n",
        "all_events_path = os.path.join(EVENTS_DIR, \"slowdown_events_all.csv\")\n",
        "if not os.path.exists(all_events_path):\n",
        "    raise RuntimeError(\"No slowdown events found. Run the slowdown export first.\")\n",
        "events_df = pd.read_csv(all_events_path)\n",
        "\n",
        "required = {'File_name','t0','x0','y0'}\n",
        "missing = required - set(events_df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in events table: {missing}\")\n",
        "\n",
        "# ---- Label per-event group sizes for all FOVs using per-FOV r_fov ----\n",
        "r_map = dict(zip(meta_by_fov_cells['File_name'], meta_by_fov_cells['r_fov_um']))\n",
        "labeled = []\n",
        "for fov, g in events_df.groupby('File_name'):\n",
        "    r_fov = float(r_map[fov])\n",
        "    lab = label_group_sizes_for_fov(g, r_fov, DELTA_T_UNITS)\n",
        "    lab[\"r_fov_um\"] = r_fov\n",
        "    lab[\"k_radius\"] = K_RADIUS\n",
        "    lab[\"delta_t_units\"] = DELTA_T_UNITS\n",
        "    labeled.append(lab)\n",
        "\n",
        "labeled_events = pd.concat(labeled, ignore_index=True)\n",
        "\n",
        "# ---- Save annotated per-event table & per-FOV summary ----\n",
        "TAG = f\"rk{K_RADIUS}_dt{DELTA_T_UNITS}\"\n",
        "events_with_groups_path = os.path.join(OUT_DIR_CO, f\"slowdown_events_with_groups_{TAG}.csv\")\n",
        "labeled_events.to_csv(events_with_groups_path, index=False)\n",
        "\n",
        "per_fov_rows = []\n",
        "for fov, g in labeled_events.groupby('File_name'):\n",
        "    total = len(g)\n",
        "    together_events = int((g['co_arrest_group_size'] >= 2).sum())\n",
        "    alone_events = total - together_events\n",
        "    size_counts = g['co_arrest_group_size'].value_counts().sort_index()\n",
        "\n",
        "    row = {\n",
        "        \"File_name\": fov,\n",
        "        \"r_fov_um\": float(r_map[fov]),\n",
        "        \"k_radius\": K_RADIUS,\n",
        "        \"delta_t_units\": DELTA_T_UNITS,\n",
        "        \"total_events\": total,\n",
        "        \"together_events\": together_events,\n",
        "        \"alone_events\": alone_events,\n",
        "        \"frac_together\": together_events / total if total else np.nan,\n",
        "        \"mean_group_size\": float(g['co_arrest_group_size'].mean()) if total else np.nan,\n",
        "        \"median_group_size\": float(g['co_arrest_group_size'].median()) if total else np.nan,\n",
        "        \"max_group_size\": int(g['co_arrest_group_size'].max()) if total else np.nan,\n",
        "        \"mean_direct_neighbors\": float(g['co_arrest_neighbors'].mean()) if total else np.nan,\n",
        "        \"median_direct_neighbors\": float(g['co_arrest_neighbors'].median()) if total else np.nan,\n",
        "    }\n",
        "    for k, v in size_counts.items():\n",
        "        row[f\"count_size_{int(k)}\"] = int(v)\n",
        "        row[f\"frac_size_{int(k)}\"] = float(v) / total if total else np.nan\n",
        "    per_fov_rows.append(row)\n",
        "\n",
        "summary_groups_df = pd.DataFrame(per_fov_rows).sort_values(\"File_name\")\n",
        "summary_groups_path = os.path.join(OUT_DIR_CO, f\"co_arrest_group_summary_by_FOV_{TAG}.csv\")\n",
        "summary_groups_df.to_csv(summary_groups_path, index=False)\n",
        "\n",
        "print(\"Saved per-event groups to:\", events_with_groups_path)\n",
        "print(\"Saved per-FOV group summary to:\", summary_groups_path)\n"
      ],
      "metadata": {
        "id": "0YSZK7FEz-lX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## QC: Co-arrest flag vs component size agreement\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# -------- Resolve file paths (prefer variables from earlier cells) --------\n",
        "try:\n",
        "    flags_path = events_path  # from Co-arrest fraction cell\n",
        "except NameError:\n",
        "    co_dir = os.path.join(Results_Folder, \"CoArrest_TimeWindow\")\n",
        "    cand = [fn for fn in os.listdir(co_dir)\n",
        "            if fn.startswith(\"slowdown_events_with_coarrestflags_\") and fn.endswith(\".csv\")]\n",
        "    if not cand:\n",
        "        raise RuntimeError(\"Couldn't find slowdown_events_with_coarrestflags_*.csv in CoArrest_TimeWindow.\")\n",
        "    cand.sort()\n",
        "    flags_path = os.path.join(co_dir, cand[-1])\n",
        "\n",
        "try:\n",
        "    groups_path = events_with_groups_path  # from group-size cell\n",
        "except NameError:\n",
        "    co_dir = os.path.join(Results_Folder, \"CoArrest_TimeWindow\")\n",
        "    cand = [fn for fn in os.listdir(co_dir)\n",
        "            if fn.startswith(\"slowdown_events_with_groups_\") and fn.endswith(\".csv\")]\n",
        "    if not cand:\n",
        "        raise RuntimeError(\"Couldn't find slowdown_events_with_groups_*.csv in CoArrest_TimeWindow.\")\n",
        "    cand.sort()\n",
        "    groups_path = os.path.join(co_dir, cand[-1])\n",
        "\n",
        "print(\"[QC] flags:\", flags_path)\n",
        "print(\"[QC] groups:\", groups_path)\n",
        "\n",
        "ev_flags  = pd.read_csv(flags_path)\n",
        "ev_groups = pd.read_csv(groups_path)\n",
        "\n",
        "# -------- Merge keys sanity --------\n",
        "keys = ['File_name','Unique_ID','t0','x0','y0']\n",
        "missing_flags  = [k for k in keys if k not in ev_flags.columns]\n",
        "missing_groups = [k for k in keys if k not in ev_groups.columns]\n",
        "if missing_flags or missing_groups:\n",
        "    raise ValueError(f\"Missing merge keys. flags missing {missing_flags}; groups missing {missing_groups}\")\n",
        "\n",
        "# -------- If 'co_arrest_together' is missing, recompute it per FOV --------\n",
        "def _co_arrest_flags_for_fov(ev_fov: pd.DataFrame, r: float, dt: float) -> np.ndarray:\n",
        "    n = len(ev_fov)\n",
        "    if n <= 1:\n",
        "        return np.zeros(n, dtype=bool)\n",
        "    X = ev_fov[['x0','y0']].to_numpy(float)\n",
        "    T = ev_fov['t0'].to_numpy(float)\n",
        "    D = cdist(X, X)\n",
        "    dT = np.abs(T[:, None] - T[None, :])\n",
        "    A = (D <= r) & (dT <= dt)\n",
        "    np.fill_diagonal(A, False)\n",
        "    return A.any(axis=1)\n",
        "\n",
        "if 'co_arrest_together' not in ev_flags.columns:\n",
        "    print(\"⚠️ 'co_arrest_together' not found in flags file — recomputing from positions and times.\")\n",
        "\n",
        "    # ΔT: prefer from file, then globals, else fallback\n",
        "    if 'delta_t_units' in ev_flags.columns and pd.notna(ev_flags['delta_t_units'].iloc[0]):\n",
        "        dt_in = float(ev_flags['delta_t_units'].iloc[0])\n",
        "    elif 'DELTA_T_UNITS' in globals():\n",
        "        dt_in = float(DELTA_T_UNITS)\n",
        "    else:\n",
        "        dt_in = 0.04\n",
        "        print(f\"   Using fallback ΔT={dt_in}\")\n",
        "\n",
        "    # r per FOV:\n",
        "    # 1) If r_fov_um is in the file, use it\n",
        "    if 'r_fov_um' in ev_flags.columns:\n",
        "        r_map = ev_flags.groupby('File_name')['r_fov_um'].first().to_dict()\n",
        "    # 2) Else if meta_by_fov_cells exists (built earlier), use it\n",
        "    elif 'meta_by_fov_cells' in globals() and 'r_fov_um' in meta_by_fov_cells.columns:\n",
        "        r_map = dict(zip(meta_by_fov_cells['File_name'], meta_by_fov_cells['r_fov_um']))\n",
        "    # 3) Else recompute from DIAMETER_MAP_UM & K_RADIUS\n",
        "    else:\n",
        "        if ('DIAMETER_MAP_UM' not in globals()) or ('K_RADIUS' not in globals()):\n",
        "            raise RuntimeError(\"Cannot rebuild radius: need r_fov_um in file OR meta_by_fov_cells \"\n",
        "                               \"OR (DIAMETER_MAP_UM and K_RADIUS) defined.\")\n",
        "        if 'merged_tracks_df' not in globals():\n",
        "            raise RuntimeError(\"Need merged_tracks_df to map Cells→diameter per FOV.\")\n",
        "        meta_tmp = (\n",
        "            merged_tracks_df.groupby('File_name', as_index=False)[['Cells']]\n",
        "            .agg(lambda s: s.iloc[0])\n",
        "        )\n",
        "        meta_tmp['Cells'] = meta_tmp['Cells'].astype(str)\n",
        "        unknown = sorted(set(meta_tmp['Cells'].unique()) - set(DIAMETER_MAP_UM.keys()))\n",
        "        if unknown:\n",
        "            raise ValueError(f\"Add diameters for these Cells labels in DIAMETER_MAP_UM: {unknown}\")\n",
        "        meta_tmp['diam_um'] = meta_tmp['Cells'].map(DIAMETER_MAP_UM).astype(float)\n",
        "        meta_tmp['r_fov_um'] = float(K_RADIUS) * meta_tmp['diam_um']\n",
        "        r_map = dict(zip(meta_tmp['File_name'], meta_tmp['r_fov_um']))\n",
        "\n",
        "    # Compute flags per FOV and insert column\n",
        "    flags_series = []\n",
        "    for fov, g in ev_flags.groupby('File_name'):\n",
        "        r_fov = float(r_map[fov])\n",
        "        flags = _co_arrest_flags_for_fov(g, r_fov, dt_in)\n",
        "        flags_series.append(pd.Series(flags, index=g.index))\n",
        "    ev_flags['co_arrest_together'] = pd.concat(flags_series).reindex(ev_flags.index).astype(bool)\n",
        "\n",
        "# -------- Merge with groups and check agreement --------\n",
        "merged = ev_flags.merge(\n",
        "    ev_groups[keys + ['co_arrest_group_size']],\n",
        "    on=keys,\n",
        "    how='inner',\n",
        "    validate='one_to_one'\n",
        ")\n",
        "\n",
        "merged['together_by_group'] = merged['co_arrest_group_size'] >= 2\n",
        "mismatch = (merged['co_arrest_together'] != merged['together_by_group'])\n",
        "\n",
        "n = len(merged)\n",
        "n_mismatch = int(mismatch.sum())\n",
        "print(f\"[QC] Compared {n} events; mismatches: {n_mismatch}\")\n",
        "\n",
        "if n_mismatch:\n",
        "    try:\n",
        "        display(merged.loc[mismatch].head(10))\n",
        "    except NameError:\n",
        "        print(merged.loc[mismatch].head(10))\n",
        "else:\n",
        "    print(\"✅ Co-arrest classification matches component size (size ≥ 2) for all events.\")\n",
        "\n",
        "# Optional: per-FOV localization\n",
        "per_fov_ok = (\n",
        "    merged.assign(eq=(merged['co_arrest_together'] == merged['together_by_group']))\n",
        "          .groupby('File_name')['eq'].all()\n",
        ")\n",
        "if not per_fov_ok.all():\n",
        "    bad = per_fov_ok[~per_fov_ok].index.tolist()\n",
        "    print(f\"⚠️ FOVs with mismatches: {bad}\")\n"
      ],
      "metadata": {
        "id": "i235lDafViDD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Event-level co-arrest group size by Cells / Condition / Treatment / Cells_Treatment (clusters only)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Load per-event groups (prefer in-memory; else from saved paths; else latest on disk) ----\n",
        "if 'labeled_events' in globals():\n",
        "    ev = labeled_events.copy()\n",
        "else:\n",
        "    try:\n",
        "        # from Cell 5 if available\n",
        "        ev = pd.read_csv(events_with_groups_path)\n",
        "        print(f\"[Event-level] Loaded from events_with_groups_path: {events_with_groups_path}\")\n",
        "    except Exception:\n",
        "        co_dir = os.path.join(Results_Folder, \"CoArrest_TimeWindow\")\n",
        "        cand = [fn for fn in os.listdir(co_dir)\n",
        "                if fn.startswith(\"slowdown_events_with_groups_\") and fn.endswith(\".csv\")]\n",
        "        if not cand:\n",
        "            raise RuntimeError(\"No slowdown_events_with_groups_*.csv found. Run the group-size cell first.\")\n",
        "        cand.sort()\n",
        "        ev_path = os.path.join(co_dir, cand[-1])\n",
        "        ev = pd.read_csv(ev_path)\n",
        "        print(f\"[Event-level] Loaded latest: {ev_path}\")\n",
        "\n",
        "# ---- Attach metadata to events ----\n",
        "meta_cols = ['Cells','Condition','Treatment']\n",
        "meta_by_fov = (\n",
        "    merged_tracks_df\n",
        "    .groupby('File_name', as_index=False)[meta_cols]\n",
        "    .agg(lambda s: s.iloc[0])\n",
        ")\n",
        "for c in meta_cols:\n",
        "    meta_by_fov[c] = meta_by_fov[c].astype(str)\n",
        "\n",
        "evm = ev.merge(meta_by_fov, on='File_name', how='left')\n",
        "evm['Cells_Treatment'] = evm['Cells'] + \"_\" + evm['Treatment']\n",
        "\n",
        "# ---- Buckets: clusters only (size ≥2). If you want singles included, see note below. ----\n",
        "def bucket_size(k: int) -> str:\n",
        "    try:\n",
        "        k = int(k)\n",
        "    except:\n",
        "        return \"NA\"\n",
        "    return str(k) if k <= 5 else \"6+\"\n",
        "\n",
        "evm = evm[evm['co_arrest_group_size'] >= 2].copy()\n",
        "evm['size_bucket'] = evm['co_arrest_group_size'].apply(bucket_size)\n",
        "bucket_order = ['2','3','4','5','6+']\n",
        "\n",
        "# ---- Output folder ----\n",
        "OUT_DIR_EVT = os.path.join(Results_Folder, \"CoArrest_TimeWindow_EventLevel_GroupSize\")\n",
        "os.makedirs(OUT_DIR_EVT, exist_ok=True)\n",
        "\n",
        "# Pull k and ΔT for titles if present\n",
        "k_in  = (evm['k_radius'].iloc[0] if 'k_radius' in evm.columns else (K_RADIUS if 'K_RADIUS' in globals() else None))\n",
        "dt_in = (evm['delta_t_units'].iloc[0] if 'delta_t_units' in evm.columns else (DELTA_T_UNITS if 'DELTA_T_UNITS' in globals() else None))\n",
        "def _title_suffix():\n",
        "    bits = []\n",
        "    if k_in is not None:  bits.append(f\"k={k_in:g}\")\n",
        "    if dt_in is not None: bits.append(f\"ΔT={dt_in}\")\n",
        "    return f\" ({', '.join(bits)})\" if bits else \"\"\n",
        "\n",
        "def stacked_by(group_col: str, label: str):\n",
        "    # Counts & fractions per size bucket\n",
        "    tab = (\n",
        "        evm.groupby([group_col, 'size_bucket'])\n",
        "           .size()\n",
        "           .unstack('size_bucket', fill_value=0)\n",
        "           .reindex(columns=bucket_order, fill_value=0)\n",
        "           .sort_index()\n",
        "    )\n",
        "    frac = tab.div(tab.sum(axis=1).replace(0, np.nan), axis=0)\n",
        "\n",
        "    # Save tables\n",
        "    tab.to_csv(os.path.join(OUT_DIR_EVT, f\"event_counts_cluster_sizes_by_{label}.csv\"))\n",
        "    frac.to_csv(os.path.join(OUT_DIR_EVT, f\"event_fractions_cluster_sizes_by_{label}.csv\"))\n",
        "\n",
        "    # Stacked bar (PDF)\n",
        "    fig, ax = plt.subplots(figsize=(12,6))\n",
        "    bottom = np.zeros(len(frac))\n",
        "    x = np.arange(len(frac.index))\n",
        "    for b in bucket_order:\n",
        "        vals = frac[b].values if b in frac.columns else np.zeros(len(frac))\n",
        "        ax.bar(x, vals, bottom=bottom, label=b)\n",
        "        bottom += np.nan_to_num(vals)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(frac.index, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Fraction of events (clusters only)')\n",
        "    ax.set_xlabel(label)\n",
        "    ax.set_title(f'Event-level co-arrest cluster-size distribution by {label}{_title_suffix()}')\n",
        "    ax.legend(title='Cluster size', bbox_to_anchor=(1.02,1), loc='upper left')\n",
        "    fig.tight_layout()\n",
        "    out_pdf = os.path.join(OUT_DIR_EVT, f\"stacked_event_cluster_sizes_by_{label}.pdf\")\n",
        "    fig.savefig(out_pdf)\n",
        "    plt.close(fig)\n",
        "    print(\"Saved:\", out_pdf)\n",
        "\n",
        "# ---- Make all four sets ----\n",
        "stacked_by('Cells', 'Cells')\n",
        "stacked_by('Condition', 'Condition')\n",
        "stacked_by('Treatment', 'Treatment')\n",
        "stacked_by('Cells_Treatment', 'Cells_Treatment')\n"
      ],
      "metadata": {
        "id": "1iTCWvAt1J9_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}