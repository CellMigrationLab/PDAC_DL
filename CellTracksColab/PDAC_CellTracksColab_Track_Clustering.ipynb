{"cells":[{"cell_type":"markdown","metadata":{"id":"xF4zYMmXULP7"},"source":["# **PDAC CellTracksColab - Track Clustering**\n","---\n","\n","<font size = 4>This notebook is part of the **CellTracksColab** suite, specifically adapted to analyze tracking data highlighted in the manuscript titled \"Quantitative analysis of pancreatic cancer cell attachment to endothelial cells.\" The CellTracksColab project aims to provide comprehensive tools for cell tracking data analysis, facilitating the exploration and quantification of cellular behaviors.\n","\n","\n","<font size = 4>Access the CellTracksColab project resources through the GitHub repository: [CellMigrationLab/CellTracksColab](https://github.com/CellMigrationLab/CellTracksColab).\n","\n","<font size = 4>This notebook focuses on the assessment of spatial clustering of arrested circulating cells using a modified version of Ripley's L Function.\n","\n","- **Spatial Clustering Analysis:** Learn how to perform advanced spatial clustering analyses using this notebook. Detailed instructions and examples guide you through the process, tailored specifically for cell tracking data.\n","\n","- **Spatial Clustering Analyses:** For a comprehensive guide on performing spatial clustering analysis with CellTracksColab, visit the project's [Spatial Clustering analyses wiki page](https://github.com/CellMigrationLab/CellTracksColab/wiki/Spatial-Clustering-analyses).\n","\n","\n","**Notebook Creation:** This notebook was created by [Guillaume Jacquemet](https://cellmig.org/)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JrkfFr7mgZmA"},"outputs":[],"source":["# @title #MIT License\n","\n","print(\"\"\"\n","**MIT License**\n","\n","Copyright (c) 2023 Guillaume Jacquemet\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"Y4-Ft-yNRVCc"},"source":["--------------------------------------------------------\n","# **Part 1: Prepare the session and load your data**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9h0prdayn0qG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rAP0ahCzn1V6"},"outputs":[],"source":["#@markdown ##Play to install\n","!pip -q install pandas scikit-learn\n","!pip -q install hdbscan\n","!pip -q install umap-learn\n","!pip -q install plotly\n","!pip -q install tqdm\n","\n","!git clone https://github.com/CellMigrationLab/CellTracksColab.git\n","\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import numpy as np\n","import itertools\n","from matplotlib.gridspec import GridSpec\n","import requests\n","\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import sys\n","import matplotlib.colors as mcolors\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","import itertools\n","import requests\n","import ipywidgets as widgets\n","import warnings\n","import scipy.stats as stats\n","\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.gridspec import GridSpec\n","from ipywidgets import Dropdown, interact,Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","from tqdm.notebook import tqdm\n","from IPython.display import display, clear_output\n","from scipy.spatial import ConvexHull\n","from scipy.spatial.distance import cosine, pdist\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","from sklearn.metrics import pairwise_distances\n","from scipy.stats import zscore, ks_2samp\n","from sklearn.preprocessing import MinMaxScaler\n","from multiprocessing import Pool\n","from matplotlib.ticker import FixedLocator\n","from matplotlib.ticker import FuncFormatter\n","from matplotlib.colors import LogNorm\n","sys.path.append(\"../\")\n","sys.path.append(\"CellTracksColab/\")\n","\n","import celltracks\n","from celltracks import *\n","from celltracks.Track_Plots import *\n","from celltracks.BoxPlots_Statistics import *\n","from celltracks.Track_Metrics import *\n","\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=500000):\n","    \"\"\"Save a DataFrame with a progress bar and gzip compression.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing with gzip compression\n","        with gzip.open(path, \"wt\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))"]},{"cell_type":"markdown","metadata":{"id":"3Kzd_8GUnpbw"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GA1wCrkoV4i5"},"outputs":[],"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bsDAwkSOo1gV"},"source":["## **1.3. Compile your data or load existing dataframes**\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CQKXq3giI3nX"},"outputs":[],"source":["#@markdown ##Provide the path to the dataset:\n","\n","\n","import os\n","import re\n","import glob\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import requests\n","import zipfile\n","\n","#@markdown ###You have existing dataframes, provide the path to your:\n","\n","Track_table = ''  # @param {type: \"string\"}\n","Spot_table = ''  # @param {type: \"string\"}\n","\n","#@markdown ###Provide the path to your Result folder\n","\n","Results_Folder = \"\"  # @param {type: \"string\"}\n","\n","if not Results_Folder:\n","    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n","\n","if not os.path.exists(Results_Folder):\n","    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n","\n","# Print the location of the result folder\n","print(f\"Result folder is located at: {Results_Folder}\")\n","\n","def validate_tracks_df(df):\n","    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def validate_spots_df(df):\n","    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def check_unique_id_match(df1, df2):\n","    df1_ids = set(df1['Unique_ID'])\n","    df2_ids = set(df2['Unique_ID'])\n","\n","    # Check if the IDs in the two dataframes match\n","    if df1_ids == df2_ids:\n","        print(\"The Unique_ID values in both dataframes match perfectly!\")\n","    else:\n","        missing_in_df1 = df2_ids - df1_ids\n","        missing_in_df2 = df1_ids - df2_ids\n","\n","        if missing_in_df1:\n","            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n","\n","        if missing_in_df2:\n","            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n","\n","# For existing dataframes\n","if Track_table:\n","    print(\"Loading track table file....\")\n","    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n","    if not validate_tracks_df(merged_tracks_df):\n","        print(\"Error: Validation failed for loaded tracks dataframe.\")\n","\n","if Spot_table:\n","    print(\"Loading spot table file....\")\n","    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n","    if not validate_spots_df(merged_spots_df):\n","        print(\"Error: Validation failed for loaded spots dataframe.\")\n","\n","check_for_nans(merged_spots_df, \"merged_spots_df\")\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nT0t5jqsGRoG"},"outputs":[],"source":["#@markdown ##Check Metadata\n","\n","\n","# Define the metadata columns that are expected to have identical values for each filename\n","metadata_columns = ['Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']\n","\n","# Group the DataFrame by 'File_name' and then check if all entries within each group are identical\n","consistent_metadata = True\n","for name, group in merged_tracks_df.groupby('File_name'):\n","    for col in metadata_columns:\n","        if not group[col].nunique() == 1:\n","            consistent_metadata = False\n","            print(f\"Inconsistency found for file: {name} in column: {col}\")\n","            break  # Stop checking other columns for this group and move to the next file\n","    if not consistent_metadata:\n","        break  # Stop the entire process if any inconsistency is found\n","\n","if consistent_metadata:\n","    print(\"All files have consistent metadata across the specified columns.\")\n","else:\n","    print(\"There are inconsistencies in the metadata. Please check the output for details.\")\n","\n","# Drop duplicates based on the 'File_name' to get a unique list of filenames and their metadata\n","unique_files_df = merged_tracks_df.drop_duplicates(subset=['File_name'])[['File_name', 'Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']]\n","\n","# Reset the index to clean up the DataFrame\n","unique_files_df.reset_index(drop=True, inplace=True)\n","\n","# Display the resulting DataFrame in a nicely formatted HTML table\n","unique_files_df\n","\n","import pandas as pd\n","\n","# Assuming 'df' is your DataFrame and it already contains 'Conditions' and 'Repeats' columns.\n","\n","# Group by 'Conditions' and 'Repeats' and count the occurrences\n","grouped = unique_files_df.groupby(['Condition', 'Repeat']).size().reset_index(name='counts')\n","\n","# Check if any combinations have a count greater than 1, which means they are not unique\n","non_unique_combinations = grouped[grouped['counts'] > 1]\n","\n","# Print the non-unique combinations\n","if not non_unique_combinations.empty:\n","    print(\"There are non-unique combinations of Conditions and Repeats:\")\n","    print(non_unique_combinations)\n","else:\n","    print(\"All combinations of Conditions and Repeats are unique.\")\n","\n","check_unique_id_match(merged_spots_df, merged_tracks_df)\n","\n","\n","# Group the DataFrame by 'Cells', 'ILbeta', 'Repeat' and then check if there are 4 unique 'Flow_speed' values for each group\n","consistent_flow_speeds = True\n","for (cells, ilbeta, repeat), group in merged_tracks_df.groupby(['Cells', 'Treatment', 'Repeat']):\n","    if group['Flow_speed'].nunique() != 4:\n","        consistent_flow_speeds = False\n","        print(f\"Inconsistency found for Cells: {cells}, Treatment: {Treatment_conditions}, Repeat: {repeat} - Expected 4 Flow_speeds, found {group['Flow_speed'].nunique()}\")\n","        break  # Stop the entire process if any inconsistency is found\n","\n","if consistent_flow_speeds:\n","    print(\"Each combination of 'Cells', 'Treatment', 'Repeat' has exactly 4 different 'Flow_speed' values.\")\n","else:\n","    print(\"There are inconsistencies in 'Flow_speed' values. Please check the output for details.\")\n","\n","\n","unique_cells = unique_files_df['Cells'].unique()\n","unique_flow_speeds = unique_files_df['Flow_speed'].unique()\n","unique_Treatment = unique_files_df['Treatment'].unique()\n","unique_conditions = unique_files_df['Condition'].unique()\n","\n","print(\"Unique Cells:\", unique_cells)\n","print(\"Unique Flow Speeds:\", unique_flow_speeds)\n","print(\"Unique Treatment:\", unique_Treatment)\n","print(\"Unique Conditions:\", unique_conditions)\n"]},{"cell_type":"markdown","metadata":{"id":"t5YJ9V468HwJ"},"source":["## **1.4. Filter tracks shorter than 50 spots**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mA8GBhFy8vd6"},"outputs":[],"source":["# @title ##Filter tracks shorter than 50 spots\n","\n","\n","merged_tracks_df = merged_tracks_df[merged_tracks_df['NUMBER_SPOTS'] >= 50]\n","merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]\n"]},{"cell_type":"markdown","metadata":{"id":"52STmnv43d45"},"source":["## **1.5. Visualise your tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AE881uJW5ukQ"},"outputs":[],"source":["# @title ##Run the cell and choose the file you want to inspect\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"markdown","metadata":{"id":"XtIm-c77u5ha"},"source":["--------------------------------------------------------\n","# **Part 2: Assess spatial clustering using Ripley's L function**\n","--------------------------------------------------------\n","\n","<font size = 4>**Spatial Clustering Analyses:** For a comprehensive guide on performing spatial clustering analysis with CellTracksColab, visit the project's [Spatial Clustering analyses wiki page](https://github.com/CellMigrationLab/CellTracksColab/wiki/Spatial-Clustering-analyses).\n"]},{"cell_type":"markdown","metadata":{"id":"c6I305XNQL69"},"source":["## **2.1. Filter tracks with Track_MIN_speed**\n","\n","<font size = 4>This section enables to filter the dataset so that we only keep arresting tracks.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-mbDA3xfNRqH"},"outputs":[],"source":["# @title ##Filter tracks using Min Speed\n","\n","\n","merged_tracks_df = merged_tracks_df[merged_tracks_df['Min Speed'] <= 5]\n","merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]"]},{"cell_type":"markdown","metadata":{"id":"K_Py1TcJwqwB"},"source":["## **2.2. Visualise where cells slow down in each tracks**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JVheHy0OVlhG"},"outputs":[],"source":["# @title ##Run the cell and choose the file you want to inspect to visualise track and choosen point\n","\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","import os\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","# User-defined speed threshold\n","speed_threshold = 3  # Replace with the desired threshold value\n","\n","def find_point_below_threshold(track):\n","    below_threshold = track[track['Speed'] < speed_threshold]\n","    return below_threshold.iloc[0] if not below_threshold.empty else None\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","            # Find and mark the slowdown point\n","            slowdown_point = find_point_below_threshold(unique_df)\n","            if slowdown_point is not None:\n","                plt.scatter(slowdown_point['POSITION_X'], slowdown_point['POSITION_Y'], color='red', s=50)\n","            #else:\n","                #print(f\"No slowdown point found for track {unique_id}\")\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"markdown","metadata":{"id":"hiY-Iq5HcwA0"},"source":["## **2.3 Identify the coordinates where cells slow down**\n","\n","<font size = 4>Here we identify the where circulating cells slow down on the endothelial monolayer to identify possible hotspots.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"Fn-E8ZoIQG2m"},"outputs":[],"source":["# @title ##Run the cell to identify the coordinates to use for the clustering analysis\n","\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","if not os.path.exists(Results_Folder + \"/Slow_down_coordinates\"):\n","    os.makedirs(Results_Folder + \"/Slow_down_coordinates\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# User-defined speed threshold\n","speed_threshold = 5  # Replace with the desired threshold value\n","\n","def find_point_below_threshold(track):\n","    below_threshold = track[track['Speed'] < speed_threshold]\n","    return below_threshold.iloc[0] if not below_threshold.empty else None\n","\n","def plot_slowdown_points(filename):\n","    if filename:\n","        # Filter the DataFrame based on the filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","\n","            # Find and mark the slowdown point\n","            slowdown_point = find_point_below_threshold(unique_df)\n","            if slowdown_point is not None:\n","                plt.scatter(slowdown_point['POSITION_X'], slowdown_point['POSITION_Y'], color='red', s=50, label=f'Track {unique_id}')\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Slowdown Points for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Slow_down_coordinates/Slowdown_Points_{filename}.pdf\")\n","        plt.close()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Loop through each file and generate the plot for slowdown points\n","for filename in filenames:\n","    plot_slowdown_points(filename)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C3ldnL9HwA51"},"source":["## **2.4 Compute the Ripley's L function for each FOV**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"Y0Ikco9nMopc"},"outputs":[],"source":["# @title ##Compute Ripley's L function for each FOV\n","\n","# User-defined speed threshold\n","speed_threshold = 5\n","\n","# Check and create necessary directories\n","if not os.path.exists(f\"{Results_Folder}/Track_Clustering\"):\n","    os.makedirs(f\"{Results_Folder}/Track_Clustering\")\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.spatial import distance_matrix\n","import matplotlib.pyplot as plt\n","\n","# Define Ripley's K function\n","def ripley_k(points, r, area):\n","    n = len(points)\n","    d_matrix = distance_matrix(points, points)\n","    sum_indicator = np.sum(d_matrix < r) - n  # Subtract n to exclude self-pairs\n","\n","    K_r = (area / (n ** 2)) * sum_indicator\n","\n","    # Check if K_r is negative and print relevant information\n","    if K_r < 0:\n","        print(\"Negative K_r encountered!\")\n","        print(\"Distance matrix:\", d_matrix)\n","        print(\"Sum indicator:\", sum_indicator)\n","        print(\"Area:\", area, \"Number of points:\", n, \"Distance threshold r:\", r)\n","\n","    return K_r\n","\n","\n","# Define Ripley's L function\n","\n","def ripley_l(points, r, area):\n","    K_r = ripley_k(points, r, area)\n","    # Check if K_r has negative values\n","    if np.any(K_r < 0):\n","        print(\"Warning: Negative value encountered in K_r\")\n","\n","    L_r = np.sqrt(K_r / np.pi) - r\n","    return L_r\n","\n","def find_point_below_threshold(track):\n","  below_threshold = track[track['Speed'] < speed_threshold]\n","  if not below_threshold.empty:\n","    return below_threshold.iloc[0][['POSITION_X', 'POSITION_Y']]\n","  return pd.Series([np.nan, np.nan], index=['POSITION_X', 'POSITION_Y'])\n","\n","# Define area based on your dataset's extent\n","area = (merged_spots_df['POSITION_X'].max() - merged_spots_df['POSITION_X'].min()) * \\\n","       (merged_spots_df['POSITION_Y'].max() - merged_spots_df['POSITION_Y'].min())\n","\n","# Define r values\n","r_values = np.linspace(1, 250, 250)  # Adjust as needed\n","\n","# Compute Ripley's L function for each FOV\n","l_values_per_fov_slow = {}\n","for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc=\"Processing FOVs\"):\n","    # Sort each track by POSITION_T\n","    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n","\n","    representative_points = group.groupby('TRACK_ID').apply(find_point_below_threshold).dropna()\n","    if not representative_points.empty:\n","        l_values = [ripley_l(representative_points.values, r, area) for r in tqdm(r_values, desc=f\"Calculating L for {file_name}\")]\n","        l_values_per_fov_slow[file_name] = l_values\n"]},{"cell_type":"markdown","metadata":{"id":"PE8uWDxswKBC"},"source":["## **2.5 Compute Monte Carlo simulations for each FOV**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"INwTjl0JyM70"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","\n","# @title ##Compute Monte Carlo simulations for each FOV\n","\n","\n","# Simulate random points for Monte Carlo simulations\n","def simulate_random_points(num_points, x_range, y_range):\n","    x_coords = np.random.uniform(x_range[0], x_range[1], num_points)\n","    y_coords = np.random.uniform(y_range[0], y_range[1], num_points)\n","    return np.column_stack((x_coords, y_coords))\n","\n","# Initialize simulated_l_values as an empty dictionary\n","simulated_l_values_dict_slow = {}\n","\n","# Perform Monte Carlo simulations for significance testing\n","confidence_envelopes_slow = {}\n","for file_name, group in tqdm(merged_spots_df.groupby('File_name'), desc='Processing FOVs'):\n","\n","    group = group.sort_values(by=['TRACK_ID', 'POSITION_T'])\n","    representative_points = group.groupby('TRACK_ID').apply(find_point_below_threshold).dropna()\n","\n","    simulations = [simulate_random_points(len(representative_points),\n","                                          (merged_spots_df['POSITION_X'].min(), merged_spots_df['POSITION_X'].max()),\n","                                          (merged_spots_df['POSITION_Y'].min(), merged_spots_df['POSITION_Y'].max()))\n","                   for _ in tqdm(range(10), desc=f'Simulating for {file_name}', leave=False)]\n","\n","    simulated_l_values = [[ripley_l(points, r, area) for r in r_values] for points in simulations]\n","    simulated_l_values_dict_slow[file_name] = simulated_l_values  # Store the simulated values in the dictionary\n","\n","    lower_bound = np.percentile(simulated_l_values, 2.5, axis=0)\n","    upper_bound = np.percentile(simulated_l_values, 97.5, axis=0)\n","    confidence_envelopes_slow[file_name] = (lower_bound, upper_bound)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1EYrmDpzwXR2"},"source":["## **2.6 Plot the results for each FOV**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FYkImji3TPOg"},"outputs":[],"source":["# @title ##Plots for each FOV - Slow down\n","\n","import os\n","import matplotlib.pyplot as plt\n","\n","# Visualization of Ripley's L function with confidence envelopes\n","for file_name, l_values in l_values_per_fov_slow.items():\n","    # Retrieve the confidence envelope for the current file\n","    lower_bound, upper_bound = confidence_envelopes_slow.get(file_name, (None, None))\n","\n","    # Only proceed if the confidence envelope exists\n","    if lower_bound is not None and upper_bound is not None:\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(r_values, l_values, label=f'L(r) for {file_name}')\n","        plt.fill_between(r_values, lower_bound, upper_bound, color='gray', alpha=0.5)\n","        plt.xlabel('Radius (r)')\n","        plt.ylabel(\"Ripley's L Function\")\n","        plt.title(f\"Ripley's L Function - {file_name}\")\n","        plt.legend()\n","        plt.grid(True)\n","\n","        # Save the plot as a PDF in the specified folder\n","        pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/{file_name}.pdf\")\n","        plt.savefig(pdf_path,bbox_inches='tight')\n","        plt.show()\n","        plt.close()  # Close the plot to free memory\n","    else:\n","        print(f\"No confidence envelope data available for {file_name}\")\n"]},{"cell_type":"markdown","metadata":{"id":"NPxKl7HNwd-q"},"source":["## **2.7 Define a specific radius and save as dataframe**\n","\n","This is performed to compare FOV and conditions"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"hQlY8X-nJqYb"},"outputs":[],"source":["# @title ##Define a specific radius and save as dataframe - Slow down\n","\n","\n","# Define the specific radius for comparison\n","specific_radius = 50  # Replace with your chosen radius\n","\n","# Extract L values at the specific radius\n","specific_radius_index = np.argmin(np.abs(r_values - specific_radius))  # Find the index of the closest radius value\n","l_values_at_specific_radius_slow = {fov: l_values[specific_radius_index] for fov, l_values in l_values_per_fov_slow.items()}\n","\n","# Plotting\n","plt.figure(figsize=(12, 6))\n","plt.bar(l_values_at_specific_radius_slow.keys(), l_values_at_specific_radius_slow.values())\n","plt.xlabel('Field of View')\n","plt.ylabel(f\"Ripley's L at radius {specific_radius}\")\n","plt.title(f\"Comparison of Ripley's L Function at Radius {specific_radius} Across Different FOVs\")\n","plt.xticks(rotation=45)\n","# Save the plot as a PDF in the specified folder\n","pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_at_specific_radius_slow.pdf\")\n","plt.savefig(pdf_path, bbox_inches='tight')\n","\n","plt.show()\n","\n","\n","# Create DataFrame with confidence envelopes, median, and L values at the specific radius\n","rows = []\n","for fov, (lower_bound, upper_bound) in confidence_envelopes_slow.items():\n","    l_value = l_values_per_fov_slow[fov][specific_radius_index]\n","    lower = lower_bound[specific_radius_index]\n","    upper = upper_bound[specific_radius_index]\n","\n","    # Retrieve simulated L values for the FOV\n","    simulated_l_values_for_fov_slow = simulated_l_values_dict_slow.get(fov, [])\n","\n","    # Calculate median if simulated L values are available for the FOV\n","    if simulated_l_values_for_fov_slow:\n","        median_vals = [l_vals[specific_radius_index] for l_vals in simulated_l_values_for_fov_slow]\n","        median = np.median(median_vals) if median_vals else np.nan\n","    else:\n","        median = np.nan\n","\n","    rows.append([fov, l_value, lower, upper, median])\n","\n","confidence_df = pd.DataFrame(rows, columns=['File_name', 'Ripley_L_at_Specific_Radius_slow', 'Lower_Bound_slow', 'Upper_Bound_slow', 'Median_slow'])\n","\n","# Merge with additional information\n","additional_info_df = merged_tracks_df[['File_name', 'Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']].drop_duplicates('File_name')\n","merged_df = pd.merge(confidence_df, additional_info_df, left_on='File_name', right_on='File_name')\n","\n","# Save the merged DataFrame to a CSV file\n","merged_df.to_csv(f\"{Results_Folder}/Track_Clustering/ripleys_l_values.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"j465mCknws6q"},"source":["## **2.8 Ripley's L Values Across conditions and cells**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"hN0CAyN3Kr43"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# @title ##Comparison of Ripley\\'s L Values Across Conditions\n","\n","# Convert 'Condition' to string if it's not already\n","merged_df['Condition'] = merged_df['Condition'].astype(str)\n","\n","# Create the box plot\n","plt.figure(figsize=(12, 8))\n","sns.boxplot(data=merged_df, x='Condition', y='Ripley_L_at_Specific_Radius_slow')\n","\n","# Overlay the Monte Carlo simulation results\n","for condition in merged_df['Condition'].unique():\n","    condition_data = merged_df[merged_df['Condition'] == condition]\n","\n","    # Plot median values\n","    medians = condition_data['Median_slow']\n","    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n","\n","    # Handle NaN values and calculate mean and error only for non-NaN values\n","    valid_data = condition_data.dropna(subset=['Median_slow', 'Lower_Bound_slow', 'Upper_Bound_slow'])\n","    if not valid_data.empty:\n","        median_mean = valid_data['Median_slow'].mean()\n","        lower_mean = valid_data['Lower_Bound_slow'].mean()\n","        upper_mean = valid_data['Upper_Bound_slow'].mean()\n","        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n","\n","        # Check if yerr contains valid data before plotting\n","        if not any(np.isnan(yerr)):\n","            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n","\n","# Add labels and title\n","plt.xlabel('Condition')\n","plt.ylabel('Ripley\\'s L at Specific Radius')\n","plt.title('Comparison of Ripley\\'s L Values Across Conditions with Monte Carlo Simulation Results')\n","plt.xticks(rotation=45)\n","plt.grid(True)\n","\n","# Save the figure before showing it\n","pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_Conditions_slow.pdf\")\n","plt.savefig(pdf_path, bbox_inches='tight')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"himdQhR-Kr4_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# @title ##Comparison of Ripley\\'s L Values Across Cells\n","\n","# Convert 'Condition' to string if it's not already\n","merged_df['Cells'] = merged_df['Cells'].astype(str)\n","\n","# Create the box plot\n","plt.figure(figsize=(12, 8))\n","sns.boxplot(data=merged_df, x='Cells', y='Ripley_L_at_Specific_Radius_slow')\n","\n","# Overlay the Monte Carlo simulation results\n","for condition in merged_df['Cells'].unique():\n","    condition_data = merged_df[merged_df['Cells'] == condition]\n","\n","    # Plot median values\n","    medians = condition_data['Median_slow']\n","    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n","\n","    # Handle NaN values and calculate mean and error only for non-NaN values\n","    valid_data = condition_data.dropna(subset=['Median_slow', 'Lower_Bound_slow', 'Upper_Bound_slow'])\n","    if not valid_data.empty:\n","        median_mean = valid_data['Median_slow'].mean()\n","        lower_mean = valid_data['Lower_Bound_slow'].mean()\n","        upper_mean = valid_data['Upper_Bound_slow'].mean()\n","        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n","\n","        # Check if yerr contains valid data before plotting\n","        if not any(np.isnan(yerr)):\n","            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n","\n","# Add labels and title\n","plt.xlabel('Condition')\n","plt.ylabel('Ripley\\'s L at Specific Radius')\n","plt.title('Comparison of Ripley\\'s L Values Across Conditions with Monte Carlo Simulation Results')\n","plt.xticks(rotation=45)\n","plt.grid(True)\n","\n","# Save the figure before showing it\n","pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_Cells_slow.pdf\")\n","plt.savefig(pdf_path, bbox_inches='tight')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fooHjAmK7I8O"},"outputs":[],"source":["# @title ##Comparison of Ripley\\'s L Values Across Cells and Treatment\n","\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import os\n","\n","# Convert 'Cells' and 'Treatment' to string if they are not already\n","merged_df['Cells'] = merged_df['Cells'].astype(str)\n","merged_df['Treatment'] = merged_df['Treatment'].astype(str)\n","\n","# Create a combined factor for Cells and Silencing\n","merged_df['Cells_Treatment'] = merged_df['Cells'] + \"_\" + merged_df['Treatment']\n","\n","# Create the box plot\n","plt.figure(figsize=(14, 8))\n","sns.boxplot(data=merged_df, x='Cells_Treatment', y='Ripley_L_at_Specific_Radius_slow')\n","\n","# Overlay the Monte Carlo simulation results\n","for condition in merged_df['Cells_Treatment'].unique():\n","    condition_data = merged_df[merged_df['Cells_Treatment'] == condition]\n","\n","    # Plot median values\n","    medians = condition_data['Median_slow']\n","    plt.scatter([condition] * len(medians), medians, color='red', alpha=0.5)  # Median\n","\n","    # Handle NaN values and calculate mean and error only for non-NaN values\n","    valid_data = condition_data.dropna(subset=['Median_slow', 'Lower_Bound_slow', 'Upper_Bound_slow'])\n","    if not valid_data.empty:\n","        median_mean = valid_data['Median_slow'].mean()\n","        lower_mean = valid_data['Lower_Bound_slow'].mean()\n","        upper_mean = valid_data['Upper_Bound_slow'].mean()\n","        yerr = [[median_mean - lower_mean], [upper_mean - median_mean]]\n","\n","        # Check if yerr contains valid data before plotting\n","        if not any(np.isnan(yerr)):\n","            plt.errorbar(condition, median_mean, yerr=yerr, fmt='o', color='black', alpha=0.5)  # Confidence interval\n","\n","# Add labels and title\n","plt.xlabel('Cells and Treatment')\n","plt.ylabel('Ripley\\'s L at Specific Radius')\n","plt.title('Comparison of Ripley\\'s L Values Across Cells and Silencing Conditions with Monte Carlo Simulation Results')\n","plt.xticks(rotation=45)\n","plt.grid(True)\n","\n","# Save the figure before showing it\n","pdf_path = os.path.join(f\"{Results_Folder}/Track_Clustering/l_values_Cells_Treatment_slow.pdf\")\n","plt.savefig(pdf_path, bbox_inches='tight')\n","\n","# Show the plot\n","plt.show()\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}