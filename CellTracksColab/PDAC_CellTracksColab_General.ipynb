{"cells":[{"cell_type":"markdown","metadata":{"id":"xF4zYMmXULP7"},"source":["# **PDAC CellTracksColab - General**\n","---\n","\n","### Modified CellTracksColab Notebook for Circulating Cell Attachment Analysis\n","\n","<font size = 4>This version of the CellTracksColab notebook has been specifically adapted to analyze the attachment of circulating cells to endothelial cells. It builds upon the original framework to offer specialized functionalities tailored for this complex aspect of cell migration studies.\n","\n","<font size = 4>For reference, the original CellTracksColab notebook and its comprehensive suite of tools can be found at the CellMigrationLab GitHub repository:\n","\n","<font size = 4>[CellMigrationLab/CellTracksColab](https://github.com/CellMigrationLab/CellTracksColab)\n","\n","\n","\n","<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JrkfFr7mgZmA"},"outputs":[],"source":["# @title #MIT License\n","\n","print(\"\"\"\n","**MIT License**\n","\n","Copyright (c) 2023 Guillaume Jacquemet\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"Y4-Ft-yNRVCc"},"source":["--------------------------------------------------------\n","# **Part 1: Prepare the session and load your data**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9h0prdayn0qG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rAP0ahCzn1V6"},"outputs":[],"source":["#@markdown ##Play to install\n","!pip -q install pandas scikit-learn\n","!pip -q install hdbscan\n","!pip -q install umap-learn\n","!pip -q install plotly\n","!pip -q install tqdm\n","\n","!git clone https://github.com/CellMigrationLab/CellTracksColab.git\n","\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import numpy as np\n","import itertools\n","from matplotlib.gridspec import GridSpec\n","import requests\n","\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import sys\n","import matplotlib.colors as mcolors\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","import itertools\n","import requests\n","import ipywidgets as widgets\n","import warnings\n","import scipy.stats as stats\n","\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.gridspec import GridSpec\n","from ipywidgets import Dropdown, interact,Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","from tqdm.notebook import tqdm\n","from IPython.display import display, clear_output\n","from scipy.spatial import ConvexHull\n","from scipy.spatial.distance import cosine, pdist\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","from sklearn.metrics import pairwise_distances\n","from scipy.stats import zscore, ks_2samp\n","from sklearn.preprocessing import MinMaxScaler\n","from multiprocessing import Pool\n","from matplotlib.ticker import FixedLocator\n","from matplotlib.ticker import FuncFormatter\n","from matplotlib.colors import LogNorm\n","sys.path.append(\"../\")\n","sys.path.append(\"CellTracksColab/\")\n","\n","import celltracks\n","from celltracks import *\n","from celltracks.Track_Plots import *\n","from celltracks.BoxPlots_Statistics import *\n","from celltracks.Track_Metrics import *\n","\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=500000):\n","    \"\"\"Save a DataFrame with a progress bar and gzip compression.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing with gzip compression\n","        with gzip.open(path, \"wt\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3Kzd_8GUnpbw"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GA1wCrkoV4i5"},"outputs":[],"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bsDAwkSOo1gV"},"source":["## **1.3.1 Compile your data or load existing dataframes**\n","---\n","\n","<font size = 4> Please ensure that your data is properly organised\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PmH033RIYCEa"},"outputs":[],"source":["#@markdown ##Run for the CTC vs CIC and IL-1b dataset\n","\n","\n","def populate_columns(df, filename):\n","    cells_conditions = {\n","        'Mia': 'MiaPaca-2', 'P10': 'Panc10',  'p10': 'Panc10', 'As': 'AsPc1',\n","        'neu': 'Neutrophil', 'mono': 'Monocyte', 'mon': 'Monocyte'\n","    }\n","    flow_speed_conditions = {'p1': 300, 'p2': 200, 'p3': 100, 'p4': 'wash'}\n","    Treatment_conditions = {'IL1b': 'IL1b', 'il1b': 'IL1b', 'ctrl': 'CTRL'}\n","\n","    df['Cells'] = next((v for k, v in cells_conditions.items() if k in filename), 'Unknown')\n","    df['Flow_speed'] = next((v for k, v in flow_speed_conditions.items() if k in filename), 'Unknown')\n","    df['Treatment'] = next((v for k, v in Treatment_conditions.items() if k in filename), 'CTRL')\n","    filename_without_extension = os.path.splitext(os.path.basename(filename))[0]\n","    df['File_name'] = remove_suffix(filename_without_extension)\n","    df['Condition'] = df['Cells'] + '_' + df['Flow_speed'].astype(str) + '_' + df['Treatment']\n","    match = re.search(r'n(\\d+)', filename)\n","    df['experiment_nb'] = int(match.group(1)) if match else 'Unknown'\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3Oqbt9SYGeO","cellView":"form"},"outputs":[],"source":["#@markdown ##Run for the CD44 ab, siCD44 and HA datasets\n","\n","\n","def populate_columns(df, filename):\n","    cells_conditions = {\n","        'Mia': 'MiaPaca-2', 'P10': 'Panc10',  'p10': 'Panc10', 'As': 'AsPc1',\n","        'neu': 'Neutrophil', 'mono': 'Monocyte', 'mon': 'Monocyte'\n","    }\n","    flow_speed_conditions = {'p1': 300, 'p2': 200, 'p3': 100, 'p4': 'wash'}\n","    Treatment_conditions = {'siCtrl': 'siCtrl', 'sictrl': 'siCtrl', 'si2': 'siCD44_2', 'si1': 'siCD44_1', 'si3': 'siCD44_3', 'ctrldig': 'ctrldig', 'HUdig': 'HUdig','TCdig': 'TCdig', 'blockboth': 'blockboth', 'ctrlblock': 'ctrlblock', 'HUblock': 'HUblock', 'TCblock': 'TCblock'}\n","\n","    df['Cells'] = next((v for k, v in cells_conditions.items() if k in filename), 'Unknown')\n","    df['Flow_speed'] = next((v for k, v in flow_speed_conditions.items() if k in filename), 'Unknown')\n","    df['Treatment'] = next((v for k, v in Treatment_conditions.items() if k in filename), 'Unknown')\n","    filename_without_extension = os.path.splitext(os.path.basename(filename))[0]\n","    df['File_name'] = remove_suffix(filename_without_extension)\n","    df['Condition'] = df['Cells'] + '_' + df['Flow_speed'].astype(str) + '_' + df['Treatment']\n","    match = re.search(r'n(\\d+)', filename)\n","    df['experiment_nb'] = int(match.group(1)) if match else 'Unknown'\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mcQjCa42GRoG"},"outputs":[],"source":["#@markdown ##Provide the path to your dataset:\n","\n","#@markdown ###You have multiple TrackMate files you want to compile, provide the path to your:\n","\n","import os\n","import re\n","import glob\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import requests\n","import zipfile\n","import gzip\n","\n","\n","Folder_path = ''  # @param {type: \"string\"}\n","\n","#@markdown ###You have existing dataframes, provide the path to your:\n","\n","Track_table = ''  # @param {type: \"string\"}\n","Spot_table = ''  # @param {type: \"string\"}\n","\n","#@markdown ###Provide the path to your Result folder\n","\n","Results_Folder = \"\"  # @param {type: \"string\"}\n","\n","if not Results_Folder:\n","    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n","\n","if not os.path.exists(Results_Folder):\n","    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n","\n","# Print the location of the result folder\n","print(f\"Result folder is located at: {Results_Folder}\")\n","\n","\n","def load_and_populate(file_pattern, usecols=None, chunksize=500000):\n","    df_list = []\n","    pattern = re.compile(file_pattern)\n","    files_to_process = [f for f in glob.glob(Folder_path + '/*') if pattern.match(os.path.basename(f))]\n","\n","    # Metadata list\n","    metadata_list = []\n","\n","    for filepath in tqdm(files_to_process, desc=\"Processing Files\"):\n","        print(filepath)\n","        # Get the expected number of rows in the file (subtracting header rows)\n","        expected_rows = sum(1 for row in open(filepath)) - 4\n","\n","        # Add to the metadata list\n","        metadata_list.append({\n","            'filename': os.path.basename(filepath),\n","            'expected_rows': expected_rows\n","        })\n","\n","        chunked_reader = pd.read_csv(filepath, skiprows=[1, 2, 3], usecols=usecols, chunksize=chunksize)\n","        for chunk in chunked_reader:\n","            df_list.append(populate_columns(chunk, filepath))\n","\n","    if not df_list:\n","        print(f\"No files found with pattern: {file_pattern}\")\n","        return pd.DataFrame()\n","\n","    merged_df = pd.concat(df_list, ignore_index=True)\n","\n","    # Verify the total rows in the merged dataframe matches the total expected rows from metadata\n","    total_expected_rows = sum(item['expected_rows'] for item in metadata_list)\n","    if len(merged_df) != total_expected_rows:\n","        print(f\"Warning: Mismatch in total rows. Expected {total_expected_rows}, found {len(merged_df)} in the merged dataframe.\")\n","    else:\n","        print(f\"Success: The processed dataframe matches the metadata. Total rows: {len(merged_df)}\")\n","\n","    return merged_df\n","\n","def sort_and_generate_repeat(merged_df):\n","    merged_df.sort_values(['Condition', 'experiment_nb'], inplace=True)\n","    merged_df = merged_df.groupby('Condition', group_keys=False).apply(generate_repeat)\n","    return merged_df\n","\n","def generate_repeat(group):\n","    # Convert to string if the experiment_nb has numeric and 'Unknown' values\n","    group['experiment_nb'] = group['experiment_nb'].astype(str)\n","\n","    # Handle non-numeric and missing values if needed, here we assume 'Unknown' is one such value\n","    numeric_part = group[group['experiment_nb'].str.isdigit()]\n","    non_numeric_part = group[~group['experiment_nb'].str.isdigit()]\n","\n","    # Sort numeric values and assign repeats\n","    unique_experiment_nbs_numeric = sorted(numeric_part['experiment_nb'].unique(), key=int)\n","    experiment_nb_to_repeat_numeric = {experiment_nb: i+1 for i, experiment_nb in enumerate(unique_experiment_nbs_numeric)}\n","    numeric_part['Repeat'] = numeric_part['experiment_nb'].map(experiment_nb_to_repeat_numeric)\n","\n","    # Handle non-numeric parts, you can decide how to sort and assign repeat values\n","    # Here we give all 'Unknown' the same repeat number, for example, 0\n","    non_numeric_part['Repeat'] = 0  # Or some other logic for non-numeric parts\n","\n","    # Concatenate the parts back together\n","    group = pd.concat([numeric_part, non_numeric_part])\n","\n","    return group\n","\n","\n","def remove_suffix(filename):\n","    suffixes_to_remove = [\"-tracks\", \"-spots\"]\n","    for suffix in suffixes_to_remove:\n","        if filename.endswith(suffix):\n","            filename = filename[:-len(suffix)]\n","            break\n","    return filename\n","\n","\n","def validate_tracks_df(df):\n","    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def validate_spots_df(df):\n","    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n","    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n","    for col in required_columns:\n","        if col not in df.columns:\n","            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n","            return False\n","\n","    # Additional data type checks or value ranges can be added here\n","    return True\n","\n","def check_unique_id_match(df1, df2):\n","    df1_ids = set(df1['Unique_ID'])\n","    df2_ids = set(df2['Unique_ID'])\n","\n","    # Check if the IDs in the two dataframes match\n","    if df1_ids == df2_ids:\n","        print(\"The Unique_ID values in both dataframes match perfectly!\")\n","    else:\n","        missing_in_df1 = df2_ids - df1_ids\n","        missing_in_df2 = df1_ids - df2_ids\n","\n","        if missing_in_df1:\n","            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n","\n","        if missing_in_df2:\n","            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n","            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n","\n","if Folder_path:\n","\n","    merged_tracks_df = load_and_populate(r'.*tracks.*\\.csv')\n","\n","    if not validate_tracks_df(merged_tracks_df):\n","        print(\"Error: Validation failed for merged tracks dataframe.\")\n","    else:\n","        merged_tracks_df = sort_and_generate_repeat(merged_tracks_df)\n","        merged_tracks_df['Unique_ID'] = merged_tracks_df['File_name'] + \"_\" + merged_tracks_df['TRACK_ID'].astype(str)\n","        save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz', desc=\"Saving Tracks\")\n","\n","\n","    merged_spots_df = load_and_populate(r'.*spots.*\\.csv', usecols=['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T', 'RADIUS', 'CIRCULARITY', 'SOLIDITY', 'SHAPE_INDEX'])\n","\n","    if not validate_spots_df(merged_spots_df):\n","        print(\"Error: Validation failed for merged spots dataframe.\")\n","    else:\n","        merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n","        merged_spots_df.dropna(subset=['POSITION_X', 'POSITION_Y'], inplace=True)\n","        merged_spots_df.reset_index(drop=True, inplace=True)\n","        merged_spots_df['Unique_ID'] = merged_spots_df['File_name'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n","        save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv.gz', desc=\"Saving Spots\")\n","        # Now, call the check function\n","        check_unique_id_match(merged_spots_df, merged_tracks_df)\n","        print(\"...Done\")\n","\n","# For existing dataframes\n","if Track_table:\n","    print(\"Loading track table file....\")\n","    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n","    if not validate_tracks_df(merged_tracks_df):\n","        print(\"Error: Validation failed for loaded tracks dataframe.\")\n","\n","if Spot_table:\n","    print(\"Loading spot table file....\")\n","    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n","    if not validate_spots_df(merged_spots_df):\n","        print(\"Error: Validation failed for loaded spots dataframe.\")\n","\n","\n","check_for_nans(merged_spots_df, \"merged_spots_df\")\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nT0t5jqsGRoG"},"outputs":[],"source":["#@markdown ##Check Metadata\n","\n","\n","# Define the metadata columns that are expected to have identical values for each filename\n","metadata_columns = ['Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']\n","\n","# Group the DataFrame by 'File_name' and then check if all entries within each group are identical\n","consistent_metadata = True\n","for name, group in merged_tracks_df.groupby('File_name'):\n","    for col in metadata_columns:\n","        if not group[col].nunique() == 1:\n","            consistent_metadata = False\n","            print(f\"Inconsistency found for file: {name} in column: {col}\")\n","            break  # Stop checking other columns for this group and move to the next file\n","    if not consistent_metadata:\n","        break  # Stop the entire process if any inconsistency is found\n","\n","if consistent_metadata:\n","    print(\"All files have consistent metadata across the specified columns.\")\n","else:\n","    print(\"There are inconsistencies in the metadata. Please check the output for details.\")\n","\n","# Drop duplicates based on the 'File_name' to get a unique list of filenames and their metadata\n","unique_files_df = merged_tracks_df.drop_duplicates(subset=['File_name'])[['File_name', 'Cells', 'Flow_speed', 'Treatment', 'Condition', 'experiment_nb', 'Repeat']]\n","\n","# Reset the index to clean up the DataFrame\n","unique_files_df.reset_index(drop=True, inplace=True)\n","\n","# Display the resulting DataFrame in a nicely formatted HTML table\n","unique_files_df\n","\n","import pandas as pd\n","\n","# Assuming 'df' is your DataFrame and it already contains 'Conditions' and 'Repeats' columns.\n","\n","# Group by 'Conditions' and 'Repeats' and count the occurrences\n","grouped = unique_files_df.groupby(['Condition', 'Repeat']).size().reset_index(name='counts')\n","\n","# Check if any combinations have a count greater than 1, which means they are not unique\n","non_unique_combinations = grouped[grouped['counts'] > 1]\n","\n","# Print the non-unique combinations\n","if not non_unique_combinations.empty:\n","    print(\"There are non-unique combinations of Conditions and Repeats:\")\n","    print(non_unique_combinations)\n","else:\n","    print(\"All combinations of Conditions and Repeats are unique.\")\n","\n","check_unique_id_match(merged_spots_df, merged_tracks_df)\n","\n","\n","# Group the DataFrame by 'Cells', 'Treatment', 'Repeat' and then check if there are 4 unique 'Flow_speed' values for each group\n","consistent_flow_speeds = True\n","for (cells, Treatment, repeat), group in merged_tracks_df.groupby(['Cells', 'Treatment', 'Repeat']):\n","    if group['Flow_speed'].nunique() != 4:\n","        consistent_flow_speeds = False\n","        print(f\"Inconsistency found for Cells: {cells}, Treatment: {Treatment_conditions}, Repeat: {repeat} - Expected 4 Flow_speeds, found {group['Flow_speed'].nunique()}\")\n","        break  # Stop the entire process if any inconsistency is found\n","\n","if consistent_flow_speeds:\n","    print(\"Each combination of 'Cells', 'Treatment', 'Repeat' has exactly 4 different 'Flow_speed' values.\")\n","else:\n","    print(\"There are inconsistencies in 'Flow_speed' values. Please check the output for details.\")\n","\n","\n","unique_cells = unique_files_df['Cells'].unique()\n","unique_flow_speeds = unique_files_df['Flow_speed'].unique()\n","unique_Treatment = unique_files_df['Treatment'].unique()\n","unique_conditions = unique_files_df['Condition'].unique()\n","\n","print(\"Unique Cells:\", unique_cells)\n","print(\"Unique Flow Speeds:\", unique_flow_speeds)\n","print(\"Unique Silencing:\", unique_Treatment)\n","print(\"Unique Conditions:\", unique_conditions)\n"]},{"cell_type":"markdown","metadata":{"id":"t5YJ9V468HwJ"},"source":["## **1.4. Filter tracks shorter than 50 spots**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mA8GBhFy8vd6"},"outputs":[],"source":["# @title ##Filter tracks shorter than 50 spots\n","\n","\n","merged_tracks_df = merged_tracks_df[merged_tracks_df['NUMBER_SPOTS'] >= 50]\n","merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]\n"]},{"cell_type":"markdown","metadata":{"id":"52STmnv43d45"},"source":["## **1.5. Visualise your tracks**\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AE881uJW5ukQ"},"outputs":[],"source":["# @title ##Run the cell and choose the file you want to inspect\n","\n","import ipywidgets as widgets\n","from ipywidgets import interact\n","import matplotlib.pyplot as plt\n","\n","if not os.path.exists(Results_Folder+\"/Tracks\"):\n","    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","# Create a Dropdown widget with the filenames\n","filename_dropdown = widgets.Dropdown(\n","    options=filenames,\n","    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n","    description='File Name:',\n",")\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.show()\n","    else:\n","        print(\"No valid filename selected\")\n","\n","# Link the Dropdown widget to the plotting function\n","interact(plot_coordinates, filename=filename_dropdown)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"g9aUcAm596CZ"},"outputs":[],"source":["# @title ##Batch Process\n","\n","\n","import os\n","import matplotlib.pyplot as plt\n","\n","# Ensure the Results_Folder/Tracks directory exists\n","if not os.path.exists(Results_Folder + \"/Tracks\"):\n","    os.makedirs(Results_Folder + \"/Tracks\")\n","\n","# Extract unique filenames from the dataframe\n","filenames = merged_spots_df['File_name'].unique()\n","\n","def plot_coordinates(filename):\n","    if filename:\n","        # Filter the DataFrame based on the selected filename\n","        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n","\n","        plt.figure(figsize=(10, 8))\n","        for unique_id in filtered_df['Unique_ID'].unique():\n","            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n","            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n","\n","        plt.xlabel('POSITION_X')\n","        plt.ylabel('POSITION_Y')\n","        plt.title(f'Coordinates for {filename}')\n","        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n","        plt.close()  # Close the plot to avoid displaying it\n","\n","# Loop through all filenames and generate plots\n","for filename in filenames:\n","    plot_coordinates(filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"CQ2P8sw1izGb"},"outputs":[],"source":["# @title ##Speed density plots\n","\n","\n","# Updated code to visualize distributions using the 'fill' parameter in sns.kdeplot\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def plot_distribution_by_condition_updated(df):\n","    conditions = df['Condition'].unique()\n","\n","    # Setting up the plotting environment\n","    sns.set_style(\"whitegrid\")\n","    plt.figure(figsize=(18, 20))  # Increased height to fit the fourth plot\n","\n","    # Plotting histograms for TRACK_MEAN_SPEED\n","    plt.subplot(4, 1, 1)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TRACK_MEAN_SPEED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TRACK_MEAN_SPEED by Condition')\n","    plt.legend()\n","\n","    # Plotting histograms for TRACK_MAX_SPEED\n","    plt.subplot(4, 1, 2)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TRACK_MAX_SPEED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TRACK_MAX_SPEED by Condition')\n","    plt.legend()\n","\n","    # Plotting histograms for TRACK_MIN_SPEED\n","    plt.subplot(4, 1, 3)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TRACK_MIN_SPEED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TRACK_MIN_SPEED by Condition')\n","    plt.legend()\n","\n","    # Plotting histograms for TOTAL_DISTANCE_TRAVELED\n","    plt.subplot(4, 1, 4)\n","    for condition in conditions:\n","        sns.histplot(df[df['Condition'] == condition]['TOTAL_DISTANCE_TRAVELED'], label=condition, kde=False, bins=30)\n","    plt.title('Histogram of TOTAL_DISTANCE_TRAVELED by Condition')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# You can call this function with your dataframe like this:\n","plot_distribution_by_condition_updated(merged_tracks_df)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jWasqb467Z65"},"outputs":[],"source":["# @title ##Time points per tracks\n","\n","\n","import matplotlib.pyplot as plt\n","\n","\n","# Calculate the count of time points per track\n","time_points_per_track = merged_spots_df.groupby('Unique_ID').size()\n","\n","# Plotting\n","plt.figure(figsize=(10, 6))\n","time_points_per_track.hist(bins=30, edgecolor='black')\n","plt.title('Distribution of Time Points per Track')\n","plt.xlabel('Number of Time Points')\n","plt.ylabel('Count of Tracks')\n","plt.grid(False)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Uczq5k7cRgTa"},"source":["--------------------------------------------------------\n","# **Part 2. Compute Additional Metrics (Optional)**\n","--------------------------------------------------------\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W7wUitWQjTiK"},"source":["<font size=\"4\" color=\"red\">Part 2 does not support Track splitting</font>.\n","\n","<font size=\"4\" color=\"red\">Part 2 supports 3D tracking data</font>.\n","\n","<font size=\"4\">In this section, you can compute useful track metrics. These metrics can be calculated from the start to the end of the track or using a rolling window approach.\n","\n","<font size = 4>**Usefulness of Start to End Approach**\n","\n","<font size = 4>The start to end approach calculates metrics over the entire length of the track, providing a comprehensive overview of the track's characteristics from beginning to end. This method is useful for understanding overall trends such as directionality or average speed over the entire track.\n","\n","<font size = 4>**Usefulness of the Rolling Window Approach**\n","\n","<font size = 4>The rolling window approach is particularly useful when comparing tracks of different lengths, especially when the metric is not normalized over time, such as the total distance traveled. By using rolling averages, you ensure that the comparisons account for variations in track length and provide a more consistent basis for analysis.\n","\n","<font size = 4>**Choosing the Window Size**\n","\n","- <font size = 4>**Window Size**: The `window_size` parameter determines the number of data points considered in each rolling calculation. A larger window size will smooth the data more, averaging out short-term variations and focusing on long-term trends. Conversely, a smaller window size will be more sensitive to short-term changes, capturing finer details of the movement.\n","- <font size = 4>**Selection Tips**: The optimal window size depends on the nature of your data and the specific analysis goals. It also depends on the length of your tracks.\n","</font>\n"]},{"cell_type":"markdown","metadata":{"id":"DH-d0fhURgTc"},"source":["## **2.1. Duration and speed metrics**\n","---\n","<font size = 4>When this cell is executed, it calculates various metrics for each unique track (using the whole track). Specifically, for each track, it determines the duration of the track, the average, maximum, minimum, and standard deviation of speeds, as well as the total distance traveled by the tracked object."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Pssm75s1RgTc"},"outputs":[],"source":["# @title ##Calculate duration and speed metrics\n","\n","print(\"Calculating track metrics...\")\n","\n","merged_spots_df['POSITION_Z'] = 0\n","\n","\n","merged_spots_df.dropna(subset=['POSITION_X', 'POSITION_Y', 'POSITION_Z'], inplace=True)\n","\n","tqdm.pandas(desc=\"Calculating Track Metrics\")\n","\n","columns_to_remove = [\n","    \"TRACK_DURATION\",\n","    \"TRACK_MEAN_SPEED\",\n","    \"TRACK_MAX_SPEED\",\n","    \"TRACK_MIN_SPEED\",\n","    \"TRACK_MEDIAN_SPEED\",\n","    \"TRACK_STD_SPEED\",\n","    \"TOTAL_DISTANCE_TRAVELED\"\n","]\n","\n","for column in columns_to_remove:\n","    if column in merged_tracks_df.columns:\n","        merged_tracks_df.drop(column, axis=1, inplace=True)\n","\n","merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","df_track_metrics = merged_spots_df.groupby('Unique_ID').progress_apply(calculate_track_metrics).reset_index()\n","\n","overlapping_columns = merged_tracks_df.columns.intersection(df_track_metrics.columns).drop('Unique_ID')\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","merged_tracks_df = pd.merge(merged_tracks_df, df_track_metrics, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","print(\"...Done\")"]},{"cell_type":"markdown","metadata":{"id":"GE_gsCxVZ5Gj"},"source":["<font size = 4>**Calculate duration and speed metrics using rolling windows**\n","\n","<font size = 4>When this cell is executed, it calculates various metrics for each unique track using a rolling window approach. Specifically, it computes rolling sums for distances traveled and various rolling statistics for speeds, including the mean, median, maximum, minimum, and standard deviation within the defined window.\n","\n","- <font size = 4>**Mean Speed Rolling**: The average speed within each rolling window.\n","- <font size = 4>**Median Speed Rolling**: The median speed within each rolling window.\n","- <font size = 4>**Max Speed Rolling**: The highest speed within each rolling window.\n","- <font size = 4>**Min Speed Rolling**: The lowest speed within each rolling window.\n","- <font size = 4>**Speed Standard Deviation Rolling**: The variability of speeds within each rolling window.\n","- <font size = 4>**Total Distance Traveled Rolling**: The average distance traveled within each rolling window.\n"]},{"cell_type":"code","source":["# @title ##Compute Speed and rolling distance\n","\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","\n","def compute_instantaneous_speed(dataframe):\n","    # Check for required columns\n","    required_columns = ['Unique_ID', 'POSITION_T', 'POSITION_X', 'POSITION_Y']\n","    for col in required_columns:\n","        if col not in dataframe.columns:\n","            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n","\n","    # Check for duplicate entries\n","    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n","        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n","\n","    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n","\n","    speeds = []\n","\n","    for _, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Speeds\"):\n","        # Check for NaN values in columns\n","        if track[['POSITION_X', 'POSITION_Y', 'POSITION_T']].isna().any().any():\n","            raise ValueError(f\"Track with ID '{track['Unique_ID'].iloc[0]}' contains NaN values which might affect the computation.\")\n","\n","        # Calculate the instantaneous speed using positional data and time difference\n","        speed = np.sqrt(track['POSITION_X'].diff()**2 + track['POSITION_Y'].diff()**2) / track['POSITION_T'].diff()\n","\n","        # Ensure that time differences are non-negative\n","        if (track['POSITION_T'].diff() < 0).any():\n","            raise ValueError(f\"Track with ID '{track['Unique_ID'].iloc[0]}' has negative time differences.\")\n","\n","        # Ensuring the first speed value for each track is NaN\n","        speed.iloc[0] = np.nan\n","\n","        speeds.extend(speed.tolist())\n","\n","    # Safety Check\n","    if len(speeds) != len(dataframe):\n","        raise ValueError(\"The computed speeds list length doesn't match the dataframe's length.\")\n","\n","    dataframe['Speed'] = speeds\n","\n","    return dataframe\n","\n","# Example usage:\n","merged_spots_df = compute_instantaneous_speed(merged_spots_df)\n","\n","\n","def compute_rolling_average(dataframe, window_size=5):\n","    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n","\n","    rolling_avg_speeds = []\n","\n","    # Wrap the groupby object with tqdm for progress visualization\n","    for _, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Rolling Averages\"):\n","        rolling_avg = track['Speed'].rolling(window=window_size, min_periods=1, center=True).mean()\n","        rolling_avg_speeds.extend(rolling_avg.tolist())\n","\n","    # Safety Check\n","    if len(rolling_avg_speeds) != len(dataframe):\n","        raise ValueError(\"The computed rolling averages list length doesn't match the dataframe's length.\")\n","\n","    dataframe['RollingAvgSpeed'] = rolling_avg_speeds\n","\n","    return dataframe\n","\n","# Example usage:\n","merged_spots_df = compute_rolling_average(merged_spots_df, window_size=5)\n","\n","\n","def average_speed_first_last_n(dataframe, n=5):\n","    # Ensure n is a positive integer\n","    if not isinstance(n, int) or n <= 0:\n","        raise ValueError(\"n should be a positive integer.\")\n","\n","    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n","\n","    speeds_first = {}\n","    speeds_last = {}\n","\n","    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Calculating average speeds\"):\n","        # Ensure the track has at least n points\n","        if len(track) < n:\n","            print(f\"Track {track_id} has less than {n} points. Skipping.\")\n","            continue\n","\n","\n","        # Average speed for first n time points using RollingAvgSpeed\n","        avg_speed_first = track['Speed'].iloc[:n].mean()\n","        speeds_first[track_id] = avg_speed_first\n","\n","        # Average speed for last n time points using RollingAvgSpeed\n","        avg_speed_last = track['Speed'].iloc[-n:].mean()\n","        speeds_last[track_id] = avg_speed_last\n","\n","    # Convert average speeds to DataFrames\n","    avg_speeds_first_df = pd.DataFrame(speeds_first.items(), columns=['Unique_ID', 'AvgSpeedFirstN'])\n","    avg_speeds_last_df = pd.DataFrame(speeds_last.items(), columns=['Unique_ID', 'AvgSpeedLastN'])\n","\n","    return avg_speeds_first_df, avg_speeds_last_df\n","\n","# Example usage:\n","avg_speeds_first, avg_speeds_last = average_speed_first_last_n(merged_spots_df, 5)\n","\n","\n","def compute_min_rolling_speed(dataframe):\n","    # Safeguard: Ensure required columns are present\n","    required_columns = ['Unique_ID', 'POSITION_T', 'RollingAvgSpeed']\n","    for col in required_columns:\n","        if col not in dataframe.columns:\n","            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n","\n","    # Safeguard: Check for duplicate entries\n","    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n","        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n","\n","    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n","\n","    min_speeds = {}\n","\n","    # Wrap the groupby object with tqdm for progress visualization\n","    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Min Rolling Speeds\"):\n","\n","        min_speed = track['RollingAvgSpeed'].min()\n","        min_speeds[track_id] = min_speed\n","\n","    # Convert the dictionary to a DataFrame\n","    min_speed_df = pd.DataFrame(min_speeds.items(), columns=['Unique_ID', 'MinRollingAvgSpeed'])\n","\n","    return min_speed_df\n","\n","# Compute the minimum rolling speed for merged_spots_df\n","min_rolling_speed_df = compute_min_rolling_speed(merged_spots_df)\n","\n","\n","def merge_speeds(df_main, df_to_merge, key='Unique_ID'):\n","    # Safeguard: Ensure 'key' is present in both dataframes\n","    if key not in df_main.columns or key not in df_to_merge.columns:\n","        raise ValueError(f\"The key '{key}' is not present in both dataframes to be merged.\")\n","\n","    overlapping_columns = df_main.columns.intersection(df_to_merge.columns).drop(key)\n","    df_main.drop(columns=overlapping_columns, inplace=True)\n","    return pd.merge(df_main, df_to_merge, on=key, how='left')\n","\n","\n","merged_tracks_df = merge_speeds(merged_tracks_df, avg_speeds_first)\n","merged_tracks_df = merge_speeds(merged_tracks_df, avg_speeds_last)\n","merged_tracks_df = pd.merge(merged_tracks_df, min_rolling_speed_df)\n","\n","def compute_rolling_distance(dataframe, window_size=3):\n","    \"\"\"Compute the total distance traveled within a rolling time window.\"\"\"\n","    # Safeguard: Ensure required columns are present\n","    required_columns = ['Unique_ID', 'POSITION_T', 'POSITION_X', 'POSITION_Y']\n","    for col in required_columns:\n","        if col not in dataframe.columns:\n","            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n","\n","    # Safeguard: Handle potential negative or zero values for window size\n","    if window_size <= 0:\n","        raise ValueError(\"Window size must be a positive integer.\")\n","\n","    # Safeguard: Check for duplicate entries\n","    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n","        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n","\n","    # Safeguard: Ensure window size is odd for trimming edges correctly\n","    if window_size % 2 == 0:\n","        raise ValueError(\"Please use an odd value for the window size for accurate trimming.\")\n","\n","    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n","\n","    trim_size = window_size // 2  # Determine how much to trim from the edges\n","    rolling_distances = []\n","\n","    for _, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Rolling Distance\"):\n","        # Compute the Euclidean distance between consecutive points\n","        distances = np.sqrt(track['POSITION_X'].diff()**2 + track['POSITION_Y'].diff()**2).fillna(0)\n","\n","        # Compute the rolling sum of distances\n","        rolling_distance = distances.rolling(window=window_size, center=True).sum()\n","\n","        # Trim the edges\n","        rolling_distance[:trim_size] = np.nan\n","        rolling_distance[-trim_size:] = np.nan\n","\n","        rolling_distances.extend(rolling_distance.tolist())\n","\n","    # Safeguard: Ensure the list of rolling distances matches the length of the dataframe\n","    if len(rolling_distances) != len(dataframe):\n","        raise ValueError(\"The computed rolling distances list length doesn't match the dataframe's length.\")\n","\n","    dataframe['RollingDistance'] = rolling_distances\n","    return dataframe\n","\n","merged_spots_df = compute_rolling_distance(merged_spots_df, window_size=5)\n","\n","\n","def average_rolling_distance_first_last_n(dataframe, n=1):\n","    \"\"\"Compute the average rolling distance for the first and last n points.\"\"\"\n","\n","    # Safeguard: Ensure required columns are present\n","    required_columns = ['Unique_ID', 'POSITION_T', 'RollingDistance']\n","    for col in required_columns:\n","        if col not in dataframe.columns:\n","            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n","\n","    # Safeguard: Handle potential non-positive values for n\n","    if n <= 0:\n","        raise ValueError(\"n must be a positive integer.\")\n","\n","    # Safeguard: Check for duplicate entries\n","    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n","        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n","\n","    distance_first = {}\n","    distance_last = {}\n","    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n","\n","\n","    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Calculating average rolling distances\"):\n","        avg_distance_first = track['RollingDistance'].iloc[:n].sum()\n","        distance_first[track_id] = avg_distance_first\n","\n","        avg_distance_last = track['RollingDistance'].iloc[-n:].sum()\n","        distance_last[track_id] = avg_distance_last\n","\n","    avg_distances_first_df = pd.DataFrame(distance_first.items(), columns=['Unique_ID', 'AvgRollingDistanceFirstN'])\n","    avg_distances_last_df = pd.DataFrame(distance_last.items(), columns=['Unique_ID', 'AvgRollingDistanceLastN'])\n","\n","    return avg_distances_first_df, avg_distances_last_df\n","\n","\n","def merge_rolling_distances(df_main, df_to_merge, key='Unique_ID'):\n","    \"\"\"Merge rolling distances into a dataframe.\"\"\"\n","    overlapping_columns = df_main.columns.intersection(df_to_merge.columns).drop(key)\n","\n","    # Safeguard: Ensure that the df_main is updated correctly after dropping overlapping columns\n","    df_main = df_main.drop(columns=overlapping_columns)\n","    return pd.merge(df_main, df_to_merge, on=key, how='left')\n","\n","\n","def compute_min_rolling_distance(dataframe):\n","    \"\"\"Compute the minimum rolling distance for each track.\"\"\"\n","\n","    # Safeguard: Ensure required columns are present\n","    required_columns = ['Unique_ID', 'RollingDistance']\n","    for col in required_columns:\n","        if col not in dataframe.columns:\n","            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n","\n","    min_distances = {}\n","\n","    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Min Rolling Distances\"):\n","        min_distance = track['RollingDistance'].min()\n","        min_distances[track_id] = min_distance\n","\n","    min_distance_df = pd.DataFrame(min_distances.items(), columns=['Unique_ID', 'MinRollingDistance'])\n","\n","    return min_distance_df\n","\n","# Usage and merging operations:\n","avg_distances_first, avg_distances_last = average_rolling_distance_first_last_n(merged_spots_df, 1)\n","merged_tracks_df = merge_rolling_distances(merged_tracks_df, avg_distances_first)\n","merged_tracks_df = merge_rolling_distances(merged_tracks_df, avg_distances_last)\n","\n","min_rolling_distance_df = compute_min_rolling_distance(merged_spots_df)\n","overlapping_columns = merged_tracks_df.columns.intersection(min_rolling_distance_df.columns).drop('Unique_ID')\n","\n","# Safeguard: Ensure that the merged_tracks_df is updated correctly after dropping overlapping columns\n","merged_tracks_df = merged_tracks_df.drop(columns=overlapping_columns)\n","merged_tracks_df = pd.merge(merged_tracks_df, min_rolling_distance_df, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv.gz', desc=\"Saving Spots\")\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz', desc=\"Saving Tracks\")\n","\n","# Safeguard: check for NaN\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","check_for_nans(merged_spots_df, \"merged_spots_df\")\n"],"metadata":{"cellView":"form","id":"nWKeEFFxBIl6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzFeXOcyRgTd"},"source":["## **2.2. Directionality**\n","---\n","<font size = 4>To calculate the directionality of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The directionality, denoted as \\(D\\), is calculated using the formula:\n","\n","$$ D = \\frac{d_{\\text{euclidean}}}{d_{\\text{total path}}} $$\n","\n","where \\($d_{\\text{euclidean}}$\\) is the Euclidean distance between the first and the last points of the track, calculated as:\n","\n","$$ d_{\\text{euclidean}} = \\sqrt{(x_{\\text{end}} - x_{\\text{start}})^2 + (y_{\\text{end}} - y_{\\text{start}})^2 + (z_{\\text{end}} - z_{\\text{start}})^2} $$\n","\n","and \\($d_{\\text{total path}}$\\) is the sum of the Euclidean distances between all consecutive points in the track, representing the total path length traveled. If the total path length is zero, the directionality is defined to be zero. This measure provides insight into the straightness of the path taken, with a value of 1 indicating a straight path between the start and end points, and values approaching 0 indicating more circuitous paths.</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"DeW9ltneRgTd"},"outputs":[],"source":["# @title ##Calculate directionality\n","from celltracks.Track_Metrics import calculate_directionality\n","\n","print(\"In progress...\")\n","\n","merged_spots_df.dropna(subset=['POSITION_X', 'POSITION_Y', 'POSITION_Z'], inplace=True)\n","\n","tqdm.pandas(desc=\"Calculating Directionality\")\n","\n","merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","df_directionality = merged_spots_df.groupby('Unique_ID').progress_apply(calculate_directionality).reset_index()\n","\n","overlapping_columns = merged_tracks_df.columns.intersection(df_directionality.columns).drop('Unique_ID')\n","\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","merged_tracks_df = pd.merge(merged_tracks_df, df_directionality, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","print(\"...Done\")"]},{"cell_type":"markdown","metadata":{"id":"uP6zEopSbPwD"},"source":["<font size = 4>**Calculate directionality using rolling windows**\n","\n","<font size = 4>When this cell is executed, it calculates the directionality for each unique track using a rolling window approach.\n","\n","- <font size = 4>**Directionality Rolling**: The average directionality within each rolling window, indicating how straight the path is in that segment of the track.\n"]},{"cell_type":"markdown","metadata":{"id":"TKGGTaPWRgTd"},"source":["## **2.3. Tortuosity**\n","---\n","<font size = 4>This measure provides insight into the curvature and complexity of the path taken, with a value of 1 indicating a straight path between the start and end points, and values greater than 1 indicating paths with more twists and turns.\n","To calculate the tortuosity of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The tortuosity, denoted as \\(T\\), is calculated using the formula:\n","\n","$$ T = \\frac{d_{\\text{total path}}}{d_{\\text{euclidean}}} $$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ukd-5pTXRgTd"},"outputs":[],"source":["# @title ##Calculate tortuosity\n","print(\"In progress...\")\n","\n","tqdm.pandas(desc=\"Calculating Tortuosity\")\n","\n","merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","df_tortuosity = merged_spots_df.groupby('Unique_ID').progress_apply(calculate_tortuosity).reset_index()\n","\n","overlapping_columns = merged_tracks_df.columns.intersection(df_tortuosity.columns).drop('Unique_ID')\n","\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","merged_tracks_df = pd.merge(merged_tracks_df, df_tortuosity, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","print(\"...Done\")"]},{"cell_type":"markdown","metadata":{"id":"czbdzYEIdlgj"},"source":["<font size = 4>**Calculate tortuosity using rolling windows**\n","\n","<font size = 4>When this cell is executed, it calculates the tortuosity for each unique track using a rolling window approach.\n","\n","- <font size = 4>**Tortuosity Rolling**: The average tortuosity within each rolling window, indicating how convoluted or twisted the path is in that segment of the track. Tortuosity is calculated as the ratio of the total path length to the Euclidean distance between the start and end points of each window. This metric helps in understanding the complexity of movement patterns over short segments of the track, providing insights into the movement behavior of tracked objects.\n"]},{"cell_type":"markdown","metadata":{"id":"44gPYieYRgTe"},"source":["## **2.4. Calculate the total turning angle**\n","---\n","\n","<font size = 4>This measure provides insight into the cumulative amount of turning along the path, with a value of 0 indicating a straight path with no turning, and higher values indicating paths with more turning.\n","\n","<font size = 4>To calculate the Total Turning Angle of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The Total Turning Angle, denoted as \\(A\\), is the sum of the angles between each pair of consecutive direction vectors along the track, representing the cumulative amount of turning along the path.\n","\n","<font size = 4>For each pair of consecutive segments in the track, we calculate the direction vectors \\( $\\vec{v_1}$ \\) and \\($ \\vec{v_2}$ \\), and the angle \\($ \\theta$ \\) between them is calculated using the formula:\n","\n","$$ \\cos(\\theta) = \\frac{\\vec{v_1} \\cdot \\vec{v_2}}{||\\vec{v_1}|| \\cdot ||\\vec{v_2}||} $$\n","\n","<font size = 4>where \\( $\\vec{v_1} \\cdot$ $\\vec{v_2}$ \\) is the dot product of the direction vectors, and \\( $||\\vec{v_1}||$ \\) and \\( $||\\vec{v_2}||$ \\) are the magnitudes of the direction vectors. The Total Turning Angle \\( $A$ \\) is then the sum of all the angles \\( \\$theta$ \\) calculated between each pair of consecutive direction vectors along the track:\n","\n","$$ A = \\sum \\theta $$\n","<font size = 4>\n","If either of the direction vectors is a zero vector, the angle between them is undefined, and such cases are skipped in the calculation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"av1nIJjyRgTe"},"outputs":[],"source":["# @title ##Calculate the total turning angle\n","\n","tqdm.pandas(desc=\"Calculating Total Turning Angle\")\n","\n","merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","df_turning_angle = merged_spots_df.groupby('Unique_ID').progress_apply(calculate_total_turning_angle).reset_index()\n","\n","overlapping_columns = merged_tracks_df.columns.intersection(df_turning_angle.columns).drop('Unique_ID')\n","\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","merged_tracks_df = pd.merge(merged_tracks_df, df_turning_angle, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","print(\"...Done\")"]},{"cell_type":"markdown","metadata":{"id":"Gu7AK3Maem_v"},"source":["<font size = 4>**Calculate the total turning angle using rolling windows**\n","\n","<font size = 4>When this cell is executed, it calculates the total turning angle for each unique track using a rolling window approach.\n","\n","- <font size = 4>**Total Turning Angle Rolling**: The average total turning angle within each rolling window, indicating how much the direction of movement changes over short segments of the track. This metric helps in understanding the directional changes and maneuverability of the tracked objects over time.\n"]},{"cell_type":"markdown","metadata":{"id":"-rxg8meFRgTe"},"source":["## **2.5. Calculate the Spatial Coverage**\n","---\n","\n","<font size = 4>Spatial coverage provides insight into the spatial extent covered by the object's movement, with higher values indicating that the object has covered a larger area or volume during its movement.\n","\n","\n","<font size = 4>To calculate the spatial coverage of a track in 2D or 3D space, we consider a series of points each with \\(x\\), \\(y\\), and optionally \\(z\\) coordinates, sorted by time. The spatial coverage, denoted as \\(S\\), represents the area (in 2D) or volume (in 3D) enclosed by the convex hull formed by the points in the track. It provides insight into the spatial extent covered by the moving object.\n","\n","<font size = 4>**In the implementation below we:**\n","1. <font size = 4>**Check Dimensionality**:\n","   <font size = 4>- If the variance of the \\(z\\) coordinates is zero, implying all \\(z\\) coordinates are the same, the spatial coverage is calculated in 2D using only the \\(x\\) and \\(y\\) coordinates.\n","  <font size = 4> - If the \\(z\\) coordinates vary, the spatial coverage is calculated in 3D using the \\(x\\), \\(y\\), and \\(z\\) coordinates.\n","\n","2. <font size = 4>**Form Convex Hull**:\n","   <font size = 4>- In 2D, a minimum of 3 non-collinear points is required to form a convex hull.\n","   <font size = 4>- In 3D, a minimum of 4 non-coplanar points is required to form a convex hull.\n","   <font size = 4>- If the required minimum points are not available, the spatial coverage is defined to be zero.\n","\n","3. <font size = 4>**Calculate Spatial Coverage**:\n","   <font size = 4>- In 2D, the spatial coverage \\(S\\) is the area of the convex hull formed by the points in the track.\n","   <font size = 4>- In 3D, the spatial coverage \\(S\\) is the volume of the convex hull formed by the points in the track.\n","\n","<font size = 4>**Formula:**\n","- For 2D Spatial Coverage (Area of Convex Hull), if points are \\(P_1(x_1, y_1), P_2(x_2, y_2), \\ldots, P_n(x_n, y_n)\\):\n","  $$ S_{2D} = \\text{Area of Convex Hull formed by } P_1, P_2, \\ldots, P_n $$\n","\n","- For 3D Spatial Coverage (Volume of Convex Hull), if points are \\(P_1(x_1, y_1, z_1), P_2(x_2, y_2, z_2), \\ldots, P_n(x_n, y_n, z_n)\\):\n","  $$ S_{3D} = \\text{Volume of Convex Hull formed by } P_1, P_2, \\ldots, P_n $$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ogUUK9c_RgTf"},"outputs":[],"source":["# @title ##Calculate the Spatial Coverage\n","\n","tqdm.pandas(desc=\"Calculating Spatial Coverage\")\n","\n","merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","df_spatial_coverage = merged_spots_df.groupby('Unique_ID').progress_apply(calculate_spatial_coverage).reset_index()\n","\n","overlapping_columns = merged_tracks_df.columns.intersection(df_spatial_coverage.columns).drop('Unique_ID')\n","\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","merged_tracks_df = pd.merge(merged_tracks_df, df_spatial_coverage, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","print(\"...Done\")"]},{"cell_type":"markdown","metadata":{"id":"rGgf-Elye755"},"source":["<font size = 4>**Calculate Spatial Coverage using rolling windows**\n","\n","<font size = 4>When this cell is executed, it calculates the spatial coverage for each unique track using a rolling window approach.\n","\n","- <font size = 4>**Spatial Coverage Rolling**: The average spatial coverage within each rolling window, representing the area (in 2D) or volume (in 3D) covered by the tracked object over short segments of the track. This metric helps in understanding the spatial extent and movement patterns of the tracked objects over time.\n"]},{"cell_type":"markdown","metadata":{"id":"am9LyzUsPtBy"},"source":["## **2.6. Compute additional metrics**\n","---\n","\n","<font size = 4>This cell computes various metrics for each track in the provided dataset. These metrics are derived from the information provided by your tracking software.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"U9c09HTaoZNy"},"outputs":[],"source":["# @title ##Compute additional metrics\n","\n","print(\"In progress...\")\n","\n","# List of potential metrics to compute\n","potential_metrics = [\n","    'MEAN_INTENSITY_CH1', 'MEDIAN_INTENSITY_CH1', 'MIN_INTENSITY_CH1', 'MAX_INTENSITY_CH1',\n","    'TOTAL_INTENSITY_CH1', 'STD_INTENSITY_CH1', 'CONTRAST_CH1', 'SNR_CH1', 'ELLIPSE_X0',\n","    'ELLIPSE_Y0', 'ELLIPSE_MAJOR', 'ELLIPSE_MINOR', 'ELLIPSE_THETA', 'ELLIPSE_ASPECTRATIO',\n","    'AREA', 'PERIMETER', 'CIRCULARITY', 'SOLIDITY', 'SHAPE_INDEX','MEAN_INTENSITY_CH2', 'MEDIAN_INTENSITY_CH2', 'MIN_INTENSITY_CH2', 'MAX_INTENSITY_CH2',\n","    'TOTAL_INTENSITY_CH2', 'STD_INTENSITY_CH2', 'CONTRAST_CH2', 'SNR_CH2', 'MEAN_INTENSITY_CH3', 'MEDIAN_INTENSITY_CH3', 'MIN_INTENSITY_CH3', 'MAX_INTENSITY_CH3',\n","    'TOTAL_INTENSITY_CH3', 'STD_INTENSITY_CH3', 'CONTRAST_CH3', 'SNR_CH3', 'MEAN_INTENSITY_CH4', 'MEDIAN_INTENSITY_CH4', 'MIN_INTENSITY_CH4', 'MAX_INTENSITY_CH4',\n","    'TOTAL_INTENSITY_CH4', 'STD_INTENSITY_CH4', 'CONTRAST_CH4', 'SNR_CH4',\n","    'Diameter_0',\t'Euclidean_Diameter_0',\t'Number_of_Holes_0',\t'Center_of_the_Skeleton_0',\t'Center_of_the_Skeleton_1',\n","    'Length_of_the_Skeleton_0',\t'Convexity_0',\t'Number_of_Defects_0',\t'Mean_Defect_Displacement_0',\t'Mean_Defect_Area_0',\n","    'Variance_of_Defect_Area_0',\t'Convex_Hull_Center_0',\t'Convex_Hull_Center_1', 'Object_Center_0',\t'Object_Center_1',\n","    'Object_Area_0',\t'Kurtosis_of_Intensity_0',\t'Maximum_intensity_0',\t'Mean_Intensity_0',\t'Minimum_intensity_0',\n","    'Principal_components_of_the_object_0', 'Principal_components_of_the_object_1',\t'Principal_components_of_the_object_2',\n","    'Principal_components_of_the_object_3', 'Radii_of_the_object_0',\t'Radii_of_the_object_1',\t'Skewness_of_Intensity_0',\n","    'Total_Intensity_0',\t'Variance_of_Intensity_0',\t'Bounding_Box_Maximum_0',\t'Bounding_Box_Maximum_1',\t'Bounding_Box_Minimum_0',\n","    'Bounding_Box_Minimum_1',\t'Size_in_pixels_0'\n","]\n","\n","available_metrics = check_metrics_availability(merged_spots_df, potential_metrics)\n","\n","morphological_metrics_df = compute_morphological_metrics(merged_spots_df, available_metrics)\n","\n","morphological_metrics_df.reset_index(inplace=True)\n","\n","if 'Unique_ID' in merged_tracks_df.columns:\n","    overlapping_columns = merged_tracks_df.columns.intersection(morphological_metrics_df.columns).drop('Unique_ID', errors='ignore')\n","    merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","    merged_tracks_df = merged_tracks_df.merge(morphological_metrics_df, on='Unique_ID', how='left')\n","    save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","\n","else:\n","    print(\"Error: 'Unique_ID' column missing in merged_tracks_df. Skipping merging with morphological metrics.\")\n","\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n","print(\"...Done\")"]},{"cell_type":"markdown","metadata":{"id":"32o1NIy8CP11"},"source":["## **2.7. Calculate the FMI**\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Kg0iXBV034Ve"},"outputs":[],"source":["# @title #Calculate the FMI\n","\n","from tqdm.notebook import tqdm\n","\n","def calculate_fmi(group):\n","    group = group.sort_values('POSITION_T')\n","\n","    deltas = np.sqrt(group['POSITION_X'].diff().fillna(0)**2 + group['POSITION_Y'].diff().fillna(0)**2)\n","    total_path_length = deltas.sum()\n","\n","    total_forward_displacement = group['POSITION_X'].diff().fillna(0).sum()\n","\n","    FMI = total_forward_displacement / total_path_length if total_path_length != 0 else 0\n","\n","    return pd.Series({'FMI': FMI})\n","\n","\n","# Use tqdm.pandas() for progress_apply\n","tqdm.pandas(desc=\"Processing tracks\")\n","\n","# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n","merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n","\n","# Group by track ID and calculate metrics with tqdm progress bar\n","grouped = merged_spots_df.groupby('Unique_ID')\n","df_fmi = grouped.progress_apply(calculate_fmi).reset_index()\n","\n","# Find the overlapping columns between the two DataFrames, excluding the merging key\n","overlapping_columns = merged_tracks_df.columns.intersection(df_fmi.columns).drop('Unique_ID')\n","\n","# Drop the overlapping columns from the left DataFrame\n","merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n","\n","# Merge the FMI values back into the original DataFrame\n","merged_tracks_df = pd.merge(merged_tracks_df, df_fmi, on='Unique_ID', how='left')\n","\n","save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv.gz')\n","\n","check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"joRI14WVUPuM"},"source":["-------------------------------------------\n","\n","# **Part 3. Plot track parameters**\n","-------------------------------------------\n","\n","<font size = 4> In this section you can plot all the track parameters previously computed. Data and graphs are automatically saved in your result folder.\n","\n","<font size = 4 color=\"red\"> Parameters computed are in the unit you provided when tracking your data in TrackMate.\n","\n","##**Statistical analyses**\n","### Cohen's d (Effect Size):\n","<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n","\n","### Randomization Test:\n","<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n","\n","### Bonferroni Correction:\n","<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"96j19agHQSEV"},"outputs":[],"source":["# @title ##Plot track parameters\n","\n","# Import necessary libraries\n","import os\n","import itertools\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","from matplotlib.backends.backend_pdf import PdfPages\n","import ipywidgets as widgets\n","from matplotlib.ticker import FixedLocator\n","\n","# Check and create necessary directories\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots\")\n","\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/pdf\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots/pdf\")\n","\n","if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/csv\"):\n","    os.makedirs(f\"{Results_Folder}/track_parameters_plots/csv\")\n","\n","\n","def get_selectable_columns(df):\n","    \"\"\"Get columns that can be plotted.\"\"\"\n","    exclude_cols = ['Condition', 'File_name', 'Flow_speed', 'Cells', 'Treatment', 'Repeat', 'Unique_ID',\n","                    'experiment_nb', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION',\n","                    'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n","    return [col for col in df.columns if col not in exclude_cols]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    \"\"\"Display checkboxes for selecting variables.\"\"\"\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3))\n","    ]))\n","    return variable_checkboxes\n","\n","\n","def create_filename(base, selected_cells, selected_speeds, selected_Treatment, var):\n","    \"\"\"Create a unique filename based on selected options.\"\"\"\n","    def summarize_options(options):\n","        if len(options) > 3:\n","            return f\"{len(options)}options\"\n","        return \"_\".join(options)\n","\n","    selected_options = \"_\".join([\n","        summarize_options(selected_cells),\n","        summarize_options(selected_speeds),\n","        summarize_options(selected_Treatment)\n","    ])\n","\n","    filename = f\"{base}_{selected_options}_{var}.pdf\"\n","    return filename.replace(\" \", \"_\")  # Replace spaces with underscores for file compatibility\n","\n","\n","# Create checkboxes for various attributes\n","cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in merged_tracks_df['Cells'].unique()]\n","flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in merged_tracks_df['Flow_speed'].unique()]\n","Treatment_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in merged_tracks_df['Treatment'].unique()]\n","\n","\n","# Display checkboxes\n","display(widgets.VBox([\n","    widgets.Label('Cells:'),\n","    widgets.GridBox(cells_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n","    widgets.Label('Flow Speed:'),\n","    widgets.GridBox(flow_speed_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n","    widgets.Label('Treatment:'),\n","    widgets.GridBox(Treatment_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4))\n","\n","]))\n","\n","# Convert Flow_speed to string for checkbox matching\n","merged_tracks_df['Flow_speed'] = merged_tracks_df['Flow_speed'].astype(str)\n","\n","# Define the plotting function\n","def plot_selected_vars(button, variable_checkboxes):\n","    print(\"Plotting in progress...\")\n","\n","    # Fetch selected values\n","    selected_cells = [box.description for box in cells_checkboxes if box.value]\n","    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n","    selected_Treatment = [box.description for box in Treatment_checkboxes if box.value]\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","\n","    # Filter dataframe\n","    filtered_df = merged_tracks_df.copy()\n","    filtered_df = filtered_df[filtered_df['Cells'].isin(selected_cells)]\n","    filtered_df = filtered_df[filtered_df['Flow_speed'].isin(selected_speeds)]\n","    filtered_df = filtered_df[filtered_df['Treatment'].isin(selected_Treatment)]\n","\n","    # Initialize matrices for statistics\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df['Condition'].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      filename = create_filename(\"track_parameters_plots\", selected_cells, selected_speeds, selected_Treatment, var)\n","      pdf_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"pdf\", filename)\n","      csv_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"csv\", f\"{filename[:-4]}.csv\")  # Remove '.pdf' and add '.csv'\n","\n","      pdf_pages = PdfPages(pdf_path)\n","\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = filtered_df[filtered_df['Condition'] == cond1][var]\n","        group2 = filtered_df[filtered_df['Condition'] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = cohen_d(new_group1, new_group2)\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = (count_extreme + 1) / (n_iterations + 1)\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(csv_path)\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = filtered_df[['Condition', var, 'Repeat', 'File_name' ]]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/track_parameters_plots/csv/{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = filtered_df[var].quantile(0.25)\n","      Q3 = filtered_df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","\n","    # Plotting\n","      sns.boxplot(x='Condition', y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x='Condition', y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","# Close the PDF\n","      pdf_pages.close()\n","\n","# Display variable checkboxes and button\n","selectable_columns = get_selectable_columns(merged_tracks_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes))\n","display(button)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["Y4-Ft-yNRVCc","Uczq5k7cRgTa","joRI14WVUPuM"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}