{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **PDAC CellTracksColab - General**\n",
        "---\n",
        "\n",
        "<font size = 4>Colab Notebook for Analyzing Migration Tracks generated by [TrackMate](https://imagej.net/plugins/trackmate/)\n",
        "\n",
        "\n",
        "<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1: Prepare the session and load your data**\n",
        "--------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rAP0ahCzn1V6"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install\n",
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install hdbscan\n",
        "!pip -q install umap-learn\n",
        "!pip -q install plotly\n",
        "!pip -q install tqdm\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import itertools\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import requests\n",
        "\n",
        "\n",
        "#----------------------- Key functions -----------------------------#\n",
        "\n",
        "# Function to calculate Cohen's d\n",
        "def cohen_d(group1, group2):\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var()\n",
        "    var2 = group2.var()\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
        "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
        "\n",
        "    # Estimating the number of chunks based on the provided chunk size\n",
        "    num_chunks = int(len(df) / chunk_size) + 1\n",
        "\n",
        "    # Create a tqdm instance for progress tracking\n",
        "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
        "        # Open the file for writing\n",
        "        with open(path, \"w\") as f:\n",
        "            # Write the header once at the beginning\n",
        "            df.head(0).to_csv(f, index=False)\n",
        "\n",
        "            for chunk in np.array_split(df, num_chunks):\n",
        "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "\n",
        "def check_for_nans(df, df_name):\n",
        "    \"\"\"\n",
        "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
        "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
        "    \"\"\"\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            nan_count = df[col].isna().sum()\n",
        "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"No NaN values found in {df_name}.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDAwkSOo1gV"
      },
      "source": [
        "## **1.3. Compile your data or load existing dataframes**\n",
        "---\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQKXq3giI3nX"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Provide the path to your dataset (chunk):\n",
        "\n",
        "#@markdown ###You have multiple TrackMate files you want to compile, provide the path to your:\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "Folder_path = ''  # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ###You have existing dataframes, provide the path to your:\n",
        "\n",
        "Track_table = ''  # @param {type: \"string\"}\n",
        "Spot_table = ''  # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ###Provide the path to your Result folder\n",
        "\n",
        "Results_Folder = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "if not Results_Folder:\n",
        "    Results_Folder = '/content/Results'  # Default Results_Folder path if not defined\n",
        "\n",
        "if not os.path.exists(Results_Folder):\n",
        "    os.makedirs(Results_Folder)  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Print the location of the result folder\n",
        "print(f\"Result folder is located at: {Results_Folder}\")\n",
        "\n",
        "def populate_columns(df, filename):\n",
        "    cells_conditions = {\n",
        "        'Mia': 'MiaPaca-2', 'P10': 'Panc10',  'p10': 'Panc10', 'As': 'AsPc1',\n",
        "        'neu': 'Neutrophil', 'mono': 'Monocyte', 'mon': 'Monocyte'\n",
        "    }\n",
        "    flow_speed_conditions = {'p1': 300, 'p2': 200, 'p3': 100, 'p4': 'wash'}\n",
        "    ilbeta_conditions = {'IL1b': 'IL1b', 'il1b': 'IL1b', 'ctrl': 'CTRL'}\n",
        "\n",
        "    df['Cells'] = next((v for k, v in cells_conditions.items() if k in filename), 'Unknown')\n",
        "    df['Flow_speed'] = next((v for k, v in flow_speed_conditions.items() if k in filename), 'Unknown')\n",
        "    df['ILbeta'] = next((v for k, v in ilbeta_conditions.items() if k in filename), 'CTRL')\n",
        "    filename_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
        "    df['File_name'] = remove_suffix(filename_without_extension)\n",
        "    df['Condition'] = df['Cells'] + '_' + df['Flow_speed'].astype(str) + '_' + df['ILbeta']\n",
        "    match = re.search(r'n(\\d+)', filename)\n",
        "    df['experiment_nb'] = int(match.group(1)) if match else 'Unknown'\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_populate(file_pattern, usecols=None, chunksize=500000):\n",
        "    df_list = []\n",
        "    pattern = re.compile(file_pattern)\n",
        "    files_to_process = [f for f in glob.glob(Folder_path + '/*') if pattern.match(os.path.basename(f))]\n",
        "\n",
        "    # Metadata list\n",
        "    metadata_list = []\n",
        "\n",
        "    for filepath in tqdm(files_to_process, desc=\"Processing Files\"):\n",
        "        print(filepath)\n",
        "        # Get the expected number of rows in the file (subtracting header rows)\n",
        "        expected_rows = sum(1 for row in open(filepath)) - 4\n",
        "\n",
        "        # Add to the metadata list\n",
        "        metadata_list.append({\n",
        "            'filename': os.path.basename(filepath),\n",
        "            'expected_rows': expected_rows\n",
        "        })\n",
        "\n",
        "        chunked_reader = pd.read_csv(filepath, skiprows=[1, 2, 3], usecols=usecols, chunksize=chunksize)\n",
        "        for chunk in chunked_reader:\n",
        "            df_list.append(populate_columns(chunk, filepath))\n",
        "\n",
        "    if not df_list:\n",
        "        print(f\"No files found with pattern: {file_pattern}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    merged_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "    # Verify the total rows in the merged dataframe matches the total expected rows from metadata\n",
        "    total_expected_rows = sum(item['expected_rows'] for item in metadata_list)\n",
        "    if len(merged_df) != total_expected_rows:\n",
        "        print(f\"Warning: Mismatch in total rows. Expected {total_expected_rows}, found {len(merged_df)} in the merged dataframe.\")\n",
        "    else:\n",
        "        print(f\"Success: The processed dataframe matches the metadata. Total rows: {len(merged_df)}\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def sort_and_generate_repeat(merged_df):\n",
        "    merged_df.sort_values(['Condition', 'experiment_nb'], inplace=True)\n",
        "    merged_df = merged_df.groupby('Condition', group_keys=False).apply(generate_repeat)\n",
        "    return merged_df\n",
        "\n",
        "def generate_repeat(group):\n",
        "    # Convert to string if the experiment_nb has numeric and 'Unknown' values\n",
        "    group['experiment_nb'] = group['experiment_nb'].astype(str)\n",
        "\n",
        "    # Handle non-numeric and missing values if needed, here we assume 'Unknown' is one such value\n",
        "    numeric_part = group[group['experiment_nb'].str.isdigit()]\n",
        "    non_numeric_part = group[~group['experiment_nb'].str.isdigit()]\n",
        "\n",
        "    # Sort numeric values and assign repeats\n",
        "    unique_experiment_nbs_numeric = sorted(numeric_part['experiment_nb'].unique(), key=int)\n",
        "    experiment_nb_to_repeat_numeric = {experiment_nb: i+1 for i, experiment_nb in enumerate(unique_experiment_nbs_numeric)}\n",
        "    numeric_part['Repeat'] = numeric_part['experiment_nb'].map(experiment_nb_to_repeat_numeric)\n",
        "\n",
        "    # Handle non-numeric parts, you can decide how to sort and assign repeat values\n",
        "    # Here we give all 'Unknown' the same repeat number, for example, 0\n",
        "    non_numeric_part['Repeat'] = 0  # Or some other logic for non-numeric parts\n",
        "\n",
        "    # Concatenate the parts back together\n",
        "    group = pd.concat([numeric_part, non_numeric_part])\n",
        "\n",
        "    return group\n",
        "\n",
        "\n",
        "def remove_suffix(filename):\n",
        "    suffixes_to_remove = [\"-tracks\", \"-spots\"]\n",
        "    for suffix in suffixes_to_remove:\n",
        "        if filename.endswith(suffix):\n",
        "            filename = filename[:-len(suffix)]\n",
        "            break\n",
        "    return filename\n",
        "\n",
        "\n",
        "def validate_tracks_df(df):\n",
        "    \"\"\"Validate the tracks dataframe for necessary columns and data types.\"\"\"\n",
        "    required_columns = ['TRACK_ID']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Error: Column '{col}' missing in tracks dataframe.\")\n",
        "            return False\n",
        "\n",
        "    # Additional data type checks or value ranges can be added here\n",
        "    return True\n",
        "\n",
        "def validate_spots_df(df):\n",
        "    \"\"\"Validate the spots dataframe for necessary columns and data types.\"\"\"\n",
        "    required_columns = ['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Error: Column '{col}' missing in spots dataframe.\")\n",
        "            return False\n",
        "\n",
        "    # Additional data type checks or value ranges can be added here\n",
        "    return True\n",
        "\n",
        "def check_unique_id_match(df1, df2):\n",
        "    df1_ids = set(df1['Unique_ID'])\n",
        "    df2_ids = set(df2['Unique_ID'])\n",
        "\n",
        "    # Check if the IDs in the two dataframes match\n",
        "    if df1_ids == df2_ids:\n",
        "        print(\"The Unique_ID values in both dataframes match perfectly!\")\n",
        "    else:\n",
        "        missing_in_df1 = df2_ids - df1_ids\n",
        "        missing_in_df2 = df1_ids - df2_ids\n",
        "\n",
        "        if missing_in_df1:\n",
        "            print(f\"There are {len(missing_in_df1)} Unique_ID values present in the second dataframe but missing in the first.\")\n",
        "            print(\"Examples of these IDs are:\", list(missing_in_df1)[:5])\n",
        "\n",
        "        if missing_in_df2:\n",
        "            print(f\"There are {len(missing_in_df2)} Unique_ID values present in the first dataframe but missing in the second.\")\n",
        "            print(\"Examples of these IDs are:\", list(missing_in_df2)[:5])\n",
        "\n",
        "if Folder_path:\n",
        "\n",
        "    merged_tracks_df = load_and_populate(r'.*tracks.*\\.csv')\n",
        "\n",
        "    if not validate_tracks_df(merged_tracks_df):\n",
        "        print(\"Error: Validation failed for merged tracks dataframe.\")\n",
        "    else:\n",
        "        merged_tracks_df = sort_and_generate_repeat(merged_tracks_df)\n",
        "        merged_tracks_df['Unique_ID'] = merged_tracks_df['File_name'] + \"_\" + merged_tracks_df['TRACK_ID'].astype(str)\n",
        "        save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv', desc=\"Saving Tracks\")\n",
        "\n",
        "\n",
        "    merged_spots_df = load_and_populate(r'.*spots.*\\.csv', usecols=['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T', 'RADIUS', 'CIRCULARITY', 'SOLIDITY', 'SHAPE_INDEX'])\n",
        "\n",
        "    if not validate_spots_df(merged_spots_df):\n",
        "        print(\"Error: Validation failed for merged spots dataframe.\")\n",
        "    else:\n",
        "        merged_spots_df = sort_and_generate_repeat(merged_spots_df)\n",
        "        merged_spots_df.dropna(subset=['POSITION_X', 'POSITION_Y'], inplace=True)\n",
        "        merged_spots_df.reset_index(drop=True, inplace=True)\n",
        "        merged_spots_df['Unique_ID'] = merged_spots_df['File_name'] + \"_\" + merged_spots_df['TRACK_ID'].astype(str)\n",
        "        save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv', desc=\"Saving Spots\")\n",
        "        # Now, call the check function\n",
        "        check_unique_id_match(merged_spots_df, merged_tracks_df)\n",
        "        print(\"...Done\")\n",
        "\n",
        "# For existing dataframes\n",
        "if Track_table:\n",
        "    print(\"Loading track table file....\")\n",
        "    merged_tracks_df = pd.read_csv(Track_table, low_memory=False)\n",
        "    if not validate_tracks_df(merged_tracks_df):\n",
        "        print(\"Error: Validation failed for loaded tracks dataframe.\")\n",
        "\n",
        "if Spot_table:\n",
        "    print(\"Loading spot table file....\")\n",
        "    merged_spots_df = pd.read_csv(Spot_table, low_memory=False)\n",
        "    if not validate_spots_df(merged_spots_df):\n",
        "        print(\"Error: Validation failed for loaded spots dataframe.\")\n",
        "\n",
        "\n",
        "check_for_nans(merged_spots_df, \"merged_spots_df\")\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CpBVp_H-Ee0c"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Check Metadata\n",
        "\n",
        "\n",
        "# Define the metadata columns that are expected to have identical values for each filename\n",
        "metadata_columns = ['Cells', 'Flow_speed', 'ILbeta', 'Condition', 'experiment_nb', 'Repeat']\n",
        "\n",
        "# Group the DataFrame by 'File_name' and then check if all entries within each group are identical\n",
        "consistent_metadata = True\n",
        "for name, group in merged_tracks_df.groupby('File_name'):\n",
        "    for col in metadata_columns:\n",
        "        if not group[col].nunique() == 1:\n",
        "            consistent_metadata = False\n",
        "            print(f\"Inconsistency found for file: {name} in column: {col}\")\n",
        "            break  # Stop checking other columns for this group and move to the next file\n",
        "    if not consistent_metadata:\n",
        "        break  # Stop the entire process if any inconsistency is found\n",
        "\n",
        "if consistent_metadata:\n",
        "    print(\"All files have consistent metadata across the specified columns.\")\n",
        "else:\n",
        "    print(\"There are inconsistencies in the metadata. Please check the output for details.\")\n",
        "\n",
        "# Drop duplicates based on the 'File_name' to get a unique list of filenames and their metadata\n",
        "unique_files_df = merged_tracks_df.drop_duplicates(subset=['File_name'])[['File_name', 'Cells', 'Flow_speed', 'ILbeta', 'Condition', 'experiment_nb', 'Repeat']]\n",
        "\n",
        "# Reset the index to clean up the DataFrame\n",
        "unique_files_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the resulting DataFrame in a nicely formatted HTML table\n",
        "unique_files_df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it already contains 'Conditions' and 'Repeats' columns.\n",
        "\n",
        "# Group by 'Conditions' and 'Repeats' and count the occurrences\n",
        "grouped = unique_files_df.groupby(['Condition', 'Repeat']).size().reset_index(name='counts')\n",
        "\n",
        "# Check if any combinations have a count greater than 1, which means they are not unique\n",
        "non_unique_combinations = grouped[grouped['counts'] > 1]\n",
        "\n",
        "# Print the non-unique combinations\n",
        "if not non_unique_combinations.empty:\n",
        "    print(\"There are non-unique combinations of Conditions and Repeats:\")\n",
        "    print(non_unique_combinations)\n",
        "else:\n",
        "    print(\"All combinations of Conditions and Repeats are unique.\")\n",
        "\n",
        "check_unique_id_match(merged_spots_df, merged_tracks_df)\n",
        "\n",
        "\n",
        "# Group the DataFrame by 'Cells', 'ILbeta', 'Repeat' and then check if there are 4 unique 'Flow_speed' values for each group\n",
        "consistent_flow_speeds = True\n",
        "for (cells, ilbeta, repeat), group in merged_tracks_df.groupby(['Cells', 'ILbeta', 'Repeat']):\n",
        "    if group['Flow_speed'].nunique() != 4:\n",
        "        consistent_flow_speeds = False\n",
        "        print(f\"Inconsistency found for Cells: {cells}, ILbeta: {ilbeta}, Repeat: {repeat} - Expected 4 Flow_speeds, found {group['Flow_speed'].nunique()}\")\n",
        "        break  # Stop the entire process if any inconsistency is found\n",
        "\n",
        "if consistent_flow_speeds:\n",
        "    print(\"Each combination of 'Cells', 'ILbeta', 'Repeat' has exactly 4 different 'Flow_speed' values.\")\n",
        "else:\n",
        "    print(\"There are inconsistencies in 'Flow_speed' values. Please check the output for details.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5YJ9V468HwJ"
      },
      "source": [
        "## **1.4. Filter tracks shorter than 50 spots**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8GBhFy8vd6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ##Filter tracks shorter than 50 spots\n",
        "\n",
        "\n",
        "merged_tracks_df = merged_tracks_df[merged_tracks_df['NUMBER_SPOTS'] >= 50]\n",
        "merged_spots_df = merged_spots_df[merged_spots_df['Unique_ID'].isin(merged_tracks_df['Unique_ID'])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52STmnv43d45"
      },
      "source": [
        "## **1.5. Visualise your tracks**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AE881uJW5ukQ"
      },
      "outputs": [],
      "source": [
        "# @title ##Run the cell and choose the file you want to inspect\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if not os.path.exists(Results_Folder+\"/Tracks\"):\n",
        "    os.makedirs(Results_Folder+\"/Tracks\")  # Create Results_Folder if it doesn't exist\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "# Create a Dropdown widget with the filenames\n",
        "filename_dropdown = widgets.Dropdown(\n",
        "    options=filenames,\n",
        "    value=filenames[0] if len(filenames) > 0 else None,  # Default selected value\n",
        "    description='File Name:',\n",
        ")\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid filename selected\")\n",
        "\n",
        "# Link the Dropdown widget to the plotting function\n",
        "interact(plot_coordinates, filename=filename_dropdown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "g9aUcAm596CZ"
      },
      "outputs": [],
      "source": [
        "# @title ##Batch Process\n",
        "\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the Results_Folder/Tracks directory exists\n",
        "if not os.path.exists(Results_Folder + \"/Tracks\"):\n",
        "    os.makedirs(Results_Folder + \"/Tracks\")\n",
        "\n",
        "# Extract unique filenames from the dataframe\n",
        "filenames = merged_spots_df['File_name'].unique()\n",
        "\n",
        "def plot_coordinates(filename):\n",
        "    if filename:\n",
        "        # Filter the DataFrame based on the selected filename\n",
        "        filtered_df = merged_spots_df[merged_spots_df['File_name'] == filename]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for unique_id in filtered_df['Unique_ID'].unique():\n",
        "            unique_df = filtered_df[filtered_df['Unique_ID'] == unique_id].sort_values(by='POSITION_T')\n",
        "            plt.plot(unique_df['POSITION_X'], unique_df['POSITION_Y'], marker='o', linestyle='-', markersize=2)\n",
        "\n",
        "        plt.xlabel('POSITION_X')\n",
        "        plt.ylabel('POSITION_Y')\n",
        "        plt.title(f'Coordinates for {filename}')\n",
        "        plt.savefig(f\"{Results_Folder}/Tracks/Tracks_{filename}.pdf\")\n",
        "        plt.close()  # Close the plot to avoid displaying it\n",
        "\n",
        "# Loop through all filenames and generate plots\n",
        "for filename in filenames:\n",
        "    plot_coordinates(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQ2P8sw1izGb"
      },
      "outputs": [],
      "source": [
        "# @title ##Speed density plots\n",
        "\n",
        "\n",
        "# Updated code to visualize distributions using the 'fill' parameter in sns.kdeplot\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_distribution_by_condition_updated(df):\n",
        "    conditions = df['Condition'].unique()\n",
        "\n",
        "    # Setting up the plotting environment\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(18, 20))  # Increased height to fit the fourth plot\n",
        "\n",
        "    # Plotting histograms for TRACK_MEAN_SPEED\n",
        "    plt.subplot(4, 1, 1)\n",
        "    for condition in conditions:\n",
        "        sns.histplot(df[df['Condition'] == condition]['TRACK_MEAN_SPEED'], label=condition, kde=False, bins=30)\n",
        "    plt.title('Histogram of TRACK_MEAN_SPEED by Condition')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting histograms for TRACK_MAX_SPEED\n",
        "    plt.subplot(4, 1, 2)\n",
        "    for condition in conditions:\n",
        "        sns.histplot(df[df['Condition'] == condition]['TRACK_MAX_SPEED'], label=condition, kde=False, bins=30)\n",
        "    plt.title('Histogram of TRACK_MAX_SPEED by Condition')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting histograms for TRACK_MIN_SPEED\n",
        "    plt.subplot(4, 1, 3)\n",
        "    for condition in conditions:\n",
        "        sns.histplot(df[df['Condition'] == condition]['TRACK_MIN_SPEED'], label=condition, kde=False, bins=30)\n",
        "    plt.title('Histogram of TRACK_MIN_SPEED by Condition')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting histograms for TOTAL_DISTANCE_TRAVELED\n",
        "    plt.subplot(4, 1, 4)\n",
        "    for condition in conditions:\n",
        "        sns.histplot(df[df['Condition'] == condition]['TOTAL_DISTANCE_TRAVELED'], label=condition, kde=False, bins=30)\n",
        "    plt.title('Histogram of TOTAL_DISTANCE_TRAVELED by Condition')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# You can call this function with your dataframe like this:\n",
        "plot_distribution_by_condition_updated(merged_tracks_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jWasqb467Z65"
      },
      "outputs": [],
      "source": [
        "# @title ##Time points per tracks\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Calculate the count of time points per track\n",
        "time_points_per_track = merged_spots_df.groupby('Unique_ID').size()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "time_points_per_track.hist(bins=30, edgecolor='black')\n",
        "plt.title('Distribution of Time Points per Track')\n",
        "plt.xlabel('Number of Time Points')\n",
        "plt.ylabel('Count of Tracks')\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUCXg5QUC4f"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 2. Compute additional metrics**\n",
        "--------------------------------------------------------\n",
        "<font size = 4 color=\"red\">Part2 does not support Track splitting</font>.\n",
        "\n",
        "<font size = 4> For users aiming to compute additional track metrics within this environment, it is crucial to disable track splitting in TrackMate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As279fwW1WwX"
      },
      "source": [
        "## **2.1. Compute Speed and rolling distance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x84gy1Ydk_MM"
      },
      "outputs": [],
      "source": [
        "# @title ##Compute Speed and rolling distance\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def compute_instantaneous_speed(dataframe):\n",
        "    # Check for required columns\n",
        "    required_columns = ['Unique_ID', 'POSITION_T', 'POSITION_X', 'POSITION_Y']\n",
        "    for col in required_columns:\n",
        "        if col not in dataframe.columns:\n",
        "            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n",
        "\n",
        "    # Check for duplicate entries\n",
        "    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n",
        "        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n",
        "\n",
        "    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n",
        "\n",
        "    speeds = []\n",
        "\n",
        "    for _, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Speeds\"):\n",
        "        # Check for NaN values in columns\n",
        "        if track[['POSITION_X', 'POSITION_Y', 'POSITION_T']].isna().any().any():\n",
        "            raise ValueError(f\"Track with ID '{track['Unique_ID'].iloc[0]}' contains NaN values which might affect the computation.\")\n",
        "\n",
        "        # Calculate the instantaneous speed using positional data and time difference\n",
        "        speed = np.sqrt(track['POSITION_X'].diff()**2 + track['POSITION_Y'].diff()**2) / track['POSITION_T'].diff()\n",
        "\n",
        "        # Ensure that time differences are non-negative\n",
        "        if (track['POSITION_T'].diff() < 0).any():\n",
        "            raise ValueError(f\"Track with ID '{track['Unique_ID'].iloc[0]}' has negative time differences.\")\n",
        "\n",
        "        # Ensuring the first speed value for each track is NaN\n",
        "        speed.iloc[0] = np.nan\n",
        "\n",
        "        speeds.extend(speed.tolist())\n",
        "\n",
        "    # Safety Check\n",
        "    if len(speeds) != len(dataframe):\n",
        "        raise ValueError(\"The computed speeds list length doesn't match the dataframe's length.\")\n",
        "\n",
        "    dataframe['Speed'] = speeds\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# Example usage:\n",
        "merged_spots_df = compute_instantaneous_speed(merged_spots_df)\n",
        "\n",
        "\n",
        "def compute_rolling_average(dataframe, window_size=5):\n",
        "    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n",
        "\n",
        "    rolling_avg_speeds = []\n",
        "\n",
        "    # Wrap the groupby object with tqdm for progress visualization\n",
        "    for _, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Rolling Averages\"):\n",
        "        rolling_avg = track['Speed'].rolling(window=window_size, min_periods=1, center=True).mean()\n",
        "        rolling_avg_speeds.extend(rolling_avg.tolist())\n",
        "\n",
        "    # Safety Check\n",
        "    if len(rolling_avg_speeds) != len(dataframe):\n",
        "        raise ValueError(\"The computed rolling averages list length doesn't match the dataframe's length.\")\n",
        "\n",
        "    dataframe['RollingAvgSpeed'] = rolling_avg_speeds\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# Example usage:\n",
        "merged_spots_df = compute_rolling_average(merged_spots_df, window_size=5)\n",
        "\n",
        "\n",
        "def average_speed_first_last_n(dataframe, n=5):\n",
        "    # Ensure n is a positive integer\n",
        "    if not isinstance(n, int) or n <= 0:\n",
        "        raise ValueError(\"n should be a positive integer.\")\n",
        "\n",
        "    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n",
        "\n",
        "    speeds_first = {}\n",
        "    speeds_last = {}\n",
        "\n",
        "    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Calculating average speeds\"):\n",
        "        # Ensure the track has at least n points\n",
        "        if len(track) < n:\n",
        "            print(f\"Track {track_id} has less than {n} points. Skipping.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Average speed for first n time points using RollingAvgSpeed\n",
        "        avg_speed_first = track['Speed'].iloc[:n].mean()\n",
        "        speeds_first[track_id] = avg_speed_first\n",
        "\n",
        "        # Average speed for last n time points using RollingAvgSpeed\n",
        "        avg_speed_last = track['Speed'].iloc[-n:].mean()\n",
        "        speeds_last[track_id] = avg_speed_last\n",
        "\n",
        "    # Convert average speeds to DataFrames\n",
        "    avg_speeds_first_df = pd.DataFrame(speeds_first.items(), columns=['Unique_ID', 'AvgSpeedFirstN'])\n",
        "    avg_speeds_last_df = pd.DataFrame(speeds_last.items(), columns=['Unique_ID', 'AvgSpeedLastN'])\n",
        "\n",
        "    return avg_speeds_first_df, avg_speeds_last_df\n",
        "\n",
        "# Example usage:\n",
        "avg_speeds_first, avg_speeds_last = average_speed_first_last_n(merged_spots_df, 5)\n",
        "\n",
        "\n",
        "def compute_min_rolling_speed(dataframe):\n",
        "    # Safeguard: Ensure required columns are present\n",
        "    required_columns = ['Unique_ID', 'POSITION_T', 'RollingAvgSpeed']\n",
        "    for col in required_columns:\n",
        "        if col not in dataframe.columns:\n",
        "            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n",
        "\n",
        "    # Safeguard: Check for duplicate entries\n",
        "    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n",
        "        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n",
        "\n",
        "    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n",
        "\n",
        "    min_speeds = {}\n",
        "\n",
        "    # Wrap the groupby object with tqdm for progress visualization\n",
        "    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Min Rolling Speeds\"):\n",
        "\n",
        "        min_speed = track['RollingAvgSpeed'].min()\n",
        "        min_speeds[track_id] = min_speed\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    min_speed_df = pd.DataFrame(min_speeds.items(), columns=['Unique_ID', 'MinRollingAvgSpeed'])\n",
        "\n",
        "    return min_speed_df\n",
        "\n",
        "# Compute the minimum rolling speed for merged_spots_df\n",
        "min_rolling_speed_df = compute_min_rolling_speed(merged_spots_df)\n",
        "\n",
        "\n",
        "def merge_speeds(df_main, df_to_merge, key='Unique_ID'):\n",
        "    # Safeguard: Ensure 'key' is present in both dataframes\n",
        "    if key not in df_main.columns or key not in df_to_merge.columns:\n",
        "        raise ValueError(f\"The key '{key}' is not present in both dataframes to be merged.\")\n",
        "\n",
        "    overlapping_columns = df_main.columns.intersection(df_to_merge.columns).drop(key)\n",
        "    df_main.drop(columns=overlapping_columns, inplace=True)\n",
        "    return pd.merge(df_main, df_to_merge, on=key, how='left')\n",
        "\n",
        "\n",
        "merged_tracks_df = merge_speeds(merged_tracks_df, avg_speeds_first)\n",
        "merged_tracks_df = merge_speeds(merged_tracks_df, avg_speeds_last)\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, min_rolling_speed_df)\n",
        "\n",
        "def compute_rolling_distance(dataframe, window_size=3):\n",
        "    \"\"\"Compute the total distance traveled within a rolling time window.\"\"\"\n",
        "    # Safeguard: Ensure required columns are present\n",
        "    required_columns = ['Unique_ID', 'POSITION_T', 'POSITION_X', 'POSITION_Y']\n",
        "    for col in required_columns:\n",
        "        if col not in dataframe.columns:\n",
        "            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n",
        "\n",
        "    # Safeguard: Handle potential negative or zero values for window size\n",
        "    if window_size <= 0:\n",
        "        raise ValueError(\"Window size must be a positive integer.\")\n",
        "\n",
        "    # Safeguard: Check for duplicate entries\n",
        "    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n",
        "        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n",
        "\n",
        "    # Safeguard: Ensure window size is odd for trimming edges correctly\n",
        "    if window_size % 2 == 0:\n",
        "        raise ValueError(\"Please use an odd value for the window size for accurate trimming.\")\n",
        "\n",
        "    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n",
        "\n",
        "    trim_size = window_size // 2  # Determine how much to trim from the edges\n",
        "    rolling_distances = []\n",
        "\n",
        "    for _, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Rolling Distance\"):\n",
        "        # Compute the Euclidean distance between consecutive points\n",
        "        distances = np.sqrt(track['POSITION_X'].diff()**2 + track['POSITION_Y'].diff()**2).fillna(0)\n",
        "\n",
        "        # Compute the rolling sum of distances\n",
        "        rolling_distance = distances.rolling(window=window_size, center=True).sum()\n",
        "\n",
        "        # Trim the edges\n",
        "        rolling_distance[:trim_size] = np.nan\n",
        "        rolling_distance[-trim_size:] = np.nan\n",
        "\n",
        "        rolling_distances.extend(rolling_distance.tolist())\n",
        "\n",
        "    # Safeguard: Ensure the list of rolling distances matches the length of the dataframe\n",
        "    if len(rolling_distances) != len(dataframe):\n",
        "        raise ValueError(\"The computed rolling distances list length doesn't match the dataframe's length.\")\n",
        "\n",
        "    dataframe['RollingDistance'] = rolling_distances\n",
        "    return dataframe\n",
        "\n",
        "merged_spots_df = compute_rolling_distance(merged_spots_df, window_size=5)\n",
        "\n",
        "\n",
        "def average_rolling_distance_first_last_n(dataframe, n=1):\n",
        "    \"\"\"Compute the average rolling distance for the first and last n points.\"\"\"\n",
        "\n",
        "    # Safeguard: Ensure required columns are present\n",
        "    required_columns = ['Unique_ID', 'POSITION_T', 'RollingDistance']\n",
        "    for col in required_columns:\n",
        "        if col not in dataframe.columns:\n",
        "            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n",
        "\n",
        "    # Safeguard: Handle potential non-positive values for n\n",
        "    if n <= 0:\n",
        "        raise ValueError(\"n must be a positive integer.\")\n",
        "\n",
        "    # Safeguard: Check for duplicate entries\n",
        "    if dataframe.duplicated(subset=['Unique_ID', 'POSITION_T']).any():\n",
        "        raise ValueError(\"There are duplicate entries based on 'Unique_ID' and 'POSITION_T'.\")\n",
        "\n",
        "    distance_first = {}\n",
        "    distance_last = {}\n",
        "    dataframe.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "    dataframe.reset_index(drop=True, inplace=True)  # Reset the index and drop the old one\n",
        "\n",
        "\n",
        "    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Calculating average rolling distances\"):\n",
        "        avg_distance_first = track['RollingDistance'].iloc[:n].sum()\n",
        "        distance_first[track_id] = avg_distance_first\n",
        "\n",
        "        avg_distance_last = track['RollingDistance'].iloc[-n:].sum()\n",
        "        distance_last[track_id] = avg_distance_last\n",
        "\n",
        "    avg_distances_first_df = pd.DataFrame(distance_first.items(), columns=['Unique_ID', 'AvgRollingDistanceFirstN'])\n",
        "    avg_distances_last_df = pd.DataFrame(distance_last.items(), columns=['Unique_ID', 'AvgRollingDistanceLastN'])\n",
        "\n",
        "    return avg_distances_first_df, avg_distances_last_df\n",
        "\n",
        "\n",
        "def merge_rolling_distances(df_main, df_to_merge, key='Unique_ID'):\n",
        "    \"\"\"Merge rolling distances into a dataframe.\"\"\"\n",
        "    overlapping_columns = df_main.columns.intersection(df_to_merge.columns).drop(key)\n",
        "\n",
        "    # Safeguard: Ensure that the df_main is updated correctly after dropping overlapping columns\n",
        "    df_main = df_main.drop(columns=overlapping_columns)\n",
        "    return pd.merge(df_main, df_to_merge, on=key, how='left')\n",
        "\n",
        "\n",
        "def compute_min_rolling_distance(dataframe):\n",
        "    \"\"\"Compute the minimum rolling distance for each track.\"\"\"\n",
        "\n",
        "    # Safeguard: Ensure required columns are present\n",
        "    required_columns = ['Unique_ID', 'RollingDistance']\n",
        "    for col in required_columns:\n",
        "        if col not in dataframe.columns:\n",
        "            raise ValueError(f\"Column '{col}' is missing in the dataframe.\")\n",
        "\n",
        "    min_distances = {}\n",
        "\n",
        "    for track_id, track in tqdm(dataframe.groupby('Unique_ID'), desc=\"Computing Min Rolling Distances\"):\n",
        "        min_distance = track['RollingDistance'].min()\n",
        "        min_distances[track_id] = min_distance\n",
        "\n",
        "    min_distance_df = pd.DataFrame(min_distances.items(), columns=['Unique_ID', 'MinRollingDistance'])\n",
        "\n",
        "    return min_distance_df\n",
        "\n",
        "# Usage and merging operations:\n",
        "avg_distances_first, avg_distances_last = average_rolling_distance_first_last_n(merged_spots_df, 1)\n",
        "merged_tracks_df = merge_rolling_distances(merged_tracks_df, avg_distances_first)\n",
        "merged_tracks_df = merge_rolling_distances(merged_tracks_df, avg_distances_last)\n",
        "\n",
        "min_rolling_distance_df = compute_min_rolling_distance(merged_spots_df)\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(min_rolling_distance_df.columns).drop('Unique_ID')\n",
        "\n",
        "# Safeguard: Ensure that the merged_tracks_df is updated correctly after dropping overlapping columns\n",
        "merged_tracks_df = merged_tracks_df.drop(columns=overlapping_columns)\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, min_rolling_distance_df, on='Unique_ID', how='left')\n",
        "\n",
        "save_dataframe_with_progress(merged_spots_df, Results_Folder + '/' + 'merged_Spots.csv', desc=\"Saving Spots\")\n",
        "save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv', desc=\"Saving Tracks\")\n",
        "\n",
        "# Safeguard: check for NaN\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
        "check_for_nans(merged_spots_df, \"merged_spots_df\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TqsbEAya-U6"
      },
      "source": [
        "## **2.2. Directionality**\n",
        "---\n",
        "<font size = 4>To calculate the directionality of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The directionality, denoted as \\(D\\), is calculated using the formula:\n",
        "\n",
        "$$ D = \\frac{d_{\\text{euclidean}}}{d_{\\text{total path}}} $$\n",
        "\n",
        "where \\($d_{\\text{euclidean}}$\\) is the Euclidean distance between the first and the last points of the track, calculated as:\n",
        "\n",
        "$$ d_{\\text{euclidean}} = \\sqrt{(x_{\\text{end}} - x_{\\text{start}})^2 + (y_{\\text{end}} - y_{\\text{start}})^2 + (z_{\\text{end}} - z_{\\text{start}})^2} $$\n",
        "\n",
        "and \\($d_{\\text{total path}}$\\) is the sum of the Euclidean distances between all consecutive points in the track, representing the total path length traveled. If the total path length is zero, the directionality is defined to be zero. This measure provides insight into the straightness of the path taken, with a value of 1 indicating a straight path between the start and end points, and values approaching 0 indicating more circuitous paths.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ijl_DXjhTvnn"
      },
      "outputs": [],
      "source": [
        "# @title ##Calculate directionality\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"In progress...\")\n",
        "\n",
        "# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n",
        "if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n",
        "    spots_df_to_use = merged_spots_df\n",
        "\n",
        "spots_df_to_use.dropna(subset=['POSITION_X', 'POSITION_Y'], inplace=True)\n",
        "\n",
        "# Function to calculate Directionality\n",
        "def calculate_directionality(group):\n",
        "\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    start_point = group.iloc[0][['POSITION_X', 'POSITION_Y']].to_numpy()\n",
        "    end_point = group.iloc[-1][['POSITION_X', 'POSITION_Y']].to_numpy()\n",
        "\n",
        "    # Calculating Euclidean distance in 3D between start and end points\n",
        "    euclidean_distance = np.linalg.norm(end_point - start_point)\n",
        "\n",
        "    # Calculating the total path length in 3D\n",
        "    deltas = np.linalg.norm(np.diff(group[['POSITION_X', 'POSITION_Y']].values, axis=0), axis=1)\n",
        "    total_path_length = deltas.sum()\n",
        "\n",
        "    # Calculating Directionality\n",
        "    D = euclidean_distance / total_path_length if total_path_length != 0 else 0\n",
        "\n",
        "    return pd.Series({'Directionality': D})\n",
        "\n",
        "\n",
        "# Create a tqdm object for the groupby apply\n",
        "tqdm.pandas(desc=\"Calculating Directionality\")\n",
        "\n",
        "# Assuming spots_df_to_use is your DataFrame\n",
        "spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate directionality for each track\n",
        "df_directionality = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_directionality).reset_index()\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_directionality.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the directionality back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_directionality, on='Unique_ID', how='left')\n",
        "\n",
        "# Save the DataFrame with the calculated directionality\n",
        "save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n",
        "\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
        "\n",
        "print(\"...Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb_W3oSleJOj"
      },
      "source": [
        "## **2.3. Tortuosity**\n",
        "---\n",
        "<font size = 4>This measure provides insight into the curvature and complexity of the path taken, with a value of 1 indicating a straight path between the start and end points, and values greater than 1 indicating paths with more twists and turns.\n",
        "To calculate the tortuosity of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The tortuosity, denoted as \\(T\\), is calculated using the formula:\n",
        "\n",
        "$$ T = \\frac{d_{\\text{total path}}}{d_{\\text{euclidean}}} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uLiAYGb2eKbC"
      },
      "outputs": [],
      "source": [
        "# @title ##Calculate tortuosity\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"In progress...\")\n",
        "\n",
        "# Initialize tqdm with pandas\n",
        "tqdm.pandas(desc=\"Calculating Tortuosity\")\n",
        "\n",
        "# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n",
        "if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n",
        "    spots_df_to_use = merged_spots_df\n",
        "\n",
        "def calculate_tortuosity(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "\n",
        "    # Apply spatial calibration to the coordinates\n",
        "    calibrated_coords = group[['POSITION_X', 'POSITION_Y']].values\n",
        "\n",
        "    start_point = calibrated_coords[0]\n",
        "    end_point = calibrated_coords[-1]\n",
        "\n",
        "    # Calculating Euclidean distance in 3D between start and end points\n",
        "    euclidean_distance = np.linalg.norm(end_point - start_point)\n",
        "\n",
        "    # Calculating the total path length in 3D\n",
        "    deltas = np.linalg.norm(np.diff(calibrated_coords, axis=0), axis=1)\n",
        "    total_path_length = deltas.sum()\n",
        "\n",
        "    # Calculating Tortuosity\n",
        "    T = total_path_length / euclidean_distance if euclidean_distance != 0 else 0\n",
        "\n",
        "    return pd.Series({'Tortuosity': T})\n",
        "\n",
        "# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n",
        "spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate tortuosity for each track using progress_apply\n",
        "df_tortuosity = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_tortuosity).reset_index()\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_tortuosity.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the tortuosity back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_tortuosity, on='Unique_ID', how='left')\n",
        "\n",
        "# Save the DataFrame with the calculated tortuosity\n",
        "save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n",
        "\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
        "\n",
        "print(\"...Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoZC4xJVhkpe"
      },
      "source": [
        "## **2.4. Calculate the total turning angle**\n",
        "---\n",
        "\n",
        "<font size = 4>This measure provides insight into the cumulative amount of turning along the path, with a value of 0 indicating a straight path with no turning, and higher values indicating paths with more turning.\n",
        "\n",
        "<font size = 4>To calculate the Total Turning Angle of a track in 3D space, we consider a series of points each with \\(x\\), \\(y\\), and \\(z\\) coordinates, sorted by time. The Total Turning Angle, denoted as \\(A\\), is the sum of the angles between each pair of consecutive direction vectors along the track, representing the cumulative amount of turning along the path.\n",
        "\n",
        "<font size = 4>For each pair of consecutive segments in the track, we calculate the direction vectors \\( $\\vec{v_1}$ \\) and \\($ \\vec{v_2}$ \\), and the angle \\($ \\theta$ \\) between them is calculated using the formula:\n",
        "\n",
        "$$ \\cos(\\theta) = \\frac{\\vec{v_1} \\cdot \\vec{v_2}}{||\\vec{v_1}|| \\cdot ||\\vec{v_2}||} $$\n",
        "\n",
        "<font size = 4>where \\( $\\vec{v_1} \\cdot$ $\\vec{v_2}$ \\) is the dot product of the direction vectors, and \\( $||\\vec{v_1}||$ \\) and \\( $||\\vec{v_2}||$ \\) are the magnitudes of the direction vectors. The Total Turning Angle \\( $A$ \\) is then the sum of all the angles \\( \\$theta$ \\) calculated between each pair of consecutive direction vectors along the track:\n",
        "\n",
        "$$ A = \\sum \\theta $$\n",
        "<font size = 4>\n",
        "If either of the direction vectors is a zero vector, the angle between them is undefined, and such cases are skipped in the calculation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1Sbn-9ephHlO"
      },
      "outputs": [],
      "source": [
        "# @title ##Calculate the total turning angle\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"In progress...\")\n",
        "\n",
        "# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n",
        "if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n",
        "    spots_df_to_use = merged_spots_df\n",
        "\n",
        "# Initialize tqdm with pandas\n",
        "tqdm.pandas(desc=\"Calculating Total Turning Angle\")\n",
        "\n",
        "# Check if spots_df_to_use is None or empty; if so, set it to merged_spots_df\n",
        "if 'spots_df_to_use' not in globals() or spots_df_to_use is None or spots_df_to_use.empty:\n",
        "    spots_df_to_use = merged_spots_df\n",
        "\n",
        "def calculate_total_turning_angle(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "    directions = group[['POSITION_X', 'POSITION_Y']].diff().dropna()\n",
        "    total_turning_angle = 0\n",
        "\n",
        "    for i in range(1, len(directions)):\n",
        "        dir1 = directions.iloc[i - 1]\n",
        "        dir2 = directions.iloc[i]\n",
        "\n",
        "        if np.linalg.norm(dir1) == 0 or np.linalg.norm(dir2) == 0:\n",
        "            continue\n",
        "\n",
        "        cos_angle = np.dot(dir1, dir2) / (np.linalg.norm(dir1) * np.linalg.norm(dir2))\n",
        "        cos_angle = np.clip(cos_angle, -1, 1)\n",
        "        angle = np.degrees(np.arccos(cos_angle))\n",
        "        total_turning_angle += angle\n",
        "\n",
        "    return pd.Series({'Total_Turning_Angle': total_turning_angle})\n",
        "\n",
        "# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n",
        "spots_df_to_use.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Calculate total turning angle for each track using progress_apply instead of apply\n",
        "df_turning_angle = spots_df_to_use.groupby('Unique_ID').progress_apply(calculate_total_turning_angle).reset_index()\n",
        "\n",
        "# Check if 'Total_Turning_Angle' is in the columns of df_turning_angle\n",
        "if 'Total_Turning_Angle' not in df_turning_angle.columns:\n",
        "    print(\"Error: 'Total_Turning_Angle' not in df_turning_angle columns\")\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_turning_angle.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the total turning angle back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_turning_angle, on='Unique_ID', how='left')\n",
        "\n",
        "# Save the DataFrame with the calculated total turning angle\n",
        "save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n",
        "\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
        "\n",
        "print(\"...Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32o1NIy8CP11"
      },
      "source": [
        "## **2.6. Calculate the FMI**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kg0iXBV034Ve"
      },
      "outputs": [],
      "source": [
        "# @title #Calculate the FMI\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def calculate_fmi(group):\n",
        "    group = group.sort_values('POSITION_T')\n",
        "\n",
        "    deltas = np.sqrt(group['POSITION_X'].diff().fillna(0)**2 + group['POSITION_Y'].diff().fillna(0)**2)\n",
        "    total_path_length = deltas.sum()\n",
        "\n",
        "    total_forward_displacement = group['POSITION_X'].diff().fillna(0).sum()\n",
        "\n",
        "    FMI = total_forward_displacement / total_path_length if total_path_length != 0 else 0\n",
        "\n",
        "    return pd.Series({'FMI': FMI})\n",
        "\n",
        "\n",
        "# Use tqdm.pandas() for progress_apply\n",
        "tqdm.pandas(desc=\"Processing tracks\")\n",
        "\n",
        "# Sort the DataFrame by 'Unique_ID' and 'POSITION_T'\n",
        "merged_spots_df.sort_values(by=['Unique_ID', 'POSITION_T'], inplace=True)\n",
        "\n",
        "# Group by track ID and calculate metrics with tqdm progress bar\n",
        "grouped = merged_spots_df.groupby('Unique_ID')\n",
        "df_fmi = grouped.progress_apply(calculate_fmi).reset_index()\n",
        "\n",
        "# Find the overlapping columns between the two DataFrames, excluding the merging key\n",
        "overlapping_columns = merged_tracks_df.columns.intersection(df_fmi.columns).drop('Unique_ID')\n",
        "\n",
        "# Drop the overlapping columns from the left DataFrame\n",
        "merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "\n",
        "# Merge the FMI values back into the original DataFrame\n",
        "merged_tracks_df = pd.merge(merged_tracks_df, df_fmi, on='Unique_ID', how='left')\n",
        "\n",
        "merged_tracks_df.to_csv(Results_Folder + '/' + 'merged_Tracks.csv', index=False)\n",
        "\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drR6HJcLvQ9P"
      },
      "source": [
        "## **2.7. additional morphological metrics**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bLtflCadvOP1"
      },
      "outputs": [],
      "source": [
        "# @title ##Compute additional morphological metrics\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"In progress...\")\n",
        "\n",
        "def compute_morphological_metrics(spots_df, metrics):\n",
        "    # Compute mean, median, std, min, and max for each metric\n",
        "    mean_df = spots_df.groupby('Unique_ID')[metrics].mean(numeric_only=True).add_prefix('MEAN_')\n",
        "    median_df = spots_df.groupby('Unique_ID')[metrics].median(numeric_only=True).add_prefix('MEDIAN_')\n",
        "    std_df = spots_df.groupby('Unique_ID')[metrics].std(numeric_only=True).add_prefix('STD_')\n",
        "    min_df = spots_df.groupby('Unique_ID')[metrics].min(numeric_only=True).add_prefix('MIN_')\n",
        "    max_df = spots_df.groupby('Unique_ID')[metrics].max(numeric_only=True).add_prefix('MAX_')\n",
        "\n",
        "    # Concatenate the computed metrics into a single dataframe without resetting the index\n",
        "    metrics_df = pd.concat([mean_df, median_df, std_df, min_df, max_df], axis=1)\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "# Required columns for compute_morphological_metrics\n",
        "required_columns_spots = ['Unique_ID', 'RADIUS', 'CIRCULARITY', 'SOLIDITY', 'SHAPE_INDEX']\n",
        "\n",
        "# Check which required columns are present in merged_spots_df\n",
        "available_columns = [col for col in required_columns_spots if col in merged_spots_df.columns]\n",
        "missing_columns = [col for col in required_columns_spots if col not in merged_spots_df.columns]\n",
        "\n",
        "# Compute the morphological metrics\n",
        "morphological_metrics_df = compute_morphological_metrics(merged_spots_df, available_columns)\n",
        "\n",
        "# Reset the index for the morphological_metrics_df to have Unique_ID as a column\n",
        "morphological_metrics_df.reset_index(inplace=True)\n",
        "\n",
        "# Find overlapping columns and merge\n",
        "if 'Unique_ID' in merged_tracks_df.columns:\n",
        "    overlapping_columns = merged_tracks_df.columns.intersection(morphological_metrics_df.columns).drop('Unique_ID', errors='ignore')\n",
        "    merged_tracks_df.drop(columns=overlapping_columns, inplace=True)\n",
        "    merged_tracks_df = merged_tracks_df.merge(morphological_metrics_df, on='Unique_ID', how='left')\n",
        "    save_dataframe_with_progress(merged_tracks_df, Results_Folder + '/' + 'merged_Tracks.csv')\n",
        "\n",
        "else:\n",
        "    print(\"Error: 'Unique_ID' column missing in merged_tracks_df. Skipping merging with morphological metrics.\")\n",
        "\n",
        "check_for_nans(merged_tracks_df, \"merged_tracks_df\")\n",
        "\n",
        "print(\"...Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joRI14WVUPuM"
      },
      "source": [
        "-------------------------------------------\n",
        "\n",
        "# **Part 3. Plot track parameters**\n",
        "-------------------------------------------\n",
        "\n",
        "<font size = 4> In this section you can plot all the track parameters previously computed. Data and graphs are automatically saved in your result folder.\n",
        "\n",
        "<font size = 4 color=\"red\"> Parameters computed are in the unit you provided when tracking your data in TrackMate.\n",
        "\n",
        "##**Statistical analyses**\n",
        "### Cohen's d (Effect Size):\n",
        "<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n",
        "\n",
        "### Randomization Test:\n",
        "<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n",
        "\n",
        "### Bonferroni Correction:\n",
        "<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MIhsPZp96z8z"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot track parameters\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n",
        "# Check and create necessary directories\n",
        "if not os.path.exists(f\"{Results_Folder}/track_parameters_plots\"):\n",
        "    os.makedirs(f\"{Results_Folder}/track_parameters_plots\")\n",
        "\n",
        "if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/pdf\"):\n",
        "    os.makedirs(f\"{Results_Folder}/track_parameters_plots/pdf\")\n",
        "\n",
        "if not os.path.exists(f\"{Results_Folder}/track_parameters_plots/csv\"):\n",
        "    os.makedirs(f\"{Results_Folder}/track_parameters_plots/csv\")\n",
        "\n",
        "# Helper functions\n",
        "def cohen_d(group1, group2):\n",
        "    \"\"\"Compute Cohen's d.\"\"\"\n",
        "    mean_diff = group1.mean() - group2.mean()\n",
        "    pooled_var = (len(group1) * group1.var() + len(group2) * group2.var()) / (len(group1) + len(group2))\n",
        "    d = mean_diff / pooled_var**0.5\n",
        "    return d\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    \"\"\"Get columns that can be plotted.\"\"\"\n",
        "    exclude_cols = ['Condition', 'File_name', 'Flow_speed', 'Cells', 'ILbeta', 'Repeat', 'Unique_ID',\n",
        "                    'experiment_nb', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION',\n",
        "                    'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n",
        "    return [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    \"\"\"Display checkboxes for selecting variables.\"\"\"\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3))\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "\n",
        "def create_filename(base, selected_cells, selected_speeds, selected_ilbetas, var):\n",
        "    \"\"\"Create a unique filename based on selected options.\"\"\"\n",
        "    def summarize_options(options):\n",
        "        if len(options) > 3:\n",
        "            return f\"{len(options)}options\"\n",
        "        return \"_\".join(options)\n",
        "\n",
        "    selected_options = \"_\".join([\n",
        "        summarize_options(selected_cells),\n",
        "        summarize_options(selected_speeds),\n",
        "        summarize_options(selected_ilbetas)\n",
        "    ])\n",
        "\n",
        "    filename = f\"{base}_{selected_options}_{var}.pdf\"\n",
        "    return filename.replace(\" \", \"_\")  # Replace spaces with underscores for file compatibility\n",
        "\n",
        "\n",
        "# Create checkboxes for various attributes\n",
        "cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in merged_tracks_df['Cells'].unique()]\n",
        "flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in merged_tracks_df['Flow_speed'].unique()]\n",
        "ilbeta_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in merged_tracks_df['ILbeta'].unique()]\n",
        "\n",
        "\n",
        "# Display checkboxes\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Cells:'),\n",
        "    widgets.GridBox(cells_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n",
        "    widgets.Label('Flow Speed:'),\n",
        "    widgets.GridBox(flow_speed_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4)),\n",
        "    widgets.Label('ILbeta:'),\n",
        "    widgets.GridBox(ilbeta_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 100px)\" % 4))\n",
        "\n",
        "]))\n",
        "\n",
        "# Convert Flow_speed to string for checkbox matching\n",
        "merged_tracks_df['Flow_speed'] = merged_tracks_df['Flow_speed'].astype(str)\n",
        "\n",
        "# Define the plotting function\n",
        "def plot_selected_vars(button, variable_checkboxes):\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "    # Fetch selected values\n",
        "    selected_cells = [box.description for box in cells_checkboxes if box.value]\n",
        "    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n",
        "    selected_ilbetas = [box.description for box in ilbeta_checkboxes if box.value]\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "\n",
        "    # Filter dataframe\n",
        "    filtered_df = merged_tracks_df.copy()\n",
        "    filtered_df = filtered_df[filtered_df['Cells'].isin(selected_cells)]\n",
        "    filtered_df = filtered_df[filtered_df['Flow_speed'].isin(selected_speeds)]\n",
        "    filtered_df = filtered_df[filtered_df['ILbeta'].isin(selected_ilbetas)]\n",
        "\n",
        "    # Initialize matrices for statistics\n",
        "    effect_size_matrices = {}\n",
        "    p_value_matrices = {}\n",
        "    bonferroni_matrices = {}\n",
        "\n",
        "    unique_conditions = filtered_df['Condition'].unique().tolist()\n",
        "    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n",
        "    alpha = 0.05\n",
        "    corrected_alpha = alpha / num_comparisons\n",
        "    n_iterations = 1000\n",
        "\n",
        "# Loop through each variable to plot\n",
        "    for var in variables_to_plot:\n",
        "\n",
        "      filename = create_filename(\"track_parameters_plots\", selected_cells, selected_speeds, selected_ilbetas, var)\n",
        "      pdf_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"pdf\", filename)\n",
        "      csv_path = os.path.join(Results_Folder, \"track_parameters_plots\", \"csv\", f\"{filename[:-4]}.csv\")  # Remove '.pdf' and add '.csv'\n",
        "\n",
        "      pdf_pages = PdfPages(pdf_path)\n",
        "\n",
        "      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "\n",
        "      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n",
        "        group1 = filtered_df[filtered_df['Condition'] == cond1][var]\n",
        "        group2 = filtered_df[filtered_df['Condition'] == cond2][var]\n",
        "\n",
        "        original_d = cohen_d(group1, group2)\n",
        "        effect_size_matrix.loc[cond1, cond2] = original_d\n",
        "        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n",
        "\n",
        "        count_extreme = 0\n",
        "        for i in range(n_iterations):\n",
        "            combined = pd.concat([group1, group2])\n",
        "            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n",
        "            new_group1 = shuffled[:len(group1)]\n",
        "            new_group2 = shuffled[len(group1):]\n",
        "\n",
        "            new_d = cohen_d(new_group1, new_group2)\n",
        "            if np.abs(new_d) >= np.abs(original_d):\n",
        "                count_extreme += 1\n",
        "\n",
        "        p_value = count_extreme / n_iterations\n",
        "        p_value_matrix.loc[cond1, cond2] = p_value\n",
        "        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n",
        "\n",
        "        # Apply Bonferroni correction\n",
        "        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n",
        "        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n",
        "        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n",
        "\n",
        "      effect_size_matrices[var] = effect_size_matrix\n",
        "      p_value_matrices[var] = p_value_matrix\n",
        "      bonferroni_matrices[var] = bonferroni_matrix\n",
        "\n",
        "    # Concatenate the three matrices side-by-side\n",
        "      combined_df = pd.concat(\n",
        "        [\n",
        "            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n",
        "            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n",
        "            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n",
        "        ], axis=1\n",
        "    )\n",
        "\n",
        "    # Save the combined DataFrame to a CSV file\n",
        "      combined_df.to_csv(csv_path)\n",
        "\n",
        "    # Create a new figure\n",
        "      fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Create a gridspec for 2 rows and 4 columns\n",
        "      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n",
        "\n",
        "    # Create the ax for boxplot using the gridspec\n",
        "      ax_box = fig.add_subplot(gs[0, :])\n",
        "\n",
        "    # Extract the data for this variable\n",
        "      data_for_var = filtered_df[['Condition', var, 'Repeat', 'File_name' ]]\n",
        "\n",
        "    # Save the data_for_var to a CSV for replotting\n",
        "      data_for_var.to_csv(f\"{Results_Folder}/track_parameters_plots/csv/{var}_boxplot_data.csv\", index=False)\n",
        "\n",
        "    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n",
        "      Q1 = filtered_df[var].quantile(0.25)\n",
        "      Q3 = filtered_df[var].quantile(0.75)\n",
        "      IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "      multiplier = 10\n",
        "      lower_bound = Q1 - multiplier * IQR\n",
        "      upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "    # Plotting\n",
        "      sns.boxplot(x='Condition', y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n",
        "      sns.stripplot(x='Condition', y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n",
        "      ax_box.set_title(f\"{var}\")\n",
        "      ax_box.set_xlabel('Condition')\n",
        "      ax_box.set_ylabel(var)\n",
        "      ax_box.set_xticklabels(ax_box.get_xticklabels(), rotation=90)\n",
        "      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "    # Statistical Analyses and Heatmaps\n",
        "\n",
        "    # Effect Size heatmap ax\n",
        "      ax_d = fig.add_subplot(gs[1, 0])\n",
        "      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"coolwarm\", cbar=True, square=True, ax=ax_d)\n",
        "      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n",
        "\n",
        "    # p-value heatmap ax\n",
        "      ax_p = fig.add_subplot(gs[1, 1])\n",
        "      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n",
        "      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n",
        "\n",
        "    # Bonferroni corrected p-value heatmap ax\n",
        "      ax_bonf = fig.add_subplot(gs[1, 2])\n",
        "      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n",
        "      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      pdf_pages.savefig(fig)\n",
        "# Close the PDF\n",
        "      pdf_pages.close()\n",
        "\n",
        "# Display variable checkboxes and button\n",
        "selectable_columns = get_selectable_columns(merged_tracks_df)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes))\n",
        "display(button)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHi3-zp3JTd"
      },
      "source": [
        "--------\n",
        "# **Part 4. Quality Control**\n",
        "--------\n",
        "\n",
        "\n",
        "### Compute Similarity Metrics between Field of Views (FOV) and between Conditions and Repeats\n",
        "\n",
        "<font size = 4>**Purpose**:\n",
        "\n",
        "<font size = 4>This section provides a set of tools to compute and visualize similarities between different field of views (FOV) based on selected track parameters. By leveraging hierarchical clustering, the resulting dendrogram offers a clear visualization of how different FOV, conditions, or repeats relate to one another. This tool is essential for:\n",
        "\n",
        "<font size = 4>1. **Quality Control**:\n",
        "    - Ensuring that FOVs from the same condition or experimental setup are more similar to each other than to FOVs from different conditions.\n",
        "    - Confirming that repeats of the same experiment yield consistent results and cluster together.\n",
        "    \n",
        "<font size = 4>2. **Data Integrity**:\n",
        "    - Identifying potential outliers or anomalies in the dataset.\n",
        "    - Assessing the overall consistency of the experiment and ensuring reproducibility.\n",
        "\n",
        "<font size = 4>**How to Use**:\n",
        "\n",
        "<font size = 4>1. **Track Parameters Selection**:\n",
        "    - A list of checkboxes allows users to select which track parameters they want to consider for similarity calculations. By default, all parameters are selected. Users can deselect parameters that they believe might not contribute significantly to the similarity.\n",
        "\n",
        "<font size = 4>2. **Similarity Metric**:\n",
        "    - Users can choose a similarity metric from a dropdown list. Options include cosine, euclidean, cityblock, jaccard, and correlation. The choice of similarity metric can influence the clustering results, so users might need to experiment with different metrics to see which one provides the most meaningful results.\n",
        "\n",
        "<font size = 4>3. **Linkage Method**:\n",
        "    - Determines how the distance between clusters is calculated in the hierarchical clustering process. Different linkage methods can produce different dendrograms, so users might want to try various methods.\n",
        "\n",
        "<font size = 4>4. **Visualization**:\n",
        "    - Once the parameters are selected, users can click on the \"Select the track parameters and visualize similarity\" button. This will compute the hierarchical clustering and display two dendrograms:\n",
        "        - One dendrogram displays similarities between individual FOVs.\n",
        "        - Another dendrogram aggregates the data based on conditions and repeats, providing a higher-level view of the similarities.\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nRYGqVS_tNkv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# @title ##Filter the data\n",
        "\n",
        "\n",
        "# Global variables to store the selected options\n",
        "global filtered_df\n",
        "filtered_df = pd.DataFrame()\n",
        "\n",
        "global selected_cells, selected_speeds, selected_ilbetas\n",
        "selected_cells, selected_speeds, selected_ilbetas = [], [], []\n",
        "\n",
        "# Function to summarize selected options into a string\n",
        "def summarize_options(options):\n",
        "    return \"_\".join([str(option) for option in options if option])  # Filters out any 'falsy' values like empty strings or None\n",
        "\n",
        "# Function to create a filename based on selected options\n",
        "def create_filename(selected_cells, selected_speeds, selected_ilbetas):\n",
        "    # Join the summarized options for each parameter with an underscore\n",
        "    selected_options = \"_\".join([\n",
        "        summarize_options(selected_cells),\n",
        "        summarize_options(selected_speeds),\n",
        "        summarize_options(selected_ilbetas)\n",
        "    ])\n",
        "\n",
        "    # Replace spaces with underscores and return the filename\n",
        "    filename = f\"{selected_options}\"\n",
        "    return filename.replace(\" \", \"_\")\n",
        "\n",
        "# Create checkboxes for each category\n",
        "cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in merged_tracks_df['Cells'].unique()]\n",
        "flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in merged_tracks_df['Flow_speed'].unique()]\n",
        "ilbeta_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in merged_tracks_df['ILbeta'].unique()]\n",
        "\n",
        "# Function to filter dataframe and update global variables based on selected checkbox values\n",
        "def filter_dataframe(button):\n",
        "    global filtered_df, selected_cells, selected_speeds, selected_ilbetas\n",
        "\n",
        "    # Trim whitespace and correct cases if necessary\n",
        "    merged_tracks_df['Cells'] = merged_tracks_df['Cells'].str.strip()\n",
        "    merged_tracks_df['Flow_speed'] = merged_tracks_df['Flow_speed'].str.strip()\n",
        "    merged_tracks_df['ILbeta'] = merged_tracks_df['ILbeta'].str.strip()\n",
        "\n",
        "    selected_cells = [box.description for box in cells_checkboxes if box.value]\n",
        "    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n",
        "    selected_ilbetas = [box.description for box in ilbeta_checkboxes if box.value]\n",
        "\n",
        "    # Debugging output\n",
        "    print(\"Selected Cells:\", selected_cells)\n",
        "    print(\"Selected Speeds:\", selected_speeds)\n",
        "    print(\"Selected ILbetas:\", selected_ilbetas)\n",
        "    print(\"Original DF length:\", len(merged_tracks_df))\n",
        "\n",
        "    filtered_df = merged_tracks_df[\n",
        "        (merged_tracks_df['Cells'].isin(selected_cells)) &\n",
        "        (merged_tracks_df['Flow_speed'].isin(selected_speeds)) &\n",
        "        (merged_tracks_df['ILbeta'].isin(selected_ilbetas))\n",
        "    ]\n",
        "\n",
        "    # More debugging output\n",
        "    print(\"Filtered DF length:\", len(filtered_df))\n",
        "    if len(filtered_df) == 0:\n",
        "        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n",
        "        print(\"Unique 'Cells' in DataFrame:\", merged_tracks_df['Cells'].unique())\n",
        "        print(\"Unique 'Flow_speed' in DataFrame:\", merged_tracks_df['Flow_speed'].unique())\n",
        "        print(\"Unique 'ILbeta' in DataFrame:\", merged_tracks_df['ILbeta'].unique())\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Now call the filter function or trigger the button to filter the dataframe and see the output.\n",
        "\n",
        "\n",
        "# Button to trigger dataframe filtering\n",
        "filter_button = widgets.Button(description=\"Filter Dataframe\")\n",
        "filter_button.on_click(filter_dataframe)\n",
        "\n",
        "# Display checkboxes and button\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Select Cells:'),\n",
        "    widgets.HBox(cells_checkboxes),\n",
        "    widgets.Label('Select Flow Speed:'),\n",
        "    widgets.HBox(flow_speed_checkboxes),\n",
        "    widgets.Label('Select ILbeta:'),\n",
        "    widgets.HBox(ilbeta_checkboxes),\n",
        "    filter_button\n",
        "]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bojHIRXv5bnN"
      },
      "outputs": [],
      "source": [
        "# @title ##Compute similarity metrics between FOV and between conditions and repeats\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import ipywidgets as widgets\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# Check and create \"Similarity\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Similarity\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Similarity\")\n",
        "\n",
        "# Columns to exclude\n",
        "excluded_columns = ['experiment_nb', 'File_name', 'Repeat', 'TRACK_INDEX', 'TRACK_ID',\n",
        "                    'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n",
        "\n",
        "filename = create_filename(selected_cells, selected_speeds, selected_ilbetas)\n",
        "\n",
        "selected_df = pd.DataFrame()\n",
        "\n",
        "# Filter out non-numeric columns but keep 'File_name'\n",
        "numeric_df = filtered_df.select_dtypes(include=['float64', 'int64']).copy()\n",
        "numeric_df['File_name'] = filtered_df['File_name']\n",
        "\n",
        "# Create a list of column names excluding 'File_name'\n",
        "column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n",
        "\n",
        "# Create a checkbox for each column\n",
        "checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n",
        "\n",
        "# Dropdown for similarity metrics\n",
        "similarity_dropdown = widgets.Dropdown(\n",
        "    options=['cosine', 'euclidean', 'cityblock', 'jaccard', 'correlation'],\n",
        "    value='cosine',\n",
        "    description='Similarity Metric:'\n",
        ")\n",
        "\n",
        "# Dropdown for linkage methods\n",
        "linkage_dropdown = widgets.Dropdown(\n",
        "    options=['single', 'complete', 'average', 'ward'],\n",
        "    value='single',\n",
        "    description='Linkage Method:'\n",
        ")\n",
        "\n",
        "# Arrange checkboxes in a 2x grid\n",
        "grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
        "\n",
        "# Create a button to trigger the selection and visualization\n",
        "button = widgets.Button(description=\"Select the track parameters and visualize similarity\", layout=widgets.Layout(width='400px'))\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_button_click(b):\n",
        "    global selected_df  # Declare selected_df as global\n",
        "\n",
        "    # Get the selected columns from the checkboxes\n",
        "    selected_columns = [box.description for box in checkboxes if box.value]\n",
        "    selected_columns.append('File_name')  # Always include 'File_name'\n",
        "\n",
        "    # Extract the selected columns from the DataFrame\n",
        "    selected_df = numeric_df[selected_columns]\n",
        "\n",
        "    # Aggregate the data by filename\n",
        "    aggregated_by_filename = selected_df.groupby('File_name').mean(numeric_only=True)\n",
        "\n",
        "    # Aggregate the data by condition and repeat\n",
        "    aggregated_by_condition_repeat = filtered_df.groupby(['Condition', 'Repeat'])[selected_columns].mean(numeric_only=True)\n",
        "\n",
        "    # Compute condensed distance matrices\n",
        "    distance_matrix_filename = pdist(aggregated_by_filename, metric=similarity_dropdown.value)\n",
        "    distance_matrix_condition_repeat = pdist(aggregated_by_condition_repeat, metric=similarity_dropdown.value)\n",
        "\n",
        "    # Perform hierarchical clustering\n",
        "    linked_filename = linkage(distance_matrix_filename, method=linkage_dropdown.value)\n",
        "    linked_condition_repeat = linkage(distance_matrix_condition_repeat, method=linkage_dropdown.value)\n",
        "\n",
        "    annotation_text = f\"Similarity Method: {similarity_dropdown.value}, Linkage Method: {linkage_dropdown.value}\"\n",
        "\n",
        "\n",
        "    # Plot the dendrograms one under the other\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    # Dendrogram for individual filenames\n",
        "    plt.subplot(2, 1, 1)\n",
        "    dendrogram(linked_filename, labels=aggregated_by_filename.index, orientation='top', distance_sort='descending', leaf_rotation=90)\n",
        "    plt.title(f'Dendrogram of Field of view Similarities\\n{annotation_text}')\n",
        "\n",
        "    # Dendrogram for aggregated data based on condition and repeat\n",
        "    plt.subplot(2, 1, 2)\n",
        "    dendrogram(linked_condition_repeat, labels=aggregated_by_condition_repeat.index, orientation='top', distance_sort='descending', leaf_rotation=90)\n",
        "    plt.title(f'Dendrogram of Aggregated Similarities by Condition and Repeat\\n{annotation_text}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the dendrogram to a PDF\n",
        "    pdf_pages = PdfPages(f\"{Results_Folder}/Similarity/{filename}_Dendrogram_Similarities.pdf\")\n",
        "\n",
        "    # Save the current figure to the PDF\n",
        "    pdf_pages.savefig()\n",
        "\n",
        "    # Close the PdfPages object to finalize the document\n",
        "    pdf_pages.close()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Set the button click event handler\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display the widgets\n",
        "display(grid, similarity_dropdown, linkage_dropdown, button)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tFTER9rg2Id"
      },
      "source": [
        "--------\n",
        "# **Part 5. Explore your high-dimensional data using UMAP and HDBSCAN**\n",
        "--------\n",
        "\n",
        "<font size = 4> The workflow provided below is inspired by [CellPlato](https://github.com/Michael-shannon/cellPLATO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7bKh1LujZsP"
      },
      "source": [
        "## **5.1. Choose the track metrics to use for clustering**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JvdnHmT0tO3r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# @title ##Filter the data\n",
        "\n",
        "\n",
        "# Global variables to store the selected options\n",
        "global filtered_df\n",
        "filtered_df = pd.DataFrame()\n",
        "\n",
        "global selected_cells, selected_speeds, selected_ilbetas\n",
        "selected_cells, selected_speeds, selected_ilbetas = [], [], []\n",
        "\n",
        "# Function to summarize selected options into a string\n",
        "def summarize_options(options):\n",
        "    return \"_\".join([str(option) for option in options if option])  # Filters out any 'falsy' values like empty strings or None\n",
        "\n",
        "# Function to create a filename based on selected options\n",
        "def create_filename(selected_cells, selected_speeds, selected_ilbetas):\n",
        "    # Join the summarized options for each parameter with an underscore\n",
        "    selected_options = \"_\".join([\n",
        "        summarize_options(selected_cells),\n",
        "        summarize_options(selected_speeds),\n",
        "        summarize_options(selected_ilbetas)\n",
        "    ])\n",
        "\n",
        "    # Replace spaces with underscores and return the filename\n",
        "    filename = f\"{selected_options}\"\n",
        "    return filename.replace(\" \", \"_\")\n",
        "\n",
        "# Create checkboxes for each category\n",
        "cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in merged_tracks_df['Cells'].unique()]\n",
        "flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in merged_tracks_df['Flow_speed'].unique()]\n",
        "ilbeta_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in merged_tracks_df['ILbeta'].unique()]\n",
        "\n",
        "# Function to filter dataframe and update global variables based on selected checkbox values\n",
        "def filter_dataframe(button):\n",
        "    global filtered_df, selected_cells, selected_speeds, selected_ilbetas\n",
        "\n",
        "    # Trim whitespace and correct cases if necessary\n",
        "    merged_tracks_df['Cells'] = merged_tracks_df['Cells'].str.strip()\n",
        "    merged_tracks_df['Flow_speed'] = merged_tracks_df['Flow_speed'].str.strip()\n",
        "    merged_tracks_df['ILbeta'] = merged_tracks_df['ILbeta'].str.strip()\n",
        "\n",
        "    selected_cells = [box.description for box in cells_checkboxes if box.value]\n",
        "    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n",
        "    selected_ilbetas = [box.description for box in ilbeta_checkboxes if box.value]\n",
        "\n",
        "    # Debugging output\n",
        "    print(\"Selected Cells:\", selected_cells)\n",
        "    print(\"Selected Speeds:\", selected_speeds)\n",
        "    print(\"Selected ILbetas:\", selected_ilbetas)\n",
        "    print(\"Original DF length:\", len(merged_tracks_df))\n",
        "\n",
        "    filtered_df = merged_tracks_df[\n",
        "        (merged_tracks_df['Cells'].isin(selected_cells)) &\n",
        "        (merged_tracks_df['Flow_speed'].isin(selected_speeds)) &\n",
        "        (merged_tracks_df['ILbeta'].isin(selected_ilbetas))\n",
        "    ]\n",
        "\n",
        "    # More debugging output\n",
        "    print(\"Filtered DF length:\", len(filtered_df))\n",
        "    if len(filtered_df) == 0:\n",
        "        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n",
        "        print(\"Unique 'Cells' in DataFrame:\", merged_tracks_df['Cells'].unique())\n",
        "        print(\"Unique 'Flow_speed' in DataFrame:\", merged_tracks_df['Flow_speed'].unique())\n",
        "        print(\"Unique 'ILbeta' in DataFrame:\", merged_tracks_df['ILbeta'].unique())\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Now call the filter function or trigger the button to filter the dataframe and see the output.\n",
        "\n",
        "\n",
        "# Button to trigger dataframe filtering\n",
        "filter_button = widgets.Button(description=\"Filter Dataframe\")\n",
        "filter_button.on_click(filter_dataframe)\n",
        "\n",
        "# Display checkboxes and button\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Select Cells:'),\n",
        "    widgets.HBox(cells_checkboxes),\n",
        "    widgets.Label('Select Flow Speed:'),\n",
        "    widgets.HBox(flow_speed_checkboxes),\n",
        "    widgets.Label('Select ILbeta:'),\n",
        "    widgets.HBox(ilbeta_checkboxes),\n",
        "    filter_button\n",
        "]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EP-kp5JkR3xh"
      },
      "outputs": [],
      "source": [
        "# @title ##Choose the track metrics to use\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap\")\n",
        "\n",
        "\n",
        "excluded_columns = ['experiment_nb', 'TRACK_INDEX', 'TRACK_ID',\n",
        "                    'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n",
        "\n",
        "# Columns you want to always include\n",
        "columns_to_include = ['File_name', 'Repeat', 'Condition', 'Unique_ID']\n",
        "\n",
        "selected_df = pd.DataFrame()\n",
        "nan_columns = pd.DataFrame()\n",
        "# Extract the columns you always want to include and ensure they exist in the original dataframe\n",
        "saved_columns = {col: filtered_df[col].copy() for col in columns_to_include if col in filtered_df}\n",
        "\n",
        "# Filter out non-numeric columns\n",
        "numeric_df = filtered_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n",
        "\n",
        "column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n",
        "\n",
        "# Create a checkbox for each column\n",
        "checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n",
        "\n",
        "# Arrange checkboxes in a 2x grid\n",
        "grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
        "\n",
        "# Create a button to trigger the selection\n",
        "button = widgets.Button(description=\"Select the track parameters\", layout=widgets.Layout(width='400px'))\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_button_click(b):\n",
        "    global selected_df  # Declare selected_df as global\n",
        "    global nan_columns\n",
        "    # Get the selected columns from the checkboxes\n",
        "    selected_columns = [box.description for box in checkboxes if box.value]\n",
        "\n",
        "    # Extract the selected columns from the DataFrame\n",
        "    selected_df = numeric_df[selected_columns].copy()\n",
        "\n",
        "    # Add back the always-included columns to selected_df\n",
        "    for col, data in saved_columns.items():\n",
        "        selected_df.loc[:, col] = data\n",
        "\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "      for col in nan_columns:\n",
        "        initial_row_count = len(selected_df)\n",
        "        selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "        dropped_row_count = initial_row_count - len(selected_df)\n",
        "\n",
        "        print(f\"Dropped {dropped_row_count} rows from column '{col}' due to NaN values.\")\n",
        "\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Set the button click event handler\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display the grid of checkboxes and the button\n",
        "display(grid, button)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLlDuCKmlxvl"
      },
      "source": [
        "## **5.2. UMAP**\n",
        "---\n",
        "\n",
        "<font size = 4> The given code performs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the merged tracks dataframe, focusing on its numeric columns, and visualizes the result. In the provided UMAP code, the parameters `n_neighbors`, `min_dist`, and `n_components` are crucial for determining the structure and appearance of the resulting low-dimensional representation of the data.\n",
        "\n",
        "<font size = 4>`n_neighbors`: This parameter controls how UMAP balances local versus global structure in the data. It determines the size of the local neighborhood UMAP will look at when learning the manifold structure of the data.\n",
        "- A smaller value emphasizes the local structure of the data, potentially at the expense of the global structure.\n",
        "- A larger value allows UMAP to consider more distant neighbors, emphasizing more on the global structure of the data.\n",
        "- Typically, values in the range of 5 to 50 are chosen, depending on the density and scale of the data.\n",
        "\n",
        "<font size = 4>`min_dist`: This parameter controls how tightly UMAP is allowed to pack points together. It determines the minimum distance between points in the low-dimensional representation.\n",
        "- Setting it to a low value will allow points to be packed more closely, potentially revealing clusters in the data.\n",
        "- A higher value ensures that points are more spread out in the representation.\n",
        "- Values usually range between 0 and 1.\n",
        "\n",
        "<font size = 4>`n_dimension`: This parameter determines the number of dimensions in the low-dimensional space that the data will be reduced to.\n",
        "For visualization purposes, `n_dimension` is typically set to 2 or 3 to obtain 2D or 3D representations, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h-HJIe9ug1bl"
      },
      "outputs": [],
      "source": [
        "# @title ##Perform UMAP\n",
        "import umap\n",
        "import plotly.offline as pyo\n",
        "\n",
        "\n",
        "filename = create_filename(selected_cells, selected_speeds, selected_ilbetas)\n",
        "\n",
        "# Check and create necessary directories\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap/{filename}\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap/{filename}\")\n",
        "\n",
        "#@markdown ###UMAP parameters:\n",
        "\n",
        "n_neighbors = 30  # @param {type: \"number\"}\n",
        "min_dist = 0.1  # @param {type: \"number\"}\n",
        "n_dimension = 2  # @param {type: \"slider\", min: 1, max: 3}\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 10 # @param {type: \"number\"}\n",
        "\n",
        "# Initialize UMAP object with the specified settings\n",
        "reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimension, random_state=42)\n",
        "# Exclude non-numeric columns when fitting UMAP\n",
        "embedding = reducer.fit_transform(selected_df.drop(columns=columns_to_include))\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f'UMAP dimension {i}' for i in range(1, n_dimension + 1)]\n",
        "\n",
        "# Extract the columns_to_include from selected_df\n",
        "included_data = selected_df[columns_to_include].reset_index(drop=True)\n",
        "\n",
        "# Concatenate the UMAP embedding with the included columns\n",
        "umap_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n",
        "\n",
        "\n",
        "# Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "nan_columns = umap_df.columns[umap_df.isna().any()].tolist()\n",
        "\n",
        "if nan_columns:\n",
        "  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "  for col in nan_columns:\n",
        "    umap_df = umap_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "\n",
        "# Visualize the UMAP projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# The plot will adjust automatically based on the n_components\n",
        "if n_dimension == 2:\n",
        "    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=umap_df, palette='Set2', s=spot_size)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/{filename}/umap_projection_2D.pdf\")  # Save 2D plot as PDF\n",
        "    plt.show()\n",
        "elif n_dimension == 1:\n",
        "    sns.stripplot(x=column_names[0], hue='Condition', data=umap_df, palette='Set2', jitter=0.05, size=spot_size)\n",
        "    plt.title('UMAP Projection of the Dataset')\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/{filename}/umap_projection_1D.pdf\")  # Save 2D plot as PDF\n",
        "    plt.show()\n",
        "else:\n",
        "    # umap_df should have columns like 'UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3', and 'condition'\n",
        "    import plotly.express as px\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Condition')\n",
        "\n",
        "    for trace in fig.data:\n",
        "      trace.marker.size = spot_size  # You can set this to any desired value\n",
        "\n",
        "    fig.show()\n",
        "    pyo.plot(fig, filename=f\"{Results_Folder}/Umap/{filename}/umap_projection.html\", auto_open=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMx8DVRMmvKQ"
      },
      "source": [
        "## **5.3. HDBSCAN**\n",
        "---\n",
        "\n",
        "<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n",
        "\n",
        "<font size = 4>In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n",
        "\n",
        "<font size = 4>`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n",
        "- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n",
        "- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n",
        "- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n",
        "\n",
        "<font size = 4>`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n",
        "- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n",
        "- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n",
        "\n",
        "<font size = 4>`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n",
        "- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PGdYdL7hnrpK"
      },
      "outputs": [],
      "source": [
        "# @title ##Run to see more information about the available metrics\n",
        "print(\"\"\"\n",
        "Metric                   Description                                                               Suitable For\n",
        "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "Euclidean                Standard distance metric.                                                 Numerical data.\n",
        "Manhattan                Sum of absolute differences.                                              Numerical/Categorical data.\n",
        "Chebyshev                Maximum value of absolute differences.                                    Numerical data.\n",
        "Minkowski                Generalization of Euclidean and Manhattan distance.                       Numerical data.\n",
        "Bray-Curtis              Dissimilarity between sample sets.                                        Numerical data.\n",
        "Canberra                 Weighted version of Manhattan distance.                                   Numerical data.\n",
        "Mahalanobis              Distance between a point and a distribution.                              Numerical data.\n",
        "\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1VV75MUjixkQ"
      },
      "outputs": [],
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#@markdown ###HDBSCAN parameters:\n",
        "clustering_data_source = 'umap'  # @param ['umap', 'raw']\n",
        "min_samples = 50  # @param {type: \"number\"}\n",
        "min_cluster_size = 100  # @param {type: \"number\"}\n",
        "metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 10 # @param {type: \"number\"}\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)  # You may need to tune these parameters\n",
        "\n",
        "if clustering_data_source == 'umap':\n",
        "  if n_dimension == 1:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1']])  # Use only one UMAP dimension for clustering\n",
        "\n",
        "  elif n_dimension == 2:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2']])  # Use two UMAP dimensions for clustering\n",
        "\n",
        "  elif n_dimension == 3:\n",
        "    clusterer.fit(umap_df[['UMAP dimension 1', 'UMAP dimension 2', 'UMAP dimension 3']])  # Use three UMAP dimensions for clustering\n",
        "\n",
        "else:\n",
        "  clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your UMAP DataFrame\n",
        "umap_df['Cluster'] = clusterer.labels_\n",
        "\n",
        "# If the Cluster column already exists in merged_tracks_df, drop it to avoid duplications\n",
        "if 'Cluster' in filtered_df.columns:\n",
        "    filtered_df.drop(columns='Cluster', inplace=True)\n",
        "\n",
        "# Merge the Cluster column from umap_df to merged_tracks_df based on Unique_ID\n",
        "filtered_df = pd.merge(filtered_df, umap_df[['Unique_ID', 'Cluster']], on='Unique_ID', how='left')\n",
        "\n",
        "# Handle cases where some rows in merged_tracks_df might not have a corresponding cluster label\n",
        "filtered_df['Cluster'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n",
        "\n",
        "# Save the DataFrame with the identified clusters\n",
        "filtered_df.to_csv(Results_Folder + '/Umap/'+filename+'/' + 'filtered_Tracks.csv', index=False)\n",
        "\n",
        "# Plotting the results\n",
        "if n_dimension == 1:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.stripplot(data=umap_df, x='UMAP dimension 1', hue='Cluster', palette='viridis', s=spot_size)\n",
        "    plt.title('Clusters Identified by HDBSCAN (1D)')\n",
        "    plt.xlabel('UMAP dimension 1')\n",
        "    plt.ylabel('Count')\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/{filename}/HDBSCAN_clusters_1D.pdf\")  # Save 1D histogram as PDF\n",
        "    plt.show()\n",
        "\n",
        "if n_dimension == 2:\n",
        "\n",
        "  plt.figure(figsize=(12,10))\n",
        "  sns.scatterplot(x='UMAP dimension 1', y='UMAP dimension 2', hue='Cluster', palette='viridis', data=umap_df, s=spot_size)\n",
        "  plt.title('Clusters Identified by HDBSCAN')\n",
        "  plt.savefig(f\"{Results_Folder}/Umap/{filename}/HDBSCAN_clusters_2D.pdf\")  # Save 2D plot as PDF\n",
        "  plt.show()\n",
        "\n",
        "if n_dimension == 3:\n",
        "\n",
        "  fig = px.scatter_3d(umap_df,\n",
        "                    x='UMAP dimension 1',\n",
        "                    y='UMAP dimension 2',\n",
        "                    z='UMAP dimension 3',\n",
        "                    color='Cluster')\n",
        "\n",
        "  for trace in fig.data:\n",
        "    trace.marker.size = spot_size\n",
        "\n",
        "  fig.show()\n",
        "  pyo.plot(fig, filename=f\"{Results_Folder}/Umap/{filename}/HDBSCAN_clusters.html\", auto_open=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KWMzPMaR3OC"
      },
      "source": [
        "## **5.4. Understand your clusters using box plots**\n",
        "\n",
        "<font size = 4>The provided code aims to visually represent the distribution of different track parameters across the identified clusters. Specifically, for each parameter selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each track parameter varies within and across the clusters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e2jMmapvz8Sh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# @title ##Plot track parameters based on clusters\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap/{filename}/Track_parameters\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap/{filename}/Track_parameters\")\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar']\n",
        "    return [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def plot_selected_vars(button, variable_checkboxes, df, Results_Folder):\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "    # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "    for var in variables_to_plot:\n",
        "        # Extract data for the specific variable and cluster\n",
        "        data_to_save = df[['Cluster', var]]\n",
        "\n",
        "        # Save data for the plot to CSV\n",
        "        data_to_save.to_csv(f\"{Results_Folder}/Umap/{filename}/Track_parameters/{var}_data_by_Cluster.csv\", index=False)\n",
        "\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Plotting\n",
        "        sns.boxplot(x='Cluster', y=var, data=df, color='lightgray')  # Boxplot by cluster\n",
        "        sns.stripplot(x='Cluster', y=var, data=df, jitter=True, alpha=0.2)  # Individual data points\n",
        "\n",
        "        plt.title(f\"{var} by Cluster\")\n",
        "        plt.xlabel('Cluster')\n",
        "        plt.ylabel(var)\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plt.savefig(f\"{Results_Folder}/Umap/{filename}/Track_parameters/{var}_Boxplots_by_Cluster.pdf\")\n",
        "        plt.show()\n",
        "\n",
        "selectable_columns = get_selectable_columns(filtered_df)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "\n",
        "# Create and display the plot button\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, filtered_df, Results_Folder))\n",
        "display(button)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huCM6iMWSd_j"
      },
      "source": [
        "## **5.5. Understand your clusters using heatmaps**\n",
        "\n",
        "<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HGuLloitQN38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# @title ##Plot track normalized track parameters based on clusters as an heatmap\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap/{filename}/Track_parameters\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap/{filename}/Track_parameters\")\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar']\n",
        "    return [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "def heatmap_comparison(df, Results_Folder):\n",
        "    # Get all the selectable columns\n",
        "    variables_to_plot = get_selectable_columns(df)\n",
        "\n",
        "    # Compute median for each variable across clusters\n",
        "    median_values = df.groupby('Cluster')[variables_to_plot].median().transpose()\n",
        "\n",
        "    # Normalize the median values using Z-score\n",
        "    normalized_values = median_values.apply(zscore, axis=1)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    sns.heatmap(normalized_values, cmap='coolwarm', annot=True, linewidths=.5)\n",
        "    plt.title(\"Z-score Normalized Median Values of Variables by Cluster\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the heatmap\n",
        "    plt.savefig(f\"{Results_Folder}/Umap/{filename}/Track_parameters/Heatmap_Normalized_Median_Values_by_Cluster.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "    # Save the normalized median values data to CSV\n",
        "    normalized_values.to_csv(f\"{Results_Folder}/Umap/{filename}/Track_parameters/Normalized_Median_Values_by_Cluster.csv\")\n",
        "\n",
        "# Plot the heatmap directly\n",
        "heatmap_comparison(filtered_df, Results_Folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBFfmTo5e-Wv"
      },
      "source": [
        "## **5.6 Fingerprint**\n",
        "---\n",
        "\n",
        "<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B8S0uwynjht1"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = umap_df.groupby(['Condition', 'Cluster']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = umap_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Save the percentage_df DataFrame as a CSV file\n",
        "percentage_df.to_csv(Results_Folder+'/Umap/'+filename+'/UMAP_percentage_results.csv', index=False)\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_pages = PdfPages(Results_Folder+'/Umap/'+filename+'/UMAP_Cluster_Fingerprint_Plot.pdf')\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='Set2')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FET3NkbssMdb"
      },
      "source": [
        "--------\n",
        "# **Part 6. Explore your high-dimensional data using t-SNE and HDBSCAN**\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "baZpv5gfGCM6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# @title ##Filter the data\n",
        "\n",
        "\n",
        "# Global variables to store the selected options\n",
        "global filtered_df\n",
        "filtered_df = pd.DataFrame()\n",
        "\n",
        "global selected_cells, selected_speeds, selected_ilbetas\n",
        "selected_cells, selected_speeds, selected_ilbetas = [], [], []\n",
        "\n",
        "# Function to summarize selected options into a string\n",
        "def summarize_options(options):\n",
        "    return \"_\".join([str(option) for option in options if option])  # Filters out any 'falsy' values like empty strings or None\n",
        "\n",
        "# Function to create a filename based on selected options\n",
        "def create_filename(selected_cells, selected_speeds, selected_ilbetas):\n",
        "    # Join the summarized options for each parameter with an underscore\n",
        "    selected_options = \"_\".join([\n",
        "        summarize_options(selected_cells),\n",
        "        summarize_options(selected_speeds),\n",
        "        summarize_options(selected_ilbetas)\n",
        "    ])\n",
        "\n",
        "    # Replace spaces with underscores and return the filename\n",
        "    filename = f\"{selected_options}\"\n",
        "    return filename.replace(\" \", \"_\")\n",
        "\n",
        "# Create checkboxes for each category\n",
        "cells_checkboxes = [widgets.Checkbox(value=False, description=str(cell)) for cell in merged_tracks_df['Cells'].unique()]\n",
        "flow_speed_checkboxes = [widgets.Checkbox(value=False, description=str(speed)) for speed in merged_tracks_df['Flow_speed'].unique()]\n",
        "ilbeta_checkboxes = [widgets.Checkbox(value=False, description=str(ilbeta)) for ilbeta in merged_tracks_df['ILbeta'].unique()]\n",
        "\n",
        "# Function to filter dataframe and update global variables based on selected checkbox values\n",
        "def filter_dataframe(button):\n",
        "    global filtered_df, selected_cells, selected_speeds, selected_ilbetas\n",
        "\n",
        "    # Trim whitespace and correct cases if necessary\n",
        "    merged_tracks_df['Cells'] = merged_tracks_df['Cells'].str.strip()\n",
        "    merged_tracks_df['Flow_speed'] = merged_tracks_df['Flow_speed'].str.strip()\n",
        "    merged_tracks_df['ILbeta'] = merged_tracks_df['ILbeta'].str.strip()\n",
        "\n",
        "    selected_cells = [box.description for box in cells_checkboxes if box.value]\n",
        "    selected_speeds = [box.description for box in flow_speed_checkboxes if box.value]\n",
        "    selected_ilbetas = [box.description for box in ilbeta_checkboxes if box.value]\n",
        "\n",
        "    # Debugging output\n",
        "    print(\"Selected Cells:\", selected_cells)\n",
        "    print(\"Selected Speeds:\", selected_speeds)\n",
        "    print(\"Selected ILbetas:\", selected_ilbetas)\n",
        "    print(\"Original DF length:\", len(merged_tracks_df))\n",
        "\n",
        "    filtered_df = merged_tracks_df[\n",
        "        (merged_tracks_df['Cells'].isin(selected_cells)) &\n",
        "        (merged_tracks_df['Flow_speed'].isin(selected_speeds)) &\n",
        "        (merged_tracks_df['ILbeta'].isin(selected_ilbetas))\n",
        "    ]\n",
        "\n",
        "    # More debugging output\n",
        "    print(\"Filtered DF length:\", len(filtered_df))\n",
        "    if len(filtered_df) == 0:\n",
        "        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n",
        "        print(\"Unique 'Cells' in DataFrame:\", merged_tracks_df['Cells'].unique())\n",
        "        print(\"Unique 'Flow_speed' in DataFrame:\", merged_tracks_df['Flow_speed'].unique())\n",
        "        print(\"Unique 'ILbeta' in DataFrame:\", merged_tracks_df['ILbeta'].unique())\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Now call the filter function or trigger the button to filter the dataframe and see the output.\n",
        "\n",
        "\n",
        "# Button to trigger dataframe filtering\n",
        "filter_button = widgets.Button(description=\"Filter Dataframe\")\n",
        "filter_button.on_click(filter_dataframe)\n",
        "\n",
        "# Display checkboxes and button\n",
        "display(widgets.VBox([\n",
        "    widgets.Label('Select Cells:'),\n",
        "    widgets.HBox(cells_checkboxes),\n",
        "    widgets.Label('Select Flow Speed:'),\n",
        "    widgets.HBox(flow_speed_checkboxes),\n",
        "    widgets.Label('Select ILbeta:'),\n",
        "    widgets.HBox(ilbeta_checkboxes),\n",
        "    filter_button\n",
        "]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i5MMeGvSGLzi"
      },
      "outputs": [],
      "source": [
        "# @title ##Choose the track metrics to use\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Check and create \"pdf\" folder\n",
        "if not os.path.exists(f\"{Results_Folder}/Umap\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Umap\")\n",
        "\n",
        "\n",
        "excluded_columns = ['experiment_nb', 'TRACK_INDEX', 'TRACK_ID',\n",
        "                    'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n",
        "\n",
        "# Columns you want to always include\n",
        "columns_to_include = ['File_name', 'Repeat', 'Condition', 'Unique_ID']\n",
        "\n",
        "selected_df = pd.DataFrame()\n",
        "nan_columns = pd.DataFrame()\n",
        "# Extract the columns you always want to include and ensure they exist in the original dataframe\n",
        "saved_columns = {col: filtered_df[col].copy() for col in columns_to_include if col in filtered_df}\n",
        "\n",
        "# Filter out non-numeric columns\n",
        "numeric_df = filtered_df.select_dtypes(include=['float64', 'int64'])  # Selecting only numeric columns\n",
        "\n",
        "column_names = [col for col in numeric_df.columns if col not in excluded_columns]\n",
        "\n",
        "# Create a checkbox for each column\n",
        "checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in column_names]\n",
        "\n",
        "# Arrange checkboxes in a 2x grid\n",
        "grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n",
        "\n",
        "# Create a button to trigger the selection\n",
        "button = widgets.Button(description=\"Select the track parameters\", layout=widgets.Layout(width='400px'))\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_button_click(b):\n",
        "    global selected_df  # Declare selected_df as global\n",
        "    global nan_columns\n",
        "    # Get the selected columns from the checkboxes\n",
        "    selected_columns = [box.description for box in checkboxes if box.value]\n",
        "\n",
        "    # Extract the selected columns from the DataFrame\n",
        "    selected_df = numeric_df[selected_columns].copy()\n",
        "\n",
        "    # Add back the always-included columns to selected_df\n",
        "    for col, data in saved_columns.items():\n",
        "        selected_df.loc[:, col] = data\n",
        "\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "      for col in nan_columns:\n",
        "        initial_row_count = len(selected_df)\n",
        "        selected_df = selected_df.dropna(subset=[col])  # Drop NaN values only from columns containing them\n",
        "        dropped_row_count = initial_row_count - len(selected_df)\n",
        "\n",
        "        print(f\"Dropped {dropped_row_count} rows from column '{col}' due to NaN values.\")\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "# Set the button click event handler\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display the grid of checkboxes and the button\n",
        "display(grid, button)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0YybCsg1r3kc"
      },
      "outputs": [],
      "source": [
        "# @title ##Perform t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "filename = create_filename(selected_cells, selected_speeds, selected_ilbetas)\n",
        "\n",
        "# Check and create necessary directories\n",
        "tsne_folder_path = f\"{Results_Folder}/Tsne/{filename}\"\n",
        "if not os.path.exists(tsne_folder_path):\n",
        "    os.makedirs(tsne_folder_path)\n",
        "\n",
        "#@markdown ###t-SNE parameters:\n",
        "\n",
        "perplexity = 50  # @param {type: \"number\"}\n",
        "learning_rate = 200  # @param {type: \"number\"}\n",
        "n_iter = 1000  # @param {type: \"number\"}\n",
        "n_dimension = 2  # The number of dimensions is set to 2 for t-SNE as standard practice\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 10  # @param {type: \"number\"}\n",
        "\n",
        "# Initialize t-SNE object with the specified settings\n",
        "tsne = TSNE(n_components=n_dimension, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=42)\n",
        "\n",
        "# Exclude non-numeric columns when fitting t-SNE\n",
        "numeric_columns = selected_df._get_numeric_data()\n",
        "embedding = tsne.fit_transform(numeric_columns)\n",
        "\n",
        "# Create dynamic column names based on n_components\n",
        "column_names = [f't-SNE dimension {i+1}' for i in range(n_dimension)]\n",
        "\n",
        "# Extract the columns_to_include from selected_df\n",
        "included_data = selected_df[columns_to_include].reset_index(drop=True)\n",
        "\n",
        "# Concatenate the t-SNE embedding with the included columns\n",
        "tsne_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n",
        "\n",
        "# Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "nan_columns = tsne_df.columns[tsne_df.isna().any()].tolist()\n",
        "if nan_columns:\n",
        "  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n",
        "  tsne_df.dropna(subset=nan_columns, inplace=True)  # Drop NaN values only from columns containing them\n",
        "\n",
        "# Visualize the t-SNE projection\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=tsne_df, palette='Set2', s=spot_size)\n",
        "plt.title('t-SNE Projection of the Dataset')\n",
        "tsne_output_path = os.path.join(tsne_folder_path, 'tsne_projection_2D.pdf')\n",
        "plt.savefig(tsne_output_path)  # Save 2D plot as PDF\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YAplT3ziYYHO"
      },
      "outputs": [],
      "source": [
        "# @title ##Identify clusters using HDBSCAN\n",
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "#@markdown ###HDBSCAN parameters:\n",
        "clustering_data_source = 'tsne'  # @param ['tsne', 'raw']\n",
        "min_samples = 50  # @param {type: \"number\"}\n",
        "min_cluster_size = 100  # @param {type: \"number\"}\n",
        "metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n",
        "\n",
        "#@markdown ###Display parameters:\n",
        "spot_size = 10 # @param {type: \"number\"}\n",
        "\n",
        "# Apply HDBSCAN\n",
        "clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)\n",
        "\n",
        "# Depending on the data source, we fit HDBSCAN to the t-SNE dimensions or the raw data\n",
        "if clustering_data_source == 'tsne':\n",
        "    # We only have two t-SNE dimensions based on the previous t-SNE code provided\n",
        "    clusterer.fit(tsne_df[['t-SNE dimension 1', 't-SNE dimension 2']])\n",
        "else:\n",
        "    # If raw data is selected, we use all the numerical columns for clustering\n",
        "    clusterer.fit(selected_df.select_dtypes(include=['number']))\n",
        "\n",
        "# Add the cluster labels to your t-SNE DataFrame\n",
        "tsne_df['Cluster'] = clusterer.labels_\n",
        "\n",
        "# If the Cluster column already exists in filtered_df, drop it to avoid duplications\n",
        "if 'Cluster' in filtered_df.columns:\n",
        "    filtered_df.drop(columns='Cluster', inplace=True)\n",
        "\n",
        "# Merge the Cluster column from tsne_df to filtered_df based on Unique_ID\n",
        "filtered_df = pd.merge(filtered_df, tsne_df[['Unique_ID', 'Cluster']], on='Unique_ID', how='left')\n",
        "\n",
        "# Handle cases where some rows in filtered_df might not have a corresponding cluster label\n",
        "filtered_df['Cluster'].fillna(-1, inplace=True)  # Assigning -1 to cells that were not assigned to any cluster\n",
        "\n",
        "# Save the DataFrame with the identified clusters\n",
        "filtered_df.to_csv(os.path.join(Results_Folder, 'Tsne', filename, 'filtered_Tracks.csv'), index=False)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.scatterplot(x='t-SNE dimension 1', y='t-SNE dimension 2', hue='Cluster', palette='viridis', data=tsne_df, s=spot_size)\n",
        "plt.title('Clusters Identified by HDBSCAN')\n",
        "plt.savefig(os.path.join(Results_Folder, 'Tsne', filename, 'HDBSCAN_clusters_2D.pdf'))  # Save 2D plot as PDF\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ldFIDqvQZfjP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# @title ##Plot track parameters based on clusters\n",
        "\n",
        "# Define paths for Tsne\n",
        "tsne_track_parameters_path = os.path.join(Results_Folder, 'Tsne', filename, 'Track_parameters')\n",
        "os.makedirs(tsne_track_parameters_path, exist_ok=True)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar']\n",
        "    return [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def plot_selected_vars(button, variable_checkboxes, df, Results_Folder, filename):\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "    # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "    for var in variables_to_plot:\n",
        "        # Extract data for the specific variable and cluster\n",
        "        data_to_save = df[['Cluster', var]]\n",
        "\n",
        "        # Save data for the plot to CSV\n",
        "        data_to_save.to_csv(os.path.join(tsne_track_parameters_path, f\"{var}_data_by_Cluster.csv\"), index=False)\n",
        "\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Plotting\n",
        "        sns.boxplot(x='Cluster', y=var, data=df, color='lightgray')  # Boxplot by cluster\n",
        "        sns.stripplot(x='Cluster', y=var, data=df, jitter=True, alpha=0.2)  # Individual data points\n",
        "\n",
        "        plt.title(f\"{var} by Cluster\")\n",
        "        plt.xlabel('Cluster')\n",
        "        plt.ylabel(var)\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot to PDF\n",
        "        plt.savefig(os.path.join(tsne_track_parameters_path, f\"{var}_Boxplots_by_Cluster.pdf\"))\n",
        "        plt.show()\n",
        "\n",
        "selectable_columns = get_selectable_columns(filtered_df)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "\n",
        "# Create and display the plot button\n",
        "button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n",
        "button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, filtered_df, Results_Folder, filename))\n",
        "display(button)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MOjLyiMhY6Pi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "import pandas as pd\n",
        "\n",
        "# @title ##Plot track normalized track parameters based on clusters as a heatmap\n",
        "\n",
        "# Create \"Tsne/Track_parameters\" directory if it doesn't exist\n",
        "tsne_track_parameters_path = os.path.join(Results_Folder, 'Tsne', filename, 'Track_parameters')\n",
        "os.makedirs(tsne_track_parameters_path, exist_ok=True)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar']\n",
        "    return [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "def heatmap_comparison(df, Results_Folder, filename):\n",
        "    # Get all the selectable columns\n",
        "    variables_to_plot = get_selectable_columns(df)\n",
        "\n",
        "    # Compute median for each variable across clusters\n",
        "    median_values = df.groupby('Cluster')[variables_to_plot].median().transpose()\n",
        "\n",
        "    # Normalize the median values using Z-score\n",
        "    normalized_values = median_values.apply(zscore, axis=1)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    sns.heatmap(normalized_values, cmap='coolwarm', annot=True, linewidths=.5)\n",
        "    plt.title(\"Z-score Normalized Median Values of Variables by Cluster\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the heatmap to PDF\n",
        "    heatmap_pdf_path = os.path.join(tsne_track_parameters_path, 'Heatmap_Normalized_Median_Values_by_Cluster.pdf')\n",
        "    plt.savefig(heatmap_pdf_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Save the normalized median values data to CSV\n",
        "    normalized_values_csv_path = os.path.join(tsne_track_parameters_path, 'Normalized_Median_Values_by_Cluster.csv')\n",
        "    normalized_values.to_csv(normalized_values_csv_path)\n",
        "\n",
        "# Plot the heatmap directly\n",
        "heatmap_comparison(filtered_df, Results_Folder, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DLpeayUKZxnt"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot the 'fingerprint' of each cluster per condition\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Group by 'Condition' and 'Cluster' and calculate the size of each group\n",
        "cluster_counts = tsne_df.groupby(['Condition', 'Cluster']).size().reset_index(name='counts')\n",
        "\n",
        "# Calculate the total number of points per condition\n",
        "total_counts = tsne_df.groupby('Condition').size().reset_index(name='total_counts')\n",
        "\n",
        "# Merge the DataFrames on 'Condition' to calculate percentages\n",
        "percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n",
        "percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n",
        "\n",
        "# Save the percentage_df DataFrame as a CSV file\n",
        "percentage_df.to_csv(os.path.join(Results_Folder, 'Tsne', filename, 'TSNE_percentage_results.csv'), index=False)\n",
        "\n",
        "# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n",
        "pivot_df = percentage_df.pivot(index='Condition', columns='Cluster', values='percentage')\n",
        "\n",
        "# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n",
        "pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf_path = os.path.join(Results_Folder, 'Tsne', filename, 'TSNE_Cluster_Fingerprint_Plot.pdf')\n",
        "pdf_pages = PdfPages(pdf_path)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='Set2')\n",
        "plt.title('Percentage in each cluster per Condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Condition')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to a PDF\n",
        "pdf_pages.savefig(fig)\n",
        "\n",
        "# Close the PDF\n",
        "pdf_pages.close()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YheRveJdrFyP"
      },
      "source": [
        "## **Part 7. Version log**\n",
        "---\n",
        "<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n",
        "  - This notebook may contain bugs.\n",
        "  - Features are currently limited and will be expanded in future releases.\n",
        "\n",
        "<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n",
        "\n",
        "#### **Known Issues**:\n",
        "- Tracks are displayed in 2D in section 1.4\n",
        "\n",
        "<font size = 4>**Version 0.6**\n",
        "  - Improved organisation of the results\n",
        "  - Tracks visualisation are now saved\n",
        "\n",
        "<font size = 4>**Version 0.5**\n",
        "  - Improved part 5\n",
        "  - Added the possibility to find examplar on the raw movies when available\n",
        "  - Added the possibility to export video with the examplar labeled\n",
        "  - Code improved to deal with larger dataset (tested with over 50k tracks)\n",
        "  - test dataset now contains raw video and is hosted on Zenodo\n",
        "  - Results are now organised in folders\n",
        "  - Added progress bars\n",
        "  - Minor code fixes\n",
        "\n",
        "<font size = 4>**Version 0.4**\n",
        "\n",
        "  - Added the possibility to filter and smooth tracks\n",
        "  - Added spatial and temporal calibration\n",
        "  - Notebook is streamlined\n",
        "  - multiple bug fix\n",
        "  - Remove the t-sne\n",
        "  - Improved documentation\n",
        "\n",
        "<font size = 4>**Version 0.3**\n",
        "  - Fix a nasty bug in the import functions\n",
        "  - Add basic examplar for UMAP\n",
        "  - Added the statistical analyses and their explanations.\n",
        "  - Added a new quality control part that helps assessing the similarity of results between FOV, conditions and repeats\n",
        "  - Improved part 5 (previously part 4).\n",
        "\n",
        "<font size = 4>**Version 0.2**\n",
        "  - Added support for 3D tracks\n",
        "  - New documentation and metrics added.\n",
        "\n",
        "<font size = 4>**Version 0.1**\n",
        "This is the first release of this notebook.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}